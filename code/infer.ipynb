{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from infer_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_reload = [\"infer_functions\"]\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    __import__(module_name)\n",
    "    module = sys.modules[module_name]\n",
    "    globals().update({name: getattr(module, name) for name in dir(module) if not name.startswith('_')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_tensor_dataset = create_tensor_dataset(\"static\", 4, limit=100, categories= 0, averages=True)\n",
    "dynamic_tensor_dataset = create_tensor_dataset(\"dynamic\", 4, limit=100, categories= 0, averages=True, use_profile=True, use_multiples=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False) tensor(False) tensor(False)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in dynamic_tensor_dataset:\n",
    "    print(inputs[0].shape)  # Should output something like torch.Size([32, 60, 3])\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "with open(r\"..\\other_pickle\\measures.json\", \"r\") as file:\n",
    "    measures = json.load(file)\n",
    "\n",
    "static_size = len(measures[\"static\"])\n",
    "dynamic_size = len(measures[\"dynamic\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "STATIC_BATCH_SIZE = 20\n",
    "DYNAMIC_BATCH_SIZE = 5\n",
    "\n",
    "# static_train_dataloader = torch.utils.data.DataLoader(static_tensor_dataset, batch_size=STATIC_BATCH_SIZE, shuffle=True)\n",
    "dynamic_train_dataloader = torch.utils.data.DataLoader(dynamic_tensor_dataset, batch_size=DYNAMIC_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "LAYERS = 8\n",
    "\n",
    "class StaticLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, batch_size, layers, input, categories=0):\n",
    "        super(StaticLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.layers_num = layers\n",
    "        \n",
    "        #input is all the embedding vectors plus all the other variables\n",
    "        self.lstm = nn.LSTM(input, hidden_dim, num_layers=layers, batch_first=True) \n",
    "        self.hidden = (torch.zeros(layers,batch_size,hidden_dim),torch.zeros(layers,batch_size,hidden_dim))\n",
    "        \n",
    "        #Squeeeze them into 1 dimension\n",
    "        if categories > 0:\n",
    "            self.hidden2label = nn.Linear(hidden_dim, categories)\n",
    "        else:\n",
    "            self.hidden2label = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, batch_tensor):\n",
    "        lstm_out, self.hidden = self.lstm(batch_tensor)\n",
    "        last_timestep_output = lstm_out[:, -1, :]\n",
    "        sales = self.hidden2label(last_timestep_output)\n",
    "        return sales\n",
    "    \n",
    "    def hidden_reset(self):\n",
    "        #reset the hidden and cell state after each epoch\n",
    "        self.hidden = (torch.zeros(self.layers_num,self.batch_size,self.hidden_dim),\n",
    "                       torch.zeros(self.layers_num,self.batch_size,self.hidden_dim))\n",
    "    def batch_reset(self,batch_size):\n",
    "        self.hidden = (torch.zeros(self.layers_num,batch_size,self.hidden_dim),\n",
    "                       torch.zeros(self.layers_num,batch_size,self.hidden_dim))\n",
    "    def flatten_parameters(self):\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "static_model = StaticLSTM(HIDDEN_SIZE, STATIC_BATCH_SIZE, LAYERS, static_size, categories=6)\n",
    "static_model = static_model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = split_tensor_dataset(static_tensor_dataset)\n",
    "grid_search(StaticLSTM, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()\n",
    "loss_scores = [float('inf')]\n",
    "learning_rate = 0.01\n",
    "epochs = 2\n",
    "static_model = static_model.to(device)\n",
    "optimizer = optim.Adam(static_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    static_model.hidden_reset()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(static_train_dataloader):\n",
    "        (input, worthless_input), label = batch\n",
    "        if input.shape[0] != STATIC_BATCH_SIZE:\n",
    "            static_model.batch_reset(input.shape[0])\n",
    "            print(\"Reset triggered due to batch size mismatch\")\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = static_model(input).squeeze()\n",
    "        \n",
    "        # Ensure output and label shapes are compatible for the loss function\n",
    "        if output.shape != label.shape:\n",
    "            print(f\"Output shape: {output.shape}, Label shape: {label.shape}\")\n",
    "        \n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print(f\"Batch {i+1}, Loss: {epoch_loss / (i+1)}\")\n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(static_train_dataloader)\n",
    "    print(f\"Average loss for epoch {epoch+1}: {average_epoch_loss}\")\n",
    "    \n",
    "    if average_epoch_loss < loss_scores[-1]:\n",
    "        torch.save(static_model.state_dict(), \"../other_pickle/Static_Model.pth\")\n",
    "        print(\"Model saved\")\n",
    "    loss_scores.append(average_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_model.load_state_dict(torch.load(\"../other_pickle/Static_Model.pth\"))\n",
    "static_model.eval()\n",
    "for i, batch in list(enumerate(static_train_dataloader))[:10]:\n",
    "    (input, worthless_input), label = batch\n",
    "    output = static_model(input).squeeze()\n",
    "    # loss = loss_function(output, label)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_reload = [\"infer_functions\"]\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    __import__(module_name)\n",
    "    module = sys.modules[module_name]\n",
    "    globals().update({name: getattr(module, name) for name in dir(module) if not name.startswith('_')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-09 10:09:37,695] A new study created in memory with name: no-name-8d8a7744-7b1c-4aef-bf19-67483b5b2803\n",
      "[I 2024-06-09 10:09:45,016] Trial 0 finished with value: 0.09690393981455006 and parameters: {'hidden_dim': 279, 'layers': 2, 'batch_size': 16, 'lr': 0.003951547225549357, 'epochs': 1}. Best is trial 0 with value: 0.09690393981455006.\n",
      "[I 2024-06-09 10:09:47,856] Trial 1 finished with value: 0.09667784677979192 and parameters: {'hidden_dim': 109, 'layers': 2, 'batch_size': 32, 'lr': 0.0028993792081484773, 'epochs': 2}. Best is trial 1 with value: 0.09667784677979192.\n",
      "[I 2024-06-09 10:10:01,571] Trial 2 finished with value: 0.09688430104057269 and parameters: {'hidden_dim': 245, 'layers': 3, 'batch_size': 16, 'lr': 0.006709062114691532, 'epochs': 2}. Best is trial 1 with value: 0.09667784677979192.\n",
      "[I 2024-06-09 10:10:10,386] Trial 3 finished with value: 0.09692684511053036 and parameters: {'hidden_dim': 192, 'layers': 2, 'batch_size': 16, 'lr': 0.008198602914504749, 'epochs': 3}. Best is trial 1 with value: 0.09667784677979192.\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = split_tensor_dataset(dynamic_tensor_dataset)\n",
    "best_parameters = grid_search(DynamicLSTM, x_train, y_train, n_trials=4, categories=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 288, 'layers': 3, 'batch_size': 32, 'lr': 0.0014625720043832183, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Batch 10, Loss: 0.02233407294922456\n",
      "Batch 20, Loss: 0.013681545790981168\n",
      "Batch 30, Loss: 0.010033784893149598\n",
      "Batch 40, Loss: 0.008817536557563121\n",
      "Batch 50, Loss: 0.008975304180772047\n",
      "Batch 60, Loss: 0.011723222644648446\n",
      "Batch 70, Loss: 0.01268501901288315\n",
      "Batch 80, Loss: 0.012406697902072116\n",
      "Batch 90, Loss: 0.012234159717179074\n",
      "Batch 100, Loss: 0.01190799550741154\n",
      "Batch 110, Loss: 0.011952834011252\n",
      "Batch 120, Loss: 0.011468859347362682\n",
      "Batch 130, Loss: 0.011435107446323868\n",
      "Batch 140, Loss: 0.011485700943334155\n",
      "Batch 150, Loss: 0.011086994175080552\n",
      "Batch 160, Loss: 0.010751847390329127\n",
      "Batch 170, Loss: 0.010340846027805047\n",
      "Batch 180, Loss: 0.010031015179254022\n",
      "Batch 190, Loss: 0.009697709659195974\n",
      "Batch 200, Loss: 0.009455440548242038\n",
      "Batch 210, Loss: 0.009289782499273105\n",
      "Batch 220, Loss: 0.009041836779820024\n",
      "Batch 230, Loss: 0.00891227074619809\n",
      "Batch 240, Loss: 0.008789619473348189\n",
      "Batch 250, Loss: 0.009316966459515168\n",
      "Batch 260, Loss: 0.009134369367044053\n",
      "Batch 270, Loss: 0.010352465058952764\n",
      "Batch 280, Loss: 0.010571173635543565\n",
      "Batch 290, Loss: 0.01033533793337199\n",
      "Batch 300, Loss: 0.010121688619308886\n",
      "Batch 310, Loss: 0.009918618478107485\n",
      "Batch 320, Loss: 0.009857567137495665\n",
      "Batch 330, Loss: 0.00968759314219923\n",
      "Batch 340, Loss: 0.009577928766507017\n",
      "Batch 350, Loss: 0.01041044757409466\n",
      "Batch 360, Loss: 0.011166811854007514\n",
      "Batch 370, Loss: 0.01113999380866768\n",
      "Batch 380, Loss: 0.010954618443196656\n",
      "Batch 390, Loss: 0.010728832811255569\n",
      "Reset triggered due to batch size mismatch\n",
      "Average loss for epoch 1: 0.010617740065775568\n",
      "Model saved\n",
      "Epoch 2/1\n",
      "Batch 10, Loss: 0.007943474770938059\n",
      "Batch 20, Loss: 0.005689108322335316\n",
      "Batch 30, Loss: 0.004520996521936082\n",
      "Batch 40, Loss: 0.004528488756240969\n",
      "Batch 50, Loss: 0.00553021687263021\n",
      "Batch 60, Loss: 0.00883802686190627\n",
      "Batch 70, Loss: 0.010103194410867884\n",
      "Batch 80, Loss: 0.009973671655500048\n",
      "Batch 90, Loss: 0.010037918001122761\n",
      "Batch 100, Loss: 0.009945523044201184\n",
      "Batch 110, Loss: 0.010079413403860218\n",
      "Batch 120, Loss: 0.009740468632327599\n",
      "Batch 130, Loss: 0.009835894643661122\n",
      "Batch 140, Loss: 0.009961709911965702\n",
      "Batch 150, Loss: 0.009671079198976966\n",
      "Batch 160, Loss: 0.009431114238284131\n",
      "Batch 170, Loss: 0.009099283549264687\n",
      "Batch 180, Loss: 0.008846594790099934\n",
      "Batch 190, Loss: 0.008590775656748283\n",
      "Batch 200, Loss: 0.008379767547272144\n",
      "Batch 210, Loss: 0.00826048259159997\n",
      "Batch 220, Loss: 0.008055748143542443\n",
      "Batch 230, Loss: 0.007965616734921093\n",
      "Batch 240, Loss: 0.007876436587807342\n",
      "Batch 250, Loss: 0.008439745648735758\n",
      "Batch 260, Loss: 0.008288772038971015\n",
      "Batch 270, Loss: 0.009552200292654663\n",
      "Batch 280, Loss: 0.009787979041765413\n",
      "Batch 290, Loss: 0.009564227496992964\n",
      "Batch 300, Loss: 0.009371558332617607\n",
      "Batch 310, Loss: 0.009184199896536705\n",
      "Batch 320, Loss: 0.009142914282522892\n",
      "Batch 330, Loss: 0.008994193671510598\n",
      "Batch 340, Loss: 0.008903160210807786\n",
      "Batch 350, Loss: 0.00974656797265645\n",
      "Batch 360, Loss: 0.010526943754307056\n",
      "Batch 370, Loss: 0.010502843625475223\n",
      "Batch 380, Loss: 0.010327317755099154\n",
      "Batch 390, Loss: 0.010112224018734716\n",
      "Reset triggered due to batch size mismatch\n",
      "Average loss for epoch 2: 0.010008544734788653\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.HuberLoss(delta=1.0)\n",
    "loss_scores = [float('inf')]\n",
    "learning_rate = best_parameters[\"lr\"]\n",
    "for inputs, targets in dynamic_tensor_dataset:\n",
    "    input_size = inputs[0].shape[-1]\n",
    "    break \n",
    "dynamic_model = DynamicLSTM(best_parameters[\"hidden_dim\"], best_parameters[\"batch_size\"], best_parameters[\"layers\"], input=input_size)\n",
    "dynamic_model = dynamic_model.to(device).double()\n",
    "optimizer = optim.Adam(dynamic_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(best_parameters[\"epochs\"]):\n",
    "    print(f\"Epoch {epoch+1}/{best_parameters[\"epochs\"]}\")\n",
    "    dynamic_model.hidden_reset()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dynamic_train_dataloader):\n",
    "        (input, worthless_input), label = batch\n",
    "        if input.shape[0] != DYNAMIC_BATCH_SIZE:\n",
    "            dynamic_model.batch_reset(input.shape[0])\n",
    "            print(\"Reset triggered due to batch size mismatch\")\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = dynamic_model(input).squeeze()\n",
    "        \n",
    "        # Ensure output and label shapes are compatible for the loss function\n",
    "        if output.shape != label.shape:\n",
    "            print(f\"Output shape: {output.shape}, Label shape: {label.shape}\")\n",
    "        \n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Batch {i+1}, Loss: {epoch_loss / (i+1)}\")\n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(dynamic_train_dataloader)\n",
    "    print(f\"Average loss for epoch {epoch+1}: {average_epoch_loss}\")\n",
    "    \n",
    "    if average_epoch_loss < loss_scores[-1]:\n",
    "        torch.save(dynamic_model.state_dict(), \"../other_pickle/Dynamic_Model.pth\")\n",
    "        print(\"Model saved\")\n",
    "    loss_scores.append(average_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_model.load_state_dict(torch.load(\"../other_pickle/Dynamic_Model.pth\"))\n",
    "dynamic_model.eval()\n",
    "for i, batch in list(enumerate(dynamic_train_dataloader))[:100]:\n",
    "    (input, worthless_input), label = batch\n",
    "    output = dynamic_model(input).squeeze()\n",
    "    # loss = loss_function(output, label)\n",
    "    # print(F.softmax(output, dim=1), label)\n",
    "    print(output, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
