{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from infer_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_reload = [\"infer_functions\"]\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    __import__(module_name)\n",
    "    module = sys.modules[module_name]\n",
    "    globals().update({name: getattr(module, name) for name in dir(module) if not name.startswith('_')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catting AAPL.csv\n",
      "Catting ABBV.csv\n",
      "Catting ABT.csv\n",
      "Catting ACN.csv\n",
      "Catting ADBE.csv\n",
      "Catting AMAT.csv\n",
      "Catting AMD.csv\n",
      "Catting AMGN.csv\n",
      "Catting AMZN.csv\n",
      "Catting AVGO.csv\n",
      "Catting BA.csv\n",
      "Catting BKNG.csv\n",
      "Catting CAT.csv\n",
      "Catting CMCSA.csv\n",
      "Catting COP.csv\n",
      "Catting COST.csv\n",
      "Catting CRM.csv\n",
      "Catting CSCO.csv\n",
      "Catting CVX.csv\n",
      "Catting DHR.csv\n",
      "Catting DIS.csv\n",
      "Catting HD.csv\n",
      "Catting HON.csv\n",
      "Catting IBM.csv\n",
      "Catting INTC.csv\n",
      "Catting INTU.csv\n",
      "Catting JNJ.csv\n",
      "Catting KO.csv\n",
      "Catting LIN.csv\n",
      "Catting LLY.csv\n",
      "Catting LOW.csv\n",
      "Catting MA.csv\n",
      "Catting MCD.csv\n",
      "Catting MRK.csv\n",
      "Catting MSFT.csv\n",
      "Catting NKE.csv\n",
      "Catting NOW.csv\n",
      "Catting NVDA.csv\n",
      "Catting ORCL.csv\n",
      "Catting PEP.csv\n",
      "Catting PFE.csv\n",
      "Catting PG.csv\n",
      "Catting PM.csv\n",
      "Catting QCOM.csv\n",
      "Catting RTX.csv\n",
      "Catting SPGI.csv\n",
      "Catting TMO.csv\n",
      "Catting TMUS.csv\n",
      "Catting TSLA.csv\n",
      "Catting TXN.csv\n",
      "Catting UBER.csv\n",
      "Catting UNP.csv\n",
      "Catting UPS.csv\n",
      "Catting V.csv\n",
      "Catting VZ.csv\n",
      "Catting WMT.csv\n",
      "Catting XOM.csv\n"
     ]
    }
   ],
   "source": [
    "static_tensor_dataset = create_tensor_dataset(\"static\", 4, limit=100, categories= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catting AAPL.csv\n",
      "Catting ABBV.csv\n",
      "Catting ABT.csv\n",
      "Catting ACN.csv\n",
      "Catting ADBE.csv\n",
      "Catting AMAT.csv\n",
      "Catting AMD.csv\n",
      "Catting AMGN.csv\n",
      "Catting AMZN.csv\n",
      "Catting AVGO.csv\n",
      "Catting BA.csv\n",
      "Catting BKNG.csv\n",
      "Catting BLK.csv\n",
      "Catting BX.csv\n",
      "Catting CAT.csv\n",
      "Catting COP.csv\n",
      "Catting CRM.csv\n",
      "Catting CSCO.csv\n",
      "Catting CVX.csv\n",
      "Catting DHR.csv\n",
      "Catting DIS.csv\n",
      "Catting GE.csv\n",
      "Catting HD.csv\n",
      "Catting HON.csv\n",
      "Catting IBM.csv\n",
      "Catting INTC.csv\n",
      "Catting INTU.csv\n",
      "Catting JNJ.csv\n",
      "Catting KO.csv\n",
      "Catting LIN.csv\n",
      "Catting LLY.csv\n",
      "Catting LOW.csv\n",
      "Catting MA.csv\n",
      "Catting MCD.csv\n",
      "Catting META.csv\n",
      "Catting MRK.csv\n",
      "Catting MSFT.csv\n",
      "Catting NFLX.csv\n",
      "Catting NOW.csv\n",
      "Catting NVDA.csv\n",
      "Catting ORCL.csv\n",
      "Catting PEP.csv\n",
      "Catting PFE.csv\n",
      "Catting PG.csv\n",
      "Catting PLD.csv\n",
      "Catting PM.csv\n",
      "Catting QCOM.csv\n",
      "Catting RTX.csv\n",
      "Catting SPGI.csv\n",
      "Catting TMO.csv\n",
      "Catting TMUS.csv\n",
      "Catting TSLA.csv\n",
      "Catting TXN.csv\n",
      "Catting UBER.csv\n",
      "Catting UNH.csv\n",
      "Catting UNP.csv\n",
      "Catting VZ.csv\n",
      "Catting WMT.csv\n"
     ]
    }
   ],
   "source": [
    "dynamic_tensor_dataset = create_tensor_dataset(\"dynamic\", 4, limit=100, categories= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977\n",
      "((tensor([[1.5255e-05, 6.9777e-06, 1.1259e-05, 3.5437e-05, 1.7395e-05, 1.7972e-05,\n",
      "         1.6365e-05, 2.3616e-05],\n",
      "        [1.5761e-05, 7.0899e-06, 1.3775e-05, 6.9498e-05, 2.2771e-05, 2.5506e-05,\n",
      "         1.8180e-05, 8.8955e-06],\n",
      "        [2.0827e-05, 7.2160e-06, 1.7574e-05, 3.0669e-05, 3.3413e-05, 3.4662e-05,\n",
      "         2.4180e-05, 4.4263e-05],\n",
      "        [1.5593e-05, 4.6963e-06, 1.0889e-05, 1.3764e-05, 1.7267e-05, 1.6415e-05,\n",
      "         1.6350e-05, 4.9416e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 1.2730e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.7572e-05, 6.9777e-06, 1.9013e-05, 2.7140e-05, 3.5838e-05, 2.3573e-05,\n",
      "         2.2302e-05, 3.0932e-05],\n",
      "        [1.5537e-05, 7.0899e-06, 1.1481e-05, 3.6000e-05, 1.7275e-05, 1.8358e-05,\n",
      "         1.6685e-05, 2.4297e-05],\n",
      "        [1.6040e-05, 7.2160e-06, 1.4142e-05, 7.1264e-05, 2.4004e-05, 2.5966e-05,\n",
      "         1.8489e-05, 9.1133e-06],\n",
      "        [2.1213e-05, 7.3539e-06, 1.7919e-05, 3.0898e-05, 3.4276e-05, 3.5299e-05,\n",
      "         2.4603e-05, 4.5696e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 1.6337e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[3.3433e-05, 9.2020e-06, 2.7435e-05, 2.0996e-06, 5.9791e-05, 5.7413e-05,\n",
      "         3.9157e-05, 5.6327e-05],\n",
      "        [2.6209e-05, 9.3499e-06, 2.2851e-05, 2.7355e-05, 4.4836e-05, 4.5585e-05,\n",
      "         3.0810e-05, 4.9511e-05],\n",
      "        [2.2146e-05, 9.5161e-06, 2.1507e-05, 6.3646e-05, 4.4664e-05, 4.3612e-05,\n",
      "         2.6942e-05, 2.4795e-05],\n",
      "        [2.5964e-05, 9.6980e-06, 2.4993e-05, 5.3985e-05, 5.0349e-05, 5.2741e-05,\n",
      "         3.1848e-05, 6.7294e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.6705e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[3.7375e-05, 1.7914e-05, 3.7005e-05, 5.6580e-05, 8.2397e-05, 8.2299e-05,\n",
      "         4.7083e-05, 6.3479e-05],\n",
      "        [4.2872e-05, 1.8202e-05, 3.8030e-05, 4.3677e-05, 8.2060e-05, 8.3564e-05,\n",
      "         5.2038e-05, 9.9908e-05],\n",
      "        [3.4362e-05, 9.4364e-06, 2.8481e-05, 2.1688e-06, 6.2069e-05, 5.9202e-05,\n",
      "         4.0261e-05, 5.8873e-05],\n",
      "        [2.6940e-05, 9.6168e-06, 2.3721e-05, 2.8023e-05, 4.8080e-05, 4.6865e-05,\n",
      "         3.1616e-05, 5.1927e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 3.2387e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[4.2554e-05, 1.7847e-05, 4.3014e-05, 3.3706e-05, 1.0020e-04, 9.7660e-05,\n",
      "         5.4330e-05, 1.1677e-04],\n",
      "        [3.7921e-05, 1.8134e-05, 3.7593e-05, 5.7262e-05, 8.1520e-05, 8.3750e-05,\n",
      "         4.7822e-05, 6.5064e-05],\n",
      "        [4.3467e-05, 1.8456e-05, 3.8898e-05, 4.4618e-05, 8.6179e-05, 8.4750e-05,\n",
      "         5.2724e-05, 1.0197e-04],\n",
      "        [3.4868e-05, 9.5806e-06, 2.8931e-05, 2.1768e-06, 6.3433e-05, 6.0064e-05,\n",
      "         4.0811e-05, 6.0551e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 3.0179e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[4.3261e-05, 1.7802e-05, 4.0955e-05, 4.0087e-05, 9.0579e-05, 9.0466e-05,\n",
      "         5.3622e-05, 9.9036e-05],\n",
      "        [4.3229e-05, 1.8088e-05, 4.3752e-05, 3.4155e-05, 9.9256e-05, 9.9506e-05,\n",
      "         5.5251e-05, 1.1984e-04],\n",
      "        [3.8495e-05, 1.8409e-05, 3.8498e-05, 5.8569e-05, 8.5718e-05, 8.5044e-05,\n",
      "         4.8512e-05, 6.6489e-05],\n",
      "        [4.4162e-05, 1.8761e-05, 3.9561e-05, 4.4838e-05, 8.8183e-05, 8.6091e-05,\n",
      "         5.3512e-05, 1.0501e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 3.7482e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[5.1422e-05, 2.8138e-05, 5.3501e-05, 5.4669e-05, 1.1965e-04, 1.1918e-04,\n",
      "         6.5864e-05, 9.0914e-05],\n",
      "        [5.3156e-05, 2.8591e-05, 6.7546e-05, 1.7510e-04, 1.5650e-04, 1.6182e-04,\n",
      "         7.5132e-05, 1.4399e-04],\n",
      "        [6.5896e-05, 2.9099e-05, 7.7338e-05, 7.8744e-05, 1.8545e-04, 1.8569e-04,\n",
      "         9.0346e-05, 1.9126e-04],\n",
      "        [4.5281e-05, 1.8601e-05, 4.3345e-05, 4.1711e-05, 9.6446e-05, 9.4987e-05,\n",
      "         5.6089e-05, 1.0685e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 5.4430e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[5.4732e-05, 2.8068e-05, 5.1256e-05, 9.1109e-05, 1.1122e-04, 1.1242e-04,\n",
      "         6.7469e-05, 6.3279e-05],\n",
      "        [5.2239e-05, 2.8519e-05, 5.4420e-05, 5.5397e-05, 1.1853e-04, 1.2143e-04,\n",
      "         6.6983e-05, 9.3302e-05],\n",
      "        [5.3962e-05, 2.9026e-05, 6.9174e-05, 1.7910e-04, 1.6457e-04, 1.6433e-04,\n",
      "         7.6219e-05, 1.4715e-04],\n",
      "        [6.6950e-05, 2.9581e-05, 7.8657e-05, 7.9133e-05, 1.8977e-04, 1.8863e-04,\n",
      "         9.1697e-05, 1.9696e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 6.5627e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[6.7787e-05, 6.3708e-05, 5.8087e-05, 1.2092e-04, 1.2891e-04, 1.2877e-04,\n",
      "         8.1651e-05, 1.1683e-04],\n",
      "        [8.2951e-05, 6.4733e-05, 7.6301e-05, 9.8046e-05, 1.7536e-04, 1.8026e-04,\n",
      "         1.0407e-04, 2.4174e-04],\n",
      "        [5.6628e-05, 2.8975e-05, 5.3567e-05, 9.4739e-05, 1.1623e-04, 1.1670e-04,\n",
      "         6.9833e-05, 6.6581e-05],\n",
      "        [5.4055e-05, 2.9529e-05, 5.6867e-05, 5.7128e-05, 1.2795e-04, 1.2567e-04,\n",
      "         6.9192e-05, 9.8509e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 4.1568e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[5.7062e-05, 6.3736e-05, 4.6294e-05, 1.4969e-04, 9.3205e-05, 9.4389e-05,\n",
      "         6.6175e-05, 6.6180e-05],\n",
      "        [6.9067e-05, 6.4761e-05, 5.9259e-05, 1.2290e-04, 1.2807e-04, 1.3159e-04,\n",
      "         8.3282e-05, 1.2025e-04],\n",
      "        [8.4456e-05, 6.5912e-05, 7.8371e-05, 1.0058e-04, 1.8494e-04, 1.8359e-04,\n",
      "         1.0589e-04, 2.4776e-04],\n",
      "        [5.7703e-05, 2.9542e-05, 5.4642e-05, 9.5488e-05, 1.1929e-04, 1.1889e-04,\n",
      "         7.1086e-05, 6.8767e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 3.7220e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[6.1935e-05, 6.5852e-05, 5.0941e-05, 1.0240e-04, 1.0484e-04, 1.0631e-04,\n",
      "         7.2530e-05, 9.1502e-05],\n",
      "        [6.0043e-05, 6.6910e-05, 4.8774e-05, 1.5711e-04, 9.5633e-05, 9.9615e-05,\n",
      "         6.9706e-05, 7.0348e-05],\n",
      "        [7.2623e-05, 6.8100e-05, 6.2859e-05, 1.3020e-04, 1.3949e-04, 1.3841e-04,\n",
      "         8.7509e-05, 1.2728e-04],\n",
      "        [8.8877e-05, 6.9401e-05, 8.2560e-05, 1.0469e-04, 1.9601e-04, 1.9317e-04,\n",
      "         1.1132e-04, 2.6428e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 4.3858e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[7.3681e-05, 7.9791e-05, 6.7130e-05, 1.0655e-04, 1.4532e-04, 1.4674e-04,\n",
      "         8.9987e-05, 1.4245e-04],\n",
      "        [9.3951e-05, 8.1073e-05, 8.3326e-05, 1.8498e-04, 1.8453e-04, 1.9256e-04,\n",
      "         1.1576e-04, 2.4938e-04],\n",
      "        [6.5382e-05, 6.9360e-05, 5.4318e-05, 1.0864e-04, 1.1179e-04, 1.1259e-04,\n",
      "         7.6596e-05, 9.8231e-05],\n",
      "        [6.3391e-05, 7.0686e-05, 5.2002e-05, 1.6531e-04, 1.0533e-04, 1.0519e-04,\n",
      "         7.3467e-05, 7.5781e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 4.7887e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[6.4628e-05, 8.2626e-05, 5.7074e-05, 1.4719e-04, 1.1405e-04, 1.1494e-04,\n",
      "         7.6416e-05, 9.5584e-05],\n",
      "        [7.7706e-05, 8.3954e-05, 7.0887e-05, 1.1209e-04, 1.4944e-04, 1.5522e-04,\n",
      "         9.5005e-05, 1.5176e-04],\n",
      "        [9.9012e-05, 8.5447e-05, 8.8588e-05, 1.9642e-04, 2.0143e-04, 2.0300e-04,\n",
      "         1.2191e-04, 2.6457e-04],\n",
      "        [6.8961e-05, 7.3197e-05, 5.7352e-05, 1.1334e-04, 1.1875e-04, 1.1874e-04,\n",
      "         8.0706e-05, 1.0502e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 5.5461e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[7.4207e-05, 8.3203e-05, 6.2441e-05, 1.1285e-04, 1.2550e-04, 1.2568e-04,\n",
      "         8.6593e-05, 1.1515e-04],\n",
      "        [6.6279e-05, 8.4540e-05, 5.8606e-05, 1.5057e-04, 1.1405e-04, 1.1823e-04,\n",
      "         7.8452e-05, 9.9027e-05],\n",
      "        [7.9634e-05, 8.6043e-05, 7.3285e-05, 1.1574e-04, 1.5863e-04, 1.5912e-04,\n",
      "         9.7294e-05, 1.5656e-04],\n",
      "        [1.0155e-04, 8.7687e-05, 9.0956e-05, 1.9926e-04, 2.0808e-04, 2.0818e-04,\n",
      "         1.2491e-04, 2.7504e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 6.0328e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.2323e-04, 1.1326e-04, 1.1844e-04, 2.2144e-04, 2.7276e-04, 2.7866e-04,\n",
      "         1.5657e-04, 3.8057e-04],\n",
      "        [7.7160e-05, 8.6314e-05, 6.5009e-05, 1.1704e-04, 1.2725e-04, 1.3107e-04,\n",
      "         9.0136e-05, 1.2096e-04],\n",
      "        [6.8868e-05, 8.7848e-05, 6.1431e-05, 1.5764e-04, 1.2275e-04, 1.2288e-04,\n",
      "         8.1460e-05, 1.0358e-04],\n",
      "        [8.2812e-05, 8.9527e-05, 7.6290e-05, 1.1905e-04, 1.6615e-04, 1.6544e-04,\n",
      "         1.0107e-04, 1.6502e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 6.6853e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[9.7905e-05, 1.1404e-04, 9.4852e-05, 1.8378e-04, 2.0676e-04, 2.1151e-04,\n",
      "         1.2259e-04, 2.0993e-04],\n",
      "        [1.2636e-04, 1.1588e-04, 1.2160e-04, 2.2651e-04, 2.7274e-04, 2.8660e-04,\n",
      "         1.6073e-04, 3.9424e-04],\n",
      "        [7.9067e-05, 8.8453e-05, 6.7202e-05, 1.2084e-04, 1.3506e-04, 1.3435e-04,\n",
      "         9.2299e-05, 1.2478e-04],\n",
      "        [7.0627e-05, 9.0143e-05, 6.3067e-05, 1.5991e-04, 1.2679e-04, 1.2600e-04,\n",
      "         8.3456e-05, 1.0767e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 7.1790e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[8.8499e-05, 1.1530e-04, 7.9786e-05, 2.0661e-04, 1.6449e-04, 1.6477e-04,\n",
      "         1.0599e-04, 1.6441e-04],\n",
      "        [1.0081e-04, 1.1716e-04, 9.7793e-05, 1.8876e-04, 2.0760e-04, 2.1844e-04,\n",
      "         1.2637e-04, 2.1837e-04],\n",
      "        [1.3002e-04, 1.1924e-04, 1.2623e-04, 2.3483e-04, 2.9069e-04, 2.9499e-04,\n",
      "         1.6527e-04, 4.0836e-04],\n",
      "        [8.1424e-05, 9.1140e-05, 6.9278e-05, 1.2309e-04, 1.4009e-04, 1.3834e-04,\n",
      "         9.4954e-05, 1.3024e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 7.3021e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[9.2817e-05, 1.1648e-04, 8.4153e-05, 1.5587e-04, 1.7313e-04, 1.7284e-04,\n",
      "         1.1117e-04, 1.2647e-04],\n",
      "        [9.1053e-05, 1.1836e-04, 8.2192e-05, 2.1204e-04, 1.6502e-04, 1.7003e-04,\n",
      "         1.0916e-04, 1.7088e-04],\n",
      "        [1.0365e-04, 1.2046e-04, 1.0143e-04, 1.9554e-04, 2.2108e-04, 2.2465e-04,\n",
      "         1.2983e-04, 2.2601e-04],\n",
      "        [1.3379e-04, 1.2276e-04, 1.3002e-04, 2.3900e-04, 3.0126e-04, 3.0349e-04,\n",
      "         1.6988e-04, 4.2590e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 6.5416e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.3310e-04, 1.0749e-04, 1.2744e-04, 2.0459e-04, 2.9229e-04, 2.9222e-04,\n",
      "         1.6751e-04, 3.1301e-04],\n",
      "        [9.6688e-05, 1.2106e-04, 8.7774e-05, 1.6196e-04, 1.7586e-04, 1.8058e-04,\n",
      "         1.1593e-04, 1.3309e-04],\n",
      "        [9.4782e-05, 1.2321e-04, 8.6312e-05, 2.2239e-04, 1.7794e-04, 1.7705e-04,\n",
      "         1.1356e-04, 1.7906e-04],\n",
      "        [1.0798e-04, 1.2557e-04, 1.0578e-04, 2.0150e-04, 2.3198e-04, 2.3401e-04,\n",
      "         1.3512e-04, 2.3866e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 6.0230e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[9.4667e-05, 1.0809e-04, 8.3912e-05, 1.9567e-04, 1.6834e-04, 1.7004e-04,\n",
      "         1.1224e-04, 1.2226e-04],\n",
      "        [1.3630e-04, 1.0982e-04, 1.3068e-04, 2.0898e-04, 2.9188e-04, 3.0015e-04,\n",
      "         1.7173e-04, 3.2382e-04],\n",
      "        [9.8946e-05, 1.2390e-04, 9.0614e-05, 1.6700e-04, 1.8641e-04, 1.8486e-04,\n",
      "         1.1855e-04, 1.3710e-04],\n",
      "        [9.7075e-05, 1.2626e-04, 8.8493e-05, 2.2530e-04, 1.8355e-04, 1.8130e-04,\n",
      "         1.1618e-04, 1.8589e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 5.8590e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[8.4516e-05, 1.0941e-04, 6.8674e-05, 1.4463e-04, 1.2632e-04, 1.2435e-04,\n",
      "         9.5191e-05, 1.0453e-04],\n",
      "        [9.7595e-05, 1.1117e-04, 8.6616e-05, 2.0121e-04, 1.6922e-04, 1.7582e-04,\n",
      "         1.1584e-04, 1.2733e-04],\n",
      "        [1.4042e-04, 1.1315e-04, 1.3581e-04, 2.1692e-04, 3.1146e-04, 3.0931e-04,\n",
      "         1.7679e-04, 3.3582e-04],\n",
      "        [1.0202e-04, 1.2781e-04, 9.3524e-05, 1.7030e-04, 1.9358e-04, 1.9056e-04,\n",
      "         1.2210e-04, 1.4328e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 5.1159e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[9.3471e-05, 1.1122e-04, 7.7207e-05, 9.0578e-05, 1.4847e-04, 1.4712e-04,\n",
      "         1.0703e-04, 1.6497e-04],\n",
      "        [8.7495e-05, 1.1301e-04, 7.1185e-05, 1.4935e-04, 1.2752e-04, 1.2912e-04,\n",
      "         9.8652e-05, 1.0932e-04],\n",
      "        [1.0096e-04, 1.1501e-04, 9.0395e-05, 2.0973e-04, 1.8133e-04, 1.8194e-04,\n",
      "         1.1975e-04, 1.3260e-04],\n",
      "        [1.4538e-04, 1.1721e-04, 1.4076e-04, 2.2215e-04, 3.2479e-04, 3.2019e-04,\n",
      "         1.8286e-04, 3.5242e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 6.0733e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.4802e-04, 1.0893e-04, 1.3217e-04, 2.2096e-04, 2.9779e-04, 2.9527e-04,\n",
      "         1.8087e-04, 3.2795e-04],\n",
      "        [9.6196e-05, 1.1420e-04, 7.9558e-05, 9.2984e-05, 1.4900e-04, 1.5186e-04,\n",
      "         1.1027e-04, 1.7151e-04],\n",
      "        [8.9982e-05, 1.1623e-04, 7.3853e-05, 1.5476e-04, 1.3584e-04, 1.3283e-04,\n",
      "         1.0139e-04, 1.1317e-04],\n",
      "        [1.0392e-04, 1.1845e-04, 9.3138e-05, 2.1352e-04, 1.8798e-04, 1.8724e-04,\n",
      "         1.2313e-04, 1.3834e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 6.2132e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.0614e-04, 1.1071e-04, 9.1661e-05, 2.1549e-04, 1.8658e-04, 1.8111e-04,\n",
      "         1.2410e-04, 1.3067e-04],\n",
      "        [1.5322e-04, 1.1249e-04, 1.3698e-04, 2.2814e-04, 3.0057e-04, 3.0654e-04,\n",
      "         1.8742e-04, 3.4292e-04],\n",
      "        [9.9500e-05, 1.1813e-04, 8.3016e-05, 9.6905e-05, 1.5964e-04, 1.5712e-04,\n",
      "         1.1398e-04, 1.7858e-04],\n",
      "        [9.3149e-05, 1.2038e-04, 7.6533e-05, 1.5846e-04, 1.4163e-04, 1.3748e-04,\n",
      "         1.0485e-04, 1.1875e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 7.3787e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[9.5359e-05, 1.1141e-04, 7.8336e-05, 1.8503e-04, 1.4839e-04, 1.3921e-04,\n",
      "         1.0720e-04, 8.5407e-05],\n",
      "        [1.0878e-04, 1.1320e-04, 9.4056e-05, 2.2028e-04, 1.8645e-04, 1.8616e-04,\n",
      "         1.2732e-04, 1.3528e-04],\n",
      "        [1.5691e-04, 1.1521e-04, 1.4151e-04, 2.3540e-04, 3.1883e-04, 3.1403e-04,\n",
      "         1.9180e-04, 3.5352e-04],\n",
      "        [1.0198e-04, 1.2114e-04, 8.5175e-05, 9.8241e-05, 1.6479e-04, 1.6102e-04,\n",
      "         1.1670e-04, 1.8552e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 7.6268e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.0965e-04, 1.1246e-04, 9.0120e-05, 1.1951e-04, 1.8410e-04, 1.7121e-04,\n",
      "         1.2530e-04, 1.6702e-04],\n",
      "        [9.8032e-05, 1.1427e-04, 8.0634e-05, 1.8974e-04, 1.4875e-04, 1.4353e-04,\n",
      "         1.1033e-04, 8.8697e-05],\n",
      "        [1.1175e-04, 1.1630e-04, 9.7473e-05, 2.2800e-04, 1.9840e-04, 1.9130e-04,\n",
      "         1.3070e-04, 1.3990e-04],\n",
      "        [1.6132e-04, 1.1852e-04, 1.4565e-04, 2.3939e-04, 3.3015e-04, 3.2281e-04,\n",
      "         1.9699e-04, 3.6840e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 7.8455e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.7337e-04, 1.5399e-04, 1.5426e-04, 2.3216e-04, 3.4686e-04, 3.4493e-04,\n",
      "         2.1168e-04, 3.6315e-04],\n",
      "        [1.1234e-04, 1.1495e-04, 9.2451e-05, 1.2214e-04, 1.8393e-04, 1.7594e-04,\n",
      "         1.2852e-04, 1.7288e-04],\n",
      "        [1.0037e-04, 1.1700e-04, 8.3282e-05, 1.9573e-04, 1.5775e-04, 1.4700e-04,\n",
      "         1.1288e-04, 9.1416e-05],\n",
      "        [1.1450e-04, 1.1923e-04, 9.9983e-05, 2.3109e-04, 2.0475e-04, 1.9599e-04,\n",
      "         1.3379e-04, 1.4530e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 8.6888e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.2798e-04, 1.5548e-04, 1.0781e-04, 1.8451e-04, 2.4178e-04, 2.1114e-04,\n",
      "         1.4832e-04, 1.5768e-04],\n",
      "        [1.7867e-04, 1.5832e-04, 1.5917e-04, 2.3865e-04, 3.4855e-04, 3.5652e-04,\n",
      "         2.1838e-04, 3.7806e-04],\n",
      "        [1.1569e-04, 1.1839e-04, 9.6045e-05, 1.2673e-04, 1.9619e-04, 1.8124e-04,\n",
      "         1.3226e-04, 1.7921e-04],\n",
      "        [1.0344e-04, 1.2065e-04, 8.5925e-05, 1.9954e-04, 1.6375e-04, 1.5148e-04,\n",
      "         1.1622e-04, 9.5498e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 8.5132e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.1871e-04, 1.5617e-04, 9.7034e-05, 1.6930e-04, 2.0800e-04, 1.7296e-04,\n",
      "         1.3340e-04, 1.6704e-04],\n",
      "        [1.3455e-04, 1.6308e-04, 1.1349e-04, 1.9350e-04, 2.4787e-04, 2.2264e-04,\n",
      "         1.5610e-04, 1.6748e-04],\n",
      "        [1.8771e-04, 1.6635e-04, 1.6870e-04, 2.5262e-04, 3.7931e-04, 3.7468e-04,\n",
      "         2.2927e-04, 3.9984e-04],\n",
      "        [1.2164e-04, 1.2455e-04, 1.0109e-04, 1.3180e-04, 2.0777e-04, 1.9054e-04,\n",
      "         1.3892e-04, 1.9100e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 9.0984e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0001, 0.0002, 0.0001, 0.0001, 0.0003, 0.0002, 0.0002, 0.0002],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0003, 0.0004, 0.0004, 0.0002, 0.0004]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 1.0903e+12], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0002, 0.0002, 0.0002, 0.0003, 0.0004, 0.0003, 0.0002, 0.0004],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0001, 0.0003, 0.0002, 0.0002, 0.0003],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 7.4137e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0001, 0.0002, 0.0001, 0.0003, 0.0002, 0.0002, 0.0002, 0.0001],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0004, 0.0004, 0.0003, 0.0002, 0.0004],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0001, 0.0003, 0.0002, 0.0002, 0.0003],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 8.9567e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0003, 0.0002, 0.0002, 0.0002, 0.0001],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0004, 0.0004, 0.0004, 0.0002, 0.0004],\n",
      "        [0.0002, 0.0002, 0.0001, 0.0001, 0.0003, 0.0003, 0.0002, 0.0003]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 9.1064e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0002, 0.0002, 0.0001, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0004, 0.0004, 0.0004, 0.0002, 0.0004]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 9.8889e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0002, 0.0002, 0.0002, 0.0003, 0.0004, 0.0004, 0.0003, 0.0005],\n",
      "        [0.0002, 0.0002, 0.0001, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0001, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 1.2877e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0003, 0.0004, 0.0004, 0.0003, 0.0005],\n",
      "        [0.0002, 0.0002, 0.0001, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003],\n",
      "        [0.0001, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 1.0840e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.5429e-04, 1.8287e-04, 1.2221e-04, 7.0089e-05, 2.3043e-04, 2.0358e-04,\n",
      "         1.6950e-04, 2.4825e-04],\n",
      "        [1.5332e-04, 1.8811e-04, 1.2292e-04, 2.4484e-04, 2.2876e-04, 2.0417e-04,\n",
      "         1.6884e-04, 1.9900e-04],\n",
      "        [2.2739e-04, 1.9351e-04, 1.9867e-04, 3.5440e-04, 4.7667e-04, 4.1349e-04,\n",
      "         2.7038e-04, 5.0548e-04],\n",
      "        [1.6925e-04, 2.2264e-04, 1.3985e-04, 2.8187e-04, 3.0097e-04, 2.5732e-04,\n",
      "         1.9188e-04, 3.1472e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 1.5328e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.6759e-04, 1.8202e-04, 1.3486e-04, 8.5654e-05, 2.6307e-04, 2.3292e-04,\n",
      "         1.8626e-04, 3.2158e-04],\n",
      "        [1.5930e-04, 1.8836e-04, 1.2633e-04, 7.2178e-05, 2.3198e-04, 2.1081e-04,\n",
      "         1.7519e-04, 2.5892e-04],\n",
      "        [1.5817e-04, 1.9408e-04, 1.2793e-04, 2.5450e-04, 2.4446e-04, 2.1070e-04,\n",
      "         1.7407e-04, 2.0667e-04],\n",
      "        [2.3478e-04, 1.9992e-04, 2.0534e-04, 3.6195e-04, 4.9570e-04, 4.2687e-04,\n",
      "         2.7888e-04, 5.2902e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 1.9203e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.6307e-04, 1.8066e-04, 2.4357e-04, 1.4113e-04, 6.0044e-04, 5.3179e-04,\n",
      "         3.2273e-04, 6.0702e-04],\n",
      "        [1.7169e-04, 1.8604e-04, 1.3834e-04, 8.7528e-05, 2.6280e-04, 2.3933e-04,\n",
      "         1.9103e-04, 3.3282e-04],\n",
      "        [1.6308e-04, 1.9285e-04, 1.3047e-04, 7.4451e-05, 2.4599e-04, 2.1588e-04,\n",
      "         1.7922e-04, 2.6683e-04],\n",
      "        [1.6206e-04, 1.9896e-04, 1.3121e-04, 2.5792e-04, 2.5226e-04, 2.1585e-04,\n",
      "         1.7816e-04, 2.1463e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.2437e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.1231e-04, 1.9194e-04, 2.1190e-04, 6.7895e-04, 4.9971e-04, 4.4170e-04,\n",
      "         2.6274e-04, 3.7851e-04],\n",
      "        [2.7133e-04, 1.8590e-04, 2.5154e-04, 1.4519e-04, 6.0388e-04, 5.5012e-04,\n",
      "         3.3322e-04, 6.3248e-04],\n",
      "        [1.7695e-04, 1.9176e-04, 1.4383e-04, 9.0895e-05, 2.8056e-04, 2.4675e-04,\n",
      "         1.9675e-04, 3.4531e-04],\n",
      "        [1.6822e-04, 1.9904e-04, 1.3472e-04, 7.5961e-05, 2.5556e-04, 2.2265e-04,\n",
      "         1.8468e-04, 2.7898e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.0349e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.9716e-04, 1.9552e-04, 1.9736e-04, 6.6461e-04, 4.6259e-04, 3.8980e-04,\n",
      "         2.4027e-04, 3.3324e-04],\n",
      "        [2.1752e-04, 1.9620e-04, 2.1738e-04, 6.9387e-04, 4.9924e-04, 4.5390e-04,\n",
      "         2.6948e-04, 3.9177e-04],\n",
      "        [2.7780e-04, 1.9034e-04, 2.5980e-04, 1.4978e-04, 6.4042e-04, 5.6340e-04,\n",
      "         3.4093e-04, 6.5187e-04],\n",
      "        [1.8132e-04, 1.9660e-04, 1.4754e-04, 9.2125e-05, 2.8954e-04, 2.5280e-04,\n",
      "         2.0139e-04, 3.5864e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.2213e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0002, 0.0002, 0.0002, 0.0006, 0.0004, 0.0004, 0.0002, 0.0003],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0007, 0.0005, 0.0004, 0.0002, 0.0003],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0007, 0.0005, 0.0005, 0.0003, 0.0004],\n",
      "        [0.0003, 0.0002, 0.0003, 0.0002, 0.0007, 0.0006, 0.0004, 0.0007]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.4286e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0003, 0.0002, 0.0003, 0.0004, 0.0007, 0.0007, 0.0004, 0.0008],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0006, 0.0004, 0.0004, 0.0003, 0.0003],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0007, 0.0005, 0.0004, 0.0003, 0.0004],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0007, 0.0005, 0.0005, 0.0003, 0.0004]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.8921e+12], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0002, 0.0002, 0.0002, 0.0003, 0.0005, 0.0005, 0.0003, 0.0005],\n",
      "        [0.0003, 0.0002, 0.0003, 0.0004, 0.0007, 0.0007, 0.0004, 0.0008],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0006, 0.0005, 0.0004, 0.0003, 0.0003],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0007, 0.0005, 0.0004, 0.0003, 0.0004]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.8513e+12], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0002, 0.0002, 0.0002, 0.0002, 0.0004, 0.0004, 0.0003, 0.0004],\n",
      "        [0.0002, 0.0002, 0.0003, 0.0003, 0.0005, 0.0005, 0.0003, 0.0005],\n",
      "        [0.0003, 0.0002, 0.0003, 0.0005, 0.0008, 0.0007, 0.0004, 0.0008],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0006, 0.0005, 0.0004, 0.0003, 0.0003]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.2928e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0002, 0.0002, 0.0002, 0.0006, 0.0005, 0.0004, 0.0003, 0.0004],\n",
      "        [0.0002, 0.0002, 0.0002, 0.0003, 0.0004, 0.0004, 0.0003, 0.0004],\n",
      "        [0.0002, 0.0002, 0.0003, 0.0004, 0.0006, 0.0005, 0.0003, 0.0005],\n",
      "        [0.0003, 0.0002, 0.0003, 0.0005, 0.0008, 0.0007, 0.0004, 0.0009]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.4175e+12], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[2.9283e-04, 2.1118e-04, 2.9557e-04, 6.9886e-05, 6.6946e-04, 6.1041e-04,\n",
      "         3.6260e-04, 5.5594e-04],\n",
      "        [2.3984e-04, 2.1082e-04, 2.2813e-04, 6.2781e-04, 4.5923e-04, 4.3097e-04,\n",
      "         2.8446e-04, 3.9442e-04],\n",
      "        [2.2400e-04, 2.1008e-04, 2.2062e-04, 2.5937e-04, 4.5422e-04, 4.0670e-04,\n",
      "         2.6623e-04, 4.0315e-04],\n",
      "        [2.5641e-04, 2.0890e-04, 2.6679e-04, 3.6016e-04, 5.9941e-04, 5.3807e-04,\n",
      "         3.1765e-04, 5.1354e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.0669e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.4137e-04, 2.1102e-04, 2.4785e-04, 3.4506e-04, 5.4211e-04, 4.8256e-04,\n",
      "         2.9513e-04, 4.7436e-04],\n",
      "        [2.9985e-04, 2.1574e-04, 3.0305e-04, 7.1382e-05, 6.6846e-04, 6.2691e-04,\n",
      "         3.7170e-04, 5.7509e-04],\n",
      "        [2.4541e-04, 2.1574e-04, 2.3549e-04, 6.4727e-04, 4.8674e-04, 4.4113e-04,\n",
      "         2.9087e-04, 4.0628e-04],\n",
      "        [2.2940e-04, 2.1526e-04, 2.2618e-04, 2.6273e-04, 4.6850e-04, 4.1643e-04,\n",
      "         2.7237e-04, 4.1847e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 2.6090e+12], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.1463e-04, 2.2355e-04, 2.1627e-04, 1.8149e-04, 4.4874e-04, 3.9422e-04,\n",
      "         2.5606e-04, 4.5192e-04],\n",
      "        [2.4728e-04, 2.1568e-04, 2.5423e-04, 3.5261e-04, 5.4156e-04, 4.9584e-04,\n",
      "         3.0267e-04, 4.9093e-04],\n",
      "        [3.0697e-04, 2.2088e-04, 3.1297e-04, 7.3630e-05, 7.0883e-04, 6.4200e-04,\n",
      "         3.8026e-04, 5.9266e-04],\n",
      "        [2.5144e-04, 2.2117e-04, 2.4153e-04, 6.5597e-04, 5.0228e-04, 4.5190e-04,\n",
      "         2.9772e-04, 4.2193e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 4.0000e+00, 3.0509e+12], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[1.5056e-04, 8.5275e-05, 2.8145e-04, 2.2752e-04, 3.2945e-04, 3.4849e-04,\n",
      "         1.9276e-04, 3.2548e-04],\n",
      "        [1.4518e-04, 7.3040e-05, 2.7837e-04, 2.3113e-04, 2.6722e-04, 3.1757e-04,\n",
      "         1.8253e-04, 3.7355e-04],\n",
      "        [1.3188e-04, 7.5796e-05, 2.7276e-04, 2.3700e-04, 3.1056e-04, 3.1763e-04,\n",
      "         1.7098e-04, 3.2401e-04],\n",
      "        [1.2432e-04, 6.6846e-05, 2.4998e-04, 2.3877e-04, 2.3836e-04, 2.9465e-04,\n",
      "         1.6015e-04, 2.8053e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 9.6842e+10], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[1.3063e-04, 7.3370e-05, 2.6507e-04, 7.3746e-04, 2.9721e-04, 3.7975e-04,\n",
      "         1.8138e-04, 3.6318e-04],\n",
      "        [1.5498e-04, 8.7577e-05, 2.9008e-04, 2.3361e-04, 3.3069e-04, 3.5980e-04,\n",
      "         1.9864e-04, 3.3847e-04],\n",
      "        [1.4934e-04, 7.5138e-05, 2.8887e-04, 2.3955e-04, 2.8472e-04, 3.2676e-04,\n",
      "         1.8762e-04, 3.8681e-04],\n",
      "        [1.3577e-04, 7.8075e-05, 2.8110e-04, 2.4133e-04, 3.2201e-04, 3.2695e-04,\n",
      "         1.7584e-04, 3.3810e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 9.2384e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.4331e-04, 7.6403e-05, 2.8952e-04, 7.3240e-04, 3.5098e-04, 3.9518e-04,\n",
      "         1.9507e-04, 3.2115e-04],\n",
      "        [1.3213e-04, 7.4037e-05, 2.6844e-04, 7.4402e-04, 2.9313e-04, 3.8524e-04,\n",
      "         1.8365e-04, 3.7109e-04],\n",
      "        [1.5664e-04, 8.8522e-05, 2.9578e-04, 2.3790e-04, 3.4620e-04, 3.6377e-04,\n",
      "         2.0063e-04, 3.4437e-04],\n",
      "        [1.5106e-04, 7.6048e-05, 2.9251e-04, 2.3968e-04, 2.9007e-04, 3.3049e-04,\n",
      "         1.8960e-04, 3.9659e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.0082e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.1380e-04, 7.2888e-05, 3.1945e-04, 3.5661e-04, 1.1564e-05, 3.0298e-04,\n",
      "         2.3869e-04, 4.4035e-04],\n",
      "        [1.5713e-04, 8.1393e-05, 3.2098e-04, 3.6227e-04, 3.6020e-04, 4.6665e-04,\n",
      "         2.1995e-04, 5.9320e-04],\n",
      "        [1.5916e-04, 8.2093e-05, 3.3181e-04, 3.7148e-04, 4.4583e-04, 4.6961e-04,\n",
      "         2.2206e-04, 3.6270e-04],\n",
      "        [1.5638e-04, 7.8338e-05, 3.0746e-04, 3.7425e-04, 4.0863e-04, 4.3264e-04,\n",
      "         2.1274e-04, 4.0038e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.5439e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.8433e-04, 9.1929e-05, 4.2313e-04, 3.2845e-04, 6.7244e-04, 7.2125e-04,\n",
      "         2.8980e-04, 6.1589e-04],\n",
      "        [2.3178e-04, 9.0240e-05, 4.2252e-04, 3.3367e-04, 4.4916e-04, 4.8737e-04,\n",
      "         2.8782e-04, 8.9611e-04],\n",
      "        [1.9536e-04, 9.1845e-05, 4.2565e-04, 3.4214e-04, 1.8623e-04, 6.4460e-04,\n",
      "         2.8498e-04, 4.9000e-04],\n",
      "        [1.9747e-04, 9.6884e-05, 4.1364e-04, 3.4470e-04, 6.3320e-04, 5.8116e-04,\n",
      "         2.7497e-04, 6.2669e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.3093e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.9469e-04, 8.9571e-05, 4.2170e-04, 3.7885e-04, 7.2245e-04, 6.5674e-04,\n",
      "         2.8690e-04, 7.3012e-04],\n",
      "        [1.8769e-04, 9.3386e-05, 4.3138e-04, 3.3359e-04, 6.6763e-04, 7.3657e-04,\n",
      "         2.9539e-04, 6.3351e-04],\n",
      "        [2.3583e-04, 9.1824e-05, 4.3369e-04, 3.4207e-04, 4.7338e-04, 4.9604e-04,\n",
      "         2.9265e-04, 9.1785e-04],\n",
      "        [1.9894e-04, 9.3579e-05, 4.3390e-04, 3.4462e-04, 1.9100e-04, 6.5631e-04,\n",
      "         2.8990e-04, 5.0576e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.1270e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 3.7606e-04,  1.1624e-04,  4.2473e-04,  3.7947e-04, -1.7742e-04,\n",
      "          1.3730e-04,  3.4759e-04,  5.7712e-04],\n",
      "        [ 1.9861e-04,  9.1160e-05,  4.3073e-04,  3.8549e-04,  7.1862e-04,\n",
      "          6.7194e-04,  2.9299e-04,  7.5240e-04],\n",
      "        [ 1.9132e-04,  9.5201e-05,  4.4360e-04,  3.4263e-04,  7.0494e-04,\n",
      "          7.5107e-04,  3.0091e-04,  6.5008e-04],\n",
      "        [ 2.4059e-04,  9.3732e-05,  4.4292e-04,  3.4518e-04,  4.8641e-04,\n",
      "          5.0599e-04,  2.9826e-04,  9.4912e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.4499e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[ 3.1383e-04,  1.1424e-04,  4.1564e-04,  3.1753e-04,  4.6429e-04,\n",
      "          4.9727e-04,  3.5996e-04,  9.3083e-04],\n",
      "        [ 3.2048e-04,  9.8831e-05,  3.6241e-04,  3.2257e-04, -1.4743e-04,\n",
      "          1.1735e-04,  2.9653e-04,  4.9684e-04],\n",
      "        [ 1.6913e-04,  7.7635e-05,  3.7003e-04,  3.3076e-04,  6.3388e-04,\n",
      "          5.7239e-04,  2.4933e-04,  6.4499e-04],\n",
      "        [ 1.6306e-04,  8.1183e-05,  3.7847e-04,  2.8884e-04,  6.0511e-04,\n",
      "          6.4002e-04,  2.5619e-04,  5.6158e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.5458e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 3.2861e-04,  1.4813e-04,  4.8544e-04,  3.1741e-04,  7.2393e-06,\n",
      "          5.7314e-04,  3.8649e-04,  7.5842e-04],\n",
      "        [ 3.1950e-04,  1.1603e-04,  4.2369e-04,  3.2245e-04,  4.6091e-04,\n",
      "          5.0776e-04,  3.6685e-04,  9.5733e-04],\n",
      "        [ 3.2603e-04,  1.0055e-04,  3.7194e-04,  3.3064e-04, -1.5536e-04,\n",
      "          1.1942e-04,  3.0146e-04,  5.0882e-04],\n",
      "        [ 1.7220e-04,  7.9090e-05,  3.7715e-04,  3.3311e-04,  6.5003e-04,\n",
      "          5.8270e-04,  2.5360e-04,  6.6564e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.8585e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.8958e-04,  1.3440e-04,  4.6539e-04,  6.9345e-04,  7.1431e-04,\n",
      "          6.2645e-04,  3.6275e-04,  7.7714e-04],\n",
      "        [ 3.3459e-04,  1.5048e-04,  4.9491e-04,  3.2237e-04,  7.1876e-06,\n",
      "          5.8531e-04,  3.9394e-04,  7.8011e-04],\n",
      "        [ 3.2508e-04,  1.1807e-04,  4.3488e-04,  3.3057e-04,  4.8576e-04,\n",
      "          5.1680e-04,  3.7300e-04,  9.8054e-04],\n",
      "        [ 3.3200e-04,  1.0245e-04,  3.7915e-04,  3.3303e-04, -1.5934e-04,\n",
      "          1.2159e-04,  3.0667e-04,  5.2518e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.9110e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[3.0939e-04, 1.3111e-04, 4.9910e-04, 6.9332e-04, 1.5397e-04, 6.7792e-04,\n",
      "         3.8914e-04, 7.7798e-04],\n",
      "        [2.9487e-04, 1.3653e-04, 4.7448e-04, 7.0432e-04, 7.0923e-04, 6.3978e-04,\n",
      "         3.6977e-04, 7.9940e-04],\n",
      "        [3.4045e-04, 1.5312e-04, 5.0801e-04, 3.3050e-04, 7.5753e-06, 5.9575e-04,\n",
      "         4.0056e-04, 7.9906e-04],\n",
      "        [3.3105e-04, 1.2030e-04, 4.4333e-04, 3.3297e-04, 4.9821e-04, 5.2620e-04,\n",
      "         3.7946e-04, 1.0121e-03]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.9895e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[3.2605e-04, 1.4538e-04, 5.2610e-04, 6.9294e-04, 6.3865e-04, 6.5696e-04,\n",
      "         3.9960e-04, 1.2782e-03],\n",
      "        [3.1492e-04, 1.3315e-04, 5.0867e-04, 7.0394e-04, 1.5282e-04, 6.9211e-04,\n",
      "         3.9652e-04, 7.9999e-04],\n",
      "        [2.9993e-04, 1.3888e-04, 4.8687e-04, 7.2182e-04, 7.4724e-04, 6.5096e-04,\n",
      "         3.7585e-04, 8.1853e-04],\n",
      "        [3.4658e-04, 1.5597e-04, 5.1769e-04, 3.3279e-04, 7.7669e-06, 6.0637e-04,\n",
      "         4.0735e-04, 8.2449e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.9063e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0003, 0.0001, 0.0006, 0.0007, 0.0008, 0.0008, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0007, 0.0006, 0.0007, 0.0004, 0.0013],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0007, 0.0002, 0.0007, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0007, 0.0008, 0.0007, 0.0004, 0.0008]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 2.3937e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0003, 0.0001, 0.0005, 0.0006, 0.0009, 0.0007, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0006, 0.0007, 0.0008, 0.0008, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0002, 0.0006, 0.0007, 0.0007, 0.0007, 0.0004, 0.0013],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0007, 0.0002, 0.0007, 0.0004, 0.0008]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 2.8673e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0004, 0.0001, 0.0006, 0.0006, 0.0002, 0.0005, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0006, 0.0009, 0.0007, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0006, 0.0007, 0.0009, 0.0008, 0.0004, 0.0009],\n",
      "        [0.0003, 0.0002, 0.0006, 0.0007, 0.0007, 0.0007, 0.0004, 0.0014]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 2.7065e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0003, 0.0001, 0.0005, 0.0006, 0.0008, 0.0007, 0.0004, 0.0012],\n",
      "        [0.0004, 0.0001, 0.0006, 0.0006, 0.0002, 0.0005, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0006, 0.0009, 0.0007, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0006, 0.0007, 0.0009, 0.0008, 0.0004, 0.0009]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 2.3730e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[0.0003, 0.0001, 0.0006, 0.0006, 0.0005, 0.0008, 0.0004, 0.0012],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0006, 0.0008, 0.0007, 0.0004, 0.0013],\n",
      "        [0.0004, 0.0001, 0.0006, 0.0006, 0.0002, 0.0005, 0.0004, 0.0008],\n",
      "        [0.0003, 0.0001, 0.0005, 0.0007, 0.0010, 0.0008, 0.0004, 0.0009]],\n",
      "       dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 2.8580e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[3.0685e-04, 1.1655e-04, 4.3500e-04, 5.6686e-04, 4.7954e-05, 4.2178e-04,\n",
      "         3.4019e-04, 6.6460e-04],\n",
      "        [3.1786e-04, 1.2967e-04, 5.8959e-04, 5.7585e-04, 4.9276e-04, 8.5638e-04,\n",
      "         4.2899e-04, 1.2278e-03],\n",
      "        [3.4334e-04, 1.2188e-04, 5.4114e-04, 5.9049e-04, 8.2948e-04, 7.2936e-04,\n",
      "         4.2737e-04, 1.2945e-03],\n",
      "        [3.8666e-04, 1.3930e-04, 5.8687e-04, 7.2630e-04, 1.9910e-04, 5.3171e-04,\n",
      "         4.2813e-04, 8.7511e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 2.8199e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[1.6861e-04, 1.4962e-04, 1.6851e-04, 1.8465e-04, 3.3229e-05, 4.9511e-05,\n",
      "         1.4921e-04, 2.3512e-04],\n",
      "        [1.3850e-04, 1.3709e-04, 1.6897e-04, 1.8758e-04, 2.1144e-04, 1.5077e-04,\n",
      "         1.4902e-04, 1.9801e-04],\n",
      "        [1.5021e-04, 1.3024e-04, 1.5645e-04, 1.9235e-04, 9.9106e-05, 1.1261e-04,\n",
      "         1.4248e-04, 1.5421e-04],\n",
      "        [1.5954e-04, 1.1366e-04, 1.8233e-04, 8.8053e-05, 1.6957e-04, 1.6129e-04,\n",
      "         1.5984e-04, 1.9910e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 7.9486e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.5523e-04, 8.5772e-05, 1.8859e-04, 1.8465e-04, 1.7740e-04, 1.7332e-04,\n",
      "         1.5773e-04, 1.0179e-04],\n",
      "        [1.7171e-04, 1.5203e-04, 1.7184e-04, 1.8758e-04, 3.2999e-05, 5.0574e-05,\n",
      "         1.5212e-04, 2.4190e-04],\n",
      "        [1.4096e-04, 1.3953e-04, 1.7347e-04, 1.9235e-04, 2.2289e-04, 1.5349e-04,\n",
      "         1.5155e-04, 2.0285e-04],\n",
      "        [1.5299e-04, 1.3273e-04, 1.5952e-04, 1.9379e-04, 1.0167e-04, 1.1468e-04,\n",
      "         1.4497e-04, 1.5920e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 8.7397e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[6.3595e-05, 1.1412e-04, 7.9248e-05, 1.4126e-04, 1.8905e-04, 1.7515e-04,\n",
      "         8.0759e-05, 1.4832e-04],\n",
      "        [7.2951e-05, 1.2800e-04, 7.9870e-05, 1.4350e-04, 1.3517e-04, 1.3396e-04,\n",
      "         8.1877e-05, 1.6467e-04],\n",
      "        [1.6089e-04, 8.8700e-05, 1.9744e-04, 1.9235e-04, 1.8571e-04, 1.8023e-04,\n",
      "         1.6354e-04, 1.0728e-04],\n",
      "        [1.7800e-04, 1.5769e-04, 1.7988e-04, 1.9379e-04, 3.5685e-05, 5.2432e-05,\n",
      "         1.5741e-04, 2.5585e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.0020e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[5.8872e-05, 1.5305e-04, 7.4432e-05, 1.4126e-04, 2.1294e-04, 1.6555e-04,\n",
      "         8.0021e-05, 2.7002e-04],\n",
      "        [6.4767e-05, 1.1596e-04, 8.0811e-05, 1.4350e-04, 1.8774e-04, 1.7891e-04,\n",
      "         8.2336e-05, 1.5260e-04],\n",
      "        [7.4243e-05, 1.3027e-04, 8.2000e-05, 1.4715e-04, 1.4249e-04, 1.3638e-04,\n",
      "         8.3269e-05, 1.6871e-04],\n",
      "        [1.6387e-04, 9.0395e-05, 2.0131e-04, 1.9379e-04, 1.9051e-04, 1.8355e-04,\n",
      "         1.6641e-04, 1.1076e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.0656e+11], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[8.1153e-05, 9.1839e-05, 8.1815e-05, 1.4126e-04, 1.1540e-04, 2.0103e-04,\n",
      "         8.5603e-05, 1.0088e-04],\n",
      "        [5.9957e-05, 1.5551e-04, 7.5900e-05, 1.4350e-04, 2.1147e-04, 1.6910e-04,\n",
      "         8.1584e-05, 2.7781e-04],\n",
      "        [6.5914e-05, 1.1802e-04, 8.2966e-05, 1.4715e-04, 1.9791e-04, 1.8213e-04,\n",
      "         8.3736e-05, 1.5634e-04],\n",
      "        [7.5619e-05, 1.3276e-04, 8.3609e-05, 1.4825e-04, 1.4618e-04, 1.3888e-04,\n",
      "         8.4726e-05, 1.7417e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.0013e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.8360e-04, 1.7713e-04, 1.4601e-04, 2.2441e-04, 1.0874e-04, 1.0886e-04,\n",
      "         1.7254e-04, 8.0250e-05],\n",
      "        [1.8183e-04, 1.6679e-04, 1.5196e-04, 2.2798e-04, 1.2364e-04, 1.0895e-04,\n",
      "         1.7371e-04, 3.5997e-05],\n",
      "        [1.7533e-04, 1.9796e-04, 1.7854e-04, 3.0672e-04, 2.5182e-04, 4.3573e-04,\n",
      "         1.8501e-04, 2.2165e-04],\n",
      "        [1.2955e-04, 3.3621e-04, 1.6561e-04, 3.0901e-04, 4.7666e-04, 3.6543e-04,\n",
      "         1.7597e-04, 6.1246e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 5.4373e+10], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[1.4329e-04, 1.7878e-04, 1.4603e-04, 2.2557e-04, 2.2180e-04, 1.0992e-04,\n",
      "         1.5365e-04, 4.6501e-05],\n",
      "        [1.8795e-04, 1.8090e-04, 1.4965e-04, 2.2915e-04, 1.0854e-04, 1.1177e-04,\n",
      "         1.7681e-04, 8.2988e-05],\n",
      "        [1.8600e-04, 1.7062e-04, 1.5681e-04, 2.3497e-04, 1.3100e-04, 1.1148e-04,\n",
      "         1.7757e-04, 3.7068e-05],\n",
      "        [1.7949e-04, 2.0278e-04, 1.8297e-04, 3.1060e-04, 2.5965e-04, 4.4602e-04,\n",
      "         1.8921e-04, 2.3000e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 5.1318e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.6646e-04, 1.6966e-04, 1.5413e-04, 2.2557e-04, 1.3524e-04, 1.3190e-04,\n",
      "         1.6151e-04, 2.5012e-04],\n",
      "        [1.4593e-04, 1.8166e-04, 1.4892e-04, 2.2915e-04, 2.2027e-04, 1.1228e-04,\n",
      "         1.5665e-04, 4.7843e-05],\n",
      "        [1.9127e-04, 1.8411e-04, 1.5365e-04, 2.3497e-04, 1.1442e-04, 1.1378e-04,\n",
      "         1.7982e-04, 8.5021e-05],\n",
      "        [1.8945e-04, 1.7389e-04, 1.5988e-04, 2.3672e-04, 1.3439e-04, 1.1353e-04,\n",
      "         1.8068e-04, 3.8268e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 5.9265e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.7529e-04, 1.8311e-04, 1.6377e-04, 1.0001e-04, 1.1002e-04, 1.3645e-04,\n",
      "         1.6559e-04, 1.2495e-04],\n",
      "        [1.7031e-04, 1.7438e-04, 1.4913e-04, 1.0160e-04, 8.7925e-05, 8.2601e-05,\n",
      "         1.5874e-04, 1.6220e-05],\n",
      "        [1.7741e-04, 1.8041e-04, 1.6592e-04, 2.4161e-04, 1.4558e-04, 1.4103e-04,\n",
      "         1.7220e-04, 2.7109e-04],\n",
      "        [1.5554e-04, 1.9374e-04, 1.6029e-04, 2.4341e-04, 2.4493e-04, 1.1969e-04,\n",
      "         1.6668e-04, 5.2031e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 6.1501e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.7314e-04, 1.7291e-04, 1.6304e-04, 9.9870e-05, 1.2684e-04, 1.3089e-04,\n",
      "         1.6608e-04, 1.7959e-04],\n",
      "        [1.7827e-04, 1.8580e-04, 1.6677e-04, 1.0145e-04, 1.0911e-04, 1.3918e-04,\n",
      "         1.6859e-04, 1.2838e-04],\n",
      "        [1.7309e-04, 1.7723e-04, 1.5290e-04, 1.0403e-04, 9.2558e-05, 8.3973e-05,\n",
      "         1.6121e-04, 1.6594e-05],\n",
      "        [1.8045e-04, 1.8360e-04, 1.6894e-04, 2.4307e-04, 1.4913e-04, 1.4343e-04,\n",
      "         1.7497e-04, 2.7948e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 6.2626e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[1.6971e-04, 1.7521e-04, 1.7719e-04, 9.9870e-05, 2.1337e-04, 1.4808e-04,\n",
      "         1.7513e-04, 1.8484e-04],\n",
      "        [1.7633e-04, 1.7569e-04, 1.6626e-04, 1.0145e-04, 1.2597e-04, 1.3370e-04,\n",
      "         1.6932e-04, 1.8477e-04],\n",
      "        [1.8143e-04, 1.8910e-04, 1.7121e-04, 1.0403e-04, 1.1502e-04, 1.4169e-04,\n",
      "         1.7145e-04, 1.3152e-04],\n",
      "        [1.7630e-04, 1.8062e-04, 1.5590e-04, 1.0481e-04, 9.4950e-05, 8.5517e-05,\n",
      "         1.6404e-04, 1.7131e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 6.7791e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.0046e-04,  1.6638e-04,  1.6692e-04,  1.4227e-04,  5.4657e-04,\n",
      "          1.1047e-04,  1.6196e-04, -4.5805e-05],\n",
      "        [ 1.7481e-04,  1.8006e-04,  1.8275e-04,  1.0262e-04,  2.1432e-04,\n",
      "          1.5299e-04,  1.8060e-04,  1.9235e-04],\n",
      "        [ 1.8151e-04,  1.8086e-04,  1.7265e-04,  1.0522e-04,  1.3431e-04,\n",
      "          1.3766e-04,  1.7417e-04,  1.9146e-04],\n",
      "        [ 1.8691e-04,  1.9492e-04,  1.7657e-04,  1.0601e-04,  1.1934e-04,\n",
      "          1.4595e-04,  1.7645e-04,  1.3734e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 6.8974e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.6895e-04,  1.5537e-04,  1.7557e-04,  1.4211e-04,  1.8675e-04,\n",
      "          1.3187e-04,  1.7079e-04,  1.4236e-04],\n",
      "        [ 1.0220e-04,  1.6886e-04,  1.7002e-04,  1.4437e-04,  5.4218e-04,\n",
      "          1.1272e-04,  1.6493e-04, -4.7073e-05],\n",
      "        [ 1.7771e-04,  1.8306e-04,  1.8741e-04,  1.0510e-04,  2.2567e-04,\n",
      "          1.5557e-04,  1.8346e-04,  1.9683e-04],\n",
      "        [ 1.8466e-04,  1.8411e-04,  1.7583e-04,  1.0589e-04,  1.3762e-04,\n",
      "          1.4004e-04,  1.7702e-04,  1.9744e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 7.3151e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.7589e-04,  1.7918e-04,  1.7266e-04,  1.4199e-04,  1.3804e-04,\n",
      "          1.2887e-04,  1.6999e-04,  1.5382e-04],\n",
      "        [ 1.7192e-04,  1.5773e-04,  1.7888e-04,  1.4424e-04,  1.8530e-04,\n",
      "          1.3459e-04,  1.7398e-04,  1.4634e-04],\n",
      "        [ 1.0392e-04,  1.7172e-04,  1.7440e-04,  1.4791e-04,  5.7105e-04,\n",
      "          1.1465e-04,  1.6759e-04, -4.8184e-05],\n",
      "        [ 1.8085e-04,  1.8639e-04,  1.9092e-04,  1.0580e-04,  2.3131e-04,\n",
      "          1.5829e-04,  1.8651e-04,  2.0303e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 5.9997e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.7015e-04,  1.7223e-04,  1.7779e-04,  1.4199e-04,  1.8254e-04,\n",
      "          1.4730e-04,  1.7124e-04,  1.1379e-04],\n",
      "        [ 1.7913e-04,  1.8206e-04,  1.7607e-04,  1.4424e-04,  1.3708e-04,\n",
      "          1.3164e-04,  1.7331e-04,  1.5825e-04],\n",
      "        [ 1.7496e-04,  1.6054e-04,  1.8365e-04,  1.4791e-04,  1.9533e-04,\n",
      "          1.3702e-04,  1.7694e-04,  1.4993e-04],\n",
      "        [ 1.0584e-04,  1.7500e-04,  1.7782e-04,  1.4901e-04,  5.8581e-04,\n",
      "          1.1676e-04,  1.7053e-04, -4.9744e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 6.6993e+10], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.7855e-04,  1.5919e-04,  1.6539e-04,  1.4127e-04,  7.6361e-05,\n",
      "          9.6164e-05,  1.6372e-04, -6.6935e-05],\n",
      "        [ 1.7595e-04,  1.7768e-04,  1.8409e-04,  1.4646e-04,  1.8406e-04,\n",
      "          1.5278e-04,  1.7727e-04,  1.1887e-04],\n",
      "        [ 1.8510e-04,  1.8814e-04,  1.8354e-04,  1.5018e-04,  1.4673e-04,\n",
      "          1.3607e-04,  1.7896e-04,  1.6462e-04],\n",
      "        [ 1.8094e-04,  1.6612e-04,  1.9012e-04,  1.5130e-04,  2.0346e-04,\n",
      "          1.4168e-04,  1.8280e-04,  1.5716e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 6.1455e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.8427e-04,  1.5831e-04,  1.8436e-04,  1.4119e-04,  1.4853e-04,\n",
      "          1.4967e-04,  1.7863e-04,  1.3180e-04],\n",
      "        [ 1.8174e-04,  1.6166e-04,  1.6856e-04,  1.4343e-04,  7.5790e-05,\n",
      "          9.8172e-05,  1.6682e-04, -6.8826e-05],\n",
      "        [ 1.7896e-04,  1.8074e-04,  1.8889e-04,  1.5010e-04,  1.9392e-04,\n",
      "          1.5544e-04,  1.8018e-04,  1.2172e-04],\n",
      "        [ 1.8843e-04,  1.9163e-04,  1.8703e-04,  1.5122e-04,  1.5043e-04,\n",
      "          1.3849e-04,  1.8199e-04,  1.6985e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 5.7786e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.1958e-04,  1.5650e-04,  1.8255e-04,  1.4097e-04, -7.9332e-05,\n",
      "          1.6426e-04,  1.7731e-04,  1.7990e-04],\n",
      "        [ 1.8737e-04,  1.6061e-04,  1.8770e-04,  1.4321e-04,  1.4727e-04,\n",
      "          1.5264e-04,  1.8183e-04,  1.3539e-04],\n",
      "        [ 1.8466e-04,  1.6427e-04,  1.7278e-04,  1.4684e-04,  7.9769e-05,\n",
      "          9.9784e-05,  1.6939e-04, -7.0400e-05],\n",
      "        [ 1.8200e-04,  1.8390e-04,  1.9229e-04,  1.5098e-04,  1.9862e-04,\n",
      "          1.5805e-04,  1.8304e-04,  1.2546e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 6.2264e+10], dtype=torch.float64)), tensor([0., 1., 0., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.7684e-04,  1.5494e-04,  1.8401e-04,  1.4097e-04,  1.9242e-04,\n",
      "          1.7360e-04,  1.7835e-04,  1.6916e-04],\n",
      "        [ 2.2363e-04,  1.5902e-04,  1.8615e-04,  1.4321e-04, -7.8784e-05,\n",
      "          1.6779e-04,  1.8077e-04,  1.8509e-04],\n",
      "        [ 1.9069e-04,  1.6346e-04,  1.9271e-04,  1.4684e-04,  1.5525e-04,\n",
      "          1.5539e-04,  1.8492e-04,  1.3870e-04],\n",
      "        [ 1.8809e-04,  1.6741e-04,  1.7617e-04,  1.4794e-04,  8.1830e-05,\n",
      "          1.0162e-04,  1.7235e-04, -7.2680e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 5.6551e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 1.9573e-04,  1.6731e-04,  1.4810e-04,  1.1026e-04,  8.5723e-05,\n",
      "         -3.1386e-05,  1.7975e-04,  5.0935e-05],\n",
      "        [ 1.5281e-04,  1.3357e-04,  1.5920e-04,  1.2150e-04,  1.6213e-04,\n",
      "          1.5046e-04,  1.5428e-04,  1.4767e-04],\n",
      "        [ 1.9310e-04,  1.3732e-04,  1.6215e-04,  1.2459e-04, -7.0465e-05,\n",
      "          1.4493e-04,  1.5599e-04,  1.6088e-04],\n",
      "        [ 1.6479e-04,  1.4134e-04,  1.6671e-04,  1.2552e-04,  1.3512e-04,\n",
      "          1.3427e-04,  1.5965e-04,  1.2150e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 7.7063e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.0996e-04,  1.6975e-04,  1.8539e-04,  1.1012e-04,  5.7827e-05,\n",
      "          6.6262e-05,  1.7698e-04,  1.8462e-04],\n",
      "        [ 1.9909e-04,  1.6979e-04,  1.7946e-04,  1.1187e-04,  8.5024e-05,\n",
      "         -3.2019e-05,  1.6795e-04,  5.2339e-05],\n",
      "        [ 1.5532e-04,  1.3577e-04,  1.6325e-04,  1.2444e-04,  1.7070e-04,\n",
      "          1.5298e-04,  1.5670e-04,  1.5110e-04],\n",
      "        [ 1.9643e-04,  1.3977e-04,  1.6513e-04,  1.2536e-04, -7.2196e-05,\n",
      "          1.4741e-04,  1.5852e-04,  1.6589e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 8.4457e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.0536e-04,  1.6878e-04,  2.1216e-04,  1.0993e-04,  1.2299e-04,\n",
      "          1.2547e-04,  1.7900e-04,  2.9291e-04],\n",
      "        [ 2.1345e-04,  1.7217e-04,  1.8871e-04,  1.1167e-04,  5.7323e-05,\n",
      "          6.7562e-05,  1.8011e-04,  1.8960e-04],\n",
      "        [ 2.0225e-04,  1.7249e-04,  1.8391e-04,  1.1451e-04,  8.9466e-05,\n",
      "         -3.2537e-05,  1.7050e-04,  5.3523e-05],\n",
      "        [ 1.5791e-04,  1.3812e-04,  1.6615e-04,  1.2514e-04,  1.7480e-04,\n",
      "          1.5551e-04,  1.5916e-04,  1.5571e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 9.2710e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.7763e-04,  1.8731e-04,  2.3116e-04,  1.0993e-04, -1.6888e-04,\n",
      "          1.0703e-04,  1.9883e-04,  2.1775e-04],\n",
      "        [ 2.0915e-04,  1.7150e-04,  2.1635e-04,  1.1167e-04,  1.2214e-04,\n",
      "          1.2816e-04,  1.8250e-04,  3.0135e-04],\n",
      "        [ 2.1722e-04,  1.7523e-04,  1.9374e-04,  1.1451e-04,  6.0427e-05,\n",
      "          6.8779e-05,  1.8317e-04,  1.9424e-04],\n",
      "        [ 2.0600e-04,  1.7579e-04,  1.8752e-04,  1.1536e-04,  9.1779e-05,\n",
      "         -3.3136e-05,  1.7348e-04,  5.5256e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 9.9336e+10], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.2831e-04,  1.7611e-04,  2.3036e-04,  1.4169e-04,  8.4645e-05,\n",
      "          9.3502e-05,  1.9114e-04,  1.3906e-04],\n",
      "        [ 2.8072e-04,  1.8896e-04,  2.3403e-04,  1.1087e-04, -1.6651e-04,\n",
      "          1.0855e-04,  2.0126e-04,  2.2242e-04],\n",
      "        [ 2.1132e-04,  1.7329e-04,  2.2052e-04,  1.1368e-04,  1.2783e-04,\n",
      "          1.2953e-04,  1.8427e-04,  3.0652e-04],\n",
      "        [ 2.1966e-04,  1.7730e-04,  1.9612e-04,  1.1453e-04,  6.1544e-05,\n",
      "          6.9541e-05,  1.8504e-04,  1.9909e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.0464e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.3020e-04,  1.8913e-04,  2.3883e-04,  1.4159e-04,  1.4834e-04,\n",
      "          1.3555e-04,  2.0222e-04,  1.5782e-04],\n",
      "        [ 2.3237e-04,  1.7883e-04,  2.3475e-04,  1.4384e-04,  8.4006e-05,\n",
      "          9.5448e-05,  1.9475e-04,  1.4298e-04],\n",
      "        [ 2.8550e-04,  1.9219e-04,  2.4012e-04,  1.1361e-04, -1.7542e-04,\n",
      "          1.1043e-04,  2.0455e-04,  2.2772e-04],\n",
      "        [ 2.1510e-04,  1.7649e-04,  2.2470e-04,  1.1446e-04,  1.3105e-04,\n",
      "          1.3183e-04,  1.8737e-04,  3.1624e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.0693e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[ 2.3186e-04,  1.7645e-04,  2.3883e-04,  1.4143e-04,  1.1380e-04,\n",
      "          1.5274e-04,  1.9938e-04,  3.0478e-04],\n",
      "        [ 2.3417e-04,  1.9195e-04,  2.4327e-04,  1.4368e-04,  1.4714e-04,\n",
      "          1.3830e-04,  2.0593e-04,  1.6219e-04],\n",
      "        [ 2.3622e-04,  1.8180e-04,  2.4073e-04,  1.4733e-04,  8.8453e-05,\n",
      "          9.7057e-05,  1.9783e-04,  1.4631e-04],\n",
      "        [ 2.9046e-04,  1.9564e-04,  2.4455e-04,  1.1433e-04, -1.7974e-04,\n",
      "          1.1233e-04,  2.0789e-04,  2.3482e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.2870e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.3245e-04, 1.8039e-04, 2.4329e-04, 1.4143e-04, 1.3220e-04, 1.7884e-04,\n",
      "         2.0400e-04, 2.1630e-04],\n",
      "        [2.3614e-04, 1.7929e-04, 2.4354e-04, 1.4368e-04, 1.1302e-04, 1.5602e-04,\n",
      "         2.0327e-04, 3.1357e-04],\n",
      "        [2.3832e-04, 1.9537e-04, 2.4975e-04, 1.4733e-04, 1.5511e-04, 1.4080e-04,\n",
      "         2.0943e-04, 1.6616e-04],\n",
      "        [2.4060e-04, 1.8527e-04, 2.4545e-04, 1.4843e-04, 9.0739e-05, 9.8841e-05,\n",
      "         2.0130e-04, 1.5105e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.2704e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.2334e-04, 1.7436e-04, 2.3167e-04, 1.7695e-04, 1.3523e-04, 1.1294e-04,\n",
      "         1.9673e-04, 6.2543e-05],\n",
      "        [2.3568e-04, 1.8247e-04, 2.4699e-04, 1.4304e-04, 1.3070e-04, 1.8187e-04,\n",
      "         2.0706e-04, 2.2154e-04],\n",
      "        [2.3925e-04, 1.8167e-04, 2.4892e-04, 1.4667e-04, 1.1861e-04, 1.5813e-04,\n",
      "         2.0581e-04, 3.1982e-04],\n",
      "        [2.4166e-04, 1.9821e-04, 2.5352e-04, 1.4777e-04, 1.5841e-04, 1.4275e-04,\n",
      "         2.1215e-04, 1.7077e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.4041e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.2651e-04, 1.7470e-04, 2.4843e-04, 1.7663e-04, 2.0208e-04, 1.8398e-04,\n",
      "         2.0883e-04, 9.8032e-05],\n",
      "        [2.2705e-04, 1.7684e-04, 2.3581e-04, 1.7943e-04, 1.3405e-04, 1.1515e-04,\n",
      "         2.0021e-04, 6.4229e-05],\n",
      "        [2.3942e-04, 1.8538e-04, 2.5311e-04, 1.4640e-04, 1.3753e-04, 1.8481e-04,\n",
      "         2.1020e-04, 2.2655e-04],\n",
      "        [2.4324e-04, 1.8480e-04, 2.5334e-04, 1.4750e-04, 1.2145e-04, 1.6074e-04,\n",
      "         2.0903e-04, 3.2958e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.4837e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.3102e-04, 1.7590e-04, 2.4923e-04, 1.7653e-04, 1.9272e-04, 1.8265e-04,\n",
      "         2.1138e-04, 2.5602e-04],\n",
      "        [2.3055e-04, 1.7740e-04, 2.5318e-04, 1.7933e-04, 2.0056e-04, 1.8781e-04,\n",
      "         2.1278e-04, 1.0080e-04],\n",
      "        [2.3093e-04, 1.7988e-04, 2.4196e-04, 1.8388e-04, 1.4123e-04, 1.1716e-04,\n",
      "         2.0349e-04, 6.5763e-05],\n",
      "        [2.4371e-04, 1.8881e-04, 2.5793e-04, 1.4741e-04, 1.4100e-04, 1.8809e-04,\n",
      "         2.1375e-04, 2.3375e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.4797e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.3585e-04, 1.7785e-04, 2.5779e-04, 1.7653e-04, 2.1059e-04, 2.1177e-04,\n",
      "         2.1803e-04, 3.2801e-04],\n",
      "        [2.3528e-04, 1.7872e-04, 2.5415e-04, 1.7933e-04, 1.9139e-04, 1.8657e-04,\n",
      "         2.1550e-04, 2.6340e-04],\n",
      "        [2.3463e-04, 1.8055e-04, 2.5993e-04, 1.8388e-04, 2.1142e-04, 1.9120e-04,\n",
      "         2.1640e-04, 1.0327e-04],\n",
      "        [2.3521e-04, 1.8332e-04, 2.4670e-04, 1.8526e-04, 1.4488e-04, 1.1931e-04,\n",
      "         2.0705e-04, 6.7893e-05]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.5361e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.3246e-04, 1.7390e-04, 2.3476e-04, 1.8357e-04, 1.1320e-04, 1.1554e-04,\n",
      "         2.1506e-04, 5.8738e-05],\n",
      "        [2.4015e-04, 1.8067e-04, 2.6282e-04, 1.7929e-04, 2.0909e-04, 2.1626e-04,\n",
      "         2.3595e-04, 3.3740e-04],\n",
      "        [2.3939e-04, 1.8186e-04, 2.6087e-04, 1.8384e-04, 2.0171e-04, 1.8989e-04,\n",
      "         2.3309e-04, 2.6979e-04],\n",
      "        [2.3893e-04, 1.8396e-04, 2.6497e-04, 1.8522e-04, 2.1684e-04, 1.9467e-04,\n",
      "         2.3432e-04, 1.0659e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.3958e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n",
      "((tensor([[2.2021e-04, 1.7699e-04, 2.1449e-04, 1.8340e-04, 1.0768e-04, 1.0233e-04,\n",
      "         2.0379e-04, 1.1240e-04],\n",
      "        [2.3652e-04, 1.7653e-04, 2.3916e-04, 1.8631e-04, 1.1231e-04, 1.1791e-04,\n",
      "         2.1905e-04, 6.0374e-05],\n",
      "        [2.4417e-04, 1.8371e-04, 2.6957e-04, 1.8367e-04, 2.2020e-04, 2.1995e-04,\n",
      "         2.3973e-04, 3.4533e-04],\n",
      "        [2.4360e-04, 1.8516e-04, 2.6573e-04, 1.8504e-04, 2.0673e-04, 1.9319e-04,\n",
      "         2.3694e-04, 2.7827e-04]], dtype=torch.float64), tensor([0.0000e+00, 1.0000e+00, 3.0000e+00, 1.6188e+11], dtype=torch.float64)), tensor([0., 0., 1., 0.], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "print(len(dynamic_tensor_dataset))\n",
    "for i, thing in list(enumerate(dynamic_tensor_dataset))[:100]:\n",
    "    print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "with open(r\"..\\other_pickle\\measures.json\", \"r\") as file:\n",
    "    measures = json.load(file)\n",
    "\n",
    "static_size = len(measures[\"static\"])\n",
    "dynamic_size = len(measures[\"dynamic\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "STATIC_BATCH_SIZE = 20\n",
    "DYNAMIC_BATCH_SIZE = 2\n",
    "\n",
    "static_train_dataloader = torch.utils.data.DataLoader(static_tensor_dataset, batch_size=STATIC_BATCH_SIZE, shuffle=True)\n",
    "dynamic_train_dataloader = torch.utils.data.DataLoader(dynamic_tensor_dataset, batch_size=DYNAMIC_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "LAYERS = 8\n",
    "\n",
    "class StaticLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, batch_size, layers, input, categories=0):\n",
    "        super(StaticLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.layers_num = layers\n",
    "        \n",
    "        #input is all the embedding vectors plus all the other variables\n",
    "        self.lstm = nn.LSTM(input, hidden_dim, num_layers=layers, batch_first=True) \n",
    "        self.hidden = (torch.zeros(layers,batch_size,hidden_dim),torch.zeros(layers,batch_size,hidden_dim))\n",
    "        \n",
    "        #Squeeeze them into 1 dimension\n",
    "        if categories > 0:\n",
    "            self.hidden2label = nn.Linear(hidden_dim, categories)\n",
    "        else:\n",
    "            self.hidden2label = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, batch_tensor):\n",
    "        lstm_out, self.hidden = self.lstm(batch_tensor)\n",
    "        last_timestep_output = lstm_out[:, -1, :]\n",
    "        sales = self.hidden2label(last_timestep_output)\n",
    "        return sales\n",
    "    \n",
    "    def hidden_reset(self):\n",
    "        #reset the hidden and cell state after each epoch\n",
    "        self.hidden = (torch.zeros(self.layers_num,self.batch_size,self.hidden_dim),\n",
    "                       torch.zeros(self.layers_num,self.batch_size,self.hidden_dim))\n",
    "    def batch_reset(self,batch_size):\n",
    "        self.hidden = (torch.zeros(self.layers_num,batch_size,self.hidden_dim),\n",
    "                       torch.zeros(self.layers_num,batch_size,self.hidden_dim))\n",
    "    def flatten_parameters(self):\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "static_model = StaticLSTM(HIDDEN_SIZE, STATIC_BATCH_SIZE, LAYERS, static_size, categories=6)\n",
    "static_model = static_model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Batch 50, Loss: 0.10775196806054581\n",
      "Batch 100, Loss: 0.09472923024047718\n",
      "Batch 150, Loss: 0.09064534934671269\n",
      "Batch 200, Loss: 0.08853876835137076\n",
      "Batch 250, Loss: 0.08805167887432032\n",
      "Batch 300, Loss: 0.08702061069858083\n",
      "Batch 350, Loss: 0.08729162992295826\n",
      "Batch 400, Loss: 0.08752628200982747\n",
      "Batch 450, Loss: 0.08712476874389594\n",
      "Batch 500, Loss: 0.08722365264252695\n",
      "Batch 550, Loss: 0.08729035351475259\n",
      "Batch 600, Loss: 0.08681666268354615\n",
      "Batch 650, Loss: 0.08667377470334678\n",
      "Batch 700, Loss: 0.08624226868232394\n",
      "Batch 750, Loss: 0.08597234619382214\n",
      "Batch 800, Loss: 0.08591118572180756\n",
      "Batch 850, Loss: 0.08609858182644\n",
      "Batch 900, Loss: 0.08630155578027436\n",
      "Batch 950, Loss: 0.08613220371188478\n",
      "Batch 1000, Loss: 0.08597507376754862\n",
      "Batch 1050, Loss: 0.08602298062781169\n",
      "Batch 1100, Loss: 0.08613478705194759\n",
      "Batch 1150, Loss: 0.0860482809452679\n",
      "Batch 1200, Loss: 0.08603195771924284\n",
      "Batch 1250, Loss: 0.08583584865879834\n",
      "Batch 1300, Loss: 0.08582224631120536\n",
      "Batch 1350, Loss: 0.0856488115726623\n",
      "Batch 1400, Loss: 0.08556450089268139\n",
      "Batch 1450, Loss: 0.08540641913521413\n",
      "Batch 1500, Loss: 0.08544924399004847\n",
      "Batch 1550, Loss: 0.08540629677014523\n",
      "Batch 1600, Loss: 0.08533752354293894\n",
      "Batch 1650, Loss: 0.08541731396309045\n",
      "Batch 1700, Loss: 0.08544104140783963\n",
      "Batch 1750, Loss: 0.0855656116845936\n",
      "Batch 1800, Loss: 0.08572829902323302\n",
      "Batch 1850, Loss: 0.08577193098035062\n",
      "Batch 1900, Loss: 0.0856262743645435\n",
      "Batch 1950, Loss: 0.08555669247635353\n",
      "Batch 2000, Loss: 0.08555158706161885\n",
      "Batch 2050, Loss: 0.0855964787130967\n",
      "Batch 2100, Loss: 0.0856365368293967\n",
      "Batch 2150, Loss: 0.08572771571703384\n",
      "Batch 2200, Loss: 0.08579780897561524\n",
      "Batch 2250, Loss: 0.08567878832623316\n",
      "Batch 2300, Loss: 0.08563080845813349\n",
      "Batch 2350, Loss: 0.08559300370024885\n",
      "Batch 2400, Loss: 0.08559213563958691\n",
      "Batch 2450, Loss: 0.08561093351734526\n",
      "Batch 2500, Loss: 0.08560320059708583\n",
      "Batch 2550, Loss: 0.08558620511439627\n",
      "Batch 2600, Loss: 0.08572369860223505\n",
      "Batch 2650, Loss: 0.08574244877145301\n",
      "Batch 2700, Loss: 0.08577402324951627\n",
      "Batch 2750, Loss: 0.08580486447571019\n",
      "Batch 2800, Loss: 0.0857664001639459\n",
      "Batch 2850, Loss: 0.08574970282678024\n",
      "Batch 2900, Loss: 0.0857524058530572\n",
      "Batch 2950, Loss: 0.08581857386647204\n",
      "Batch 3000, Loss: 0.0857882625529197\n",
      "Batch 3050, Loss: 0.08576703145891458\n",
      "Batch 3100, Loss: 0.08574643885234874\n",
      "Batch 3150, Loss: 0.08577539027549744\n",
      "Batch 3200, Loss: 0.08575904876792748\n",
      "Batch 3250, Loss: 0.08576089854249079\n",
      "Batch 3300, Loss: 0.08575529175708485\n",
      "Batch 3350, Loss: 0.08572653577549617\n",
      "Batch 3400, Loss: 0.08574346178837335\n",
      "Batch 3450, Loss: 0.08576419760963094\n",
      "Batch 3500, Loss: 0.08568793215550961\n",
      "Batch 3550, Loss: 0.08563553803533062\n",
      "Batch 3600, Loss: 0.08563969854152742\n",
      "Batch 3650, Loss: 0.0856598356907902\n",
      "Batch 3700, Loss: 0.08565764223465243\n",
      "Batch 3750, Loss: 0.08568972341913186\n",
      "Batch 3800, Loss: 0.0856081676179304\n",
      "Batch 3850, Loss: 0.08556913027185371\n",
      "Batch 3900, Loss: 0.08557463357922049\n",
      "Batch 3950, Loss: 0.08552001924995269\n",
      "Batch 4000, Loss: 0.08552121940552687\n",
      "Batch 4050, Loss: 0.08552149690945852\n",
      "Batch 4100, Loss: 0.08553025096022142\n",
      "Batch 4150, Loss: 0.08553783117723092\n",
      "Batch 4200, Loss: 0.08554977427796218\n",
      "Batch 4250, Loss: 0.08551054674002574\n",
      "Batch 4300, Loss: 0.08543535730310141\n",
      "Batch 4350, Loss: 0.08549854449077628\n",
      "Batch 4400, Loss: 0.08545189646116688\n",
      "Batch 4450, Loss: 0.08547817270032523\n",
      "Batch 4500, Loss: 0.08548891383666515\n",
      "Batch 4550, Loss: 0.08556065951529031\n",
      "Batch 4600, Loss: 0.08556488672778446\n",
      "Batch 4650, Loss: 0.08554152777714905\n",
      "Batch 4700, Loss: 0.08556305332333297\n",
      "Batch 4750, Loss: 0.08556733667085908\n",
      "Batch 4800, Loss: 0.08554263634551129\n",
      "Batch 4850, Loss: 0.08551993645580547\n",
      "Batch 4900, Loss: 0.08553924219041344\n",
      "Batch 4950, Loss: 0.08555796000951378\n",
      "Batch 5000, Loss: 0.08553658047079267\n",
      "Batch 5050, Loss: 0.08554290875156234\n",
      "Batch 5100, Loss: 0.08548516678473342\n",
      "Batch 5150, Loss: 0.08548355318378115\n",
      "Batch 5200, Loss: 0.08548284314589957\n",
      "Batch 5250, Loss: 0.08548579136429692\n",
      "Batch 5300, Loss: 0.0855189094386094\n",
      "Batch 5350, Loss: 0.08554105906629615\n",
      "Batch 5400, Loss: 0.08554119242929553\n",
      "Batch 5450, Loss: 0.08553913815486339\n",
      "Batch 5500, Loss: 0.08556105968548473\n",
      "Batch 5550, Loss: 0.0855758176416671\n",
      "Batch 5600, Loss: 0.08559631024766122\n",
      "Batch 5650, Loss: 0.08555918276854241\n",
      "Batch 5700, Loss: 0.08552737360748645\n",
      "Batch 5750, Loss: 0.08554278802572492\n",
      "Batch 5800, Loss: 0.08551102143172679\n",
      "Batch 5850, Loss: 0.0855113339494247\n",
      "Batch 5900, Loss: 0.08552613187401327\n",
      "Batch 5950, Loss: 0.08547823964140071\n",
      "Batch 6000, Loss: 0.08542921220001543\n",
      "Batch 6050, Loss: 0.08544817135719768\n",
      "Batch 6100, Loss: 0.08540287419121131\n",
      "Batch 6150, Loss: 0.08541734472685104\n",
      "Batch 6200, Loss: 0.08543004858470428\n",
      "Batch 6250, Loss: 0.08542640024495003\n",
      "Batch 6300, Loss: 0.0854602136807238\n",
      "Batch 6350, Loss: 0.08547131774743426\n",
      "Batch 6400, Loss: 0.08546025293206246\n",
      "Batch 6450, Loss: 0.08547735564969876\n",
      "Batch 6500, Loss: 0.0854648871361636\n",
      "Reset triggered due to batch size mismatch\n",
      "Output shape: torch.Size([]), Label shape: torch.Size([1])\n",
      "Average loss for epoch 1: 0.08544544451204593\n",
      "Model saved\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50, Loss: 0.08341466733226487\n",
      "Batch 100, Loss: 0.0850143377189131\n",
      "Batch 150, Loss: 0.08450723371134278\n",
      "Batch 200, Loss: 0.082964066748668\n",
      "Batch 250, Loss: 0.08365548972261229\n",
      "Batch 300, Loss: 0.08410297123576765\n",
      "Batch 350, Loss: 0.08432670760269825\n",
      "Batch 400, Loss: 0.085249662832735\n",
      "Batch 450, Loss: 0.08513972609176604\n",
      "Batch 500, Loss: 0.08536617976112969\n",
      "Batch 550, Loss: 0.08575323468577666\n",
      "Batch 600, Loss: 0.08573773523330117\n",
      "Batch 650, Loss: 0.08579665911654963\n",
      "Batch 700, Loss: 0.08581389779596893\n",
      "Batch 750, Loss: 0.08609370155974287\n",
      "Batch 800, Loss: 0.08616305208052215\n",
      "Batch 850, Loss: 0.0863034998942753\n",
      "Batch 900, Loss: 0.0865207479057329\n",
      "Batch 950, Loss: 0.0865219340331799\n",
      "Batch 1000, Loss: 0.08638159780266902\n",
      "Batch 1050, Loss: 0.0863253317811039\n",
      "Batch 1100, Loss: 0.0863473679330785\n",
      "Batch 1150, Loss: 0.08642075101277094\n",
      "Batch 1200, Loss: 0.08630195078800076\n",
      "Batch 1250, Loss: 0.08623328953156471\n",
      "Batch 1300, Loss: 0.0860420839063351\n",
      "Batch 1350, Loss: 0.08606990241615561\n",
      "Batch 1400, Loss: 0.08617955950730537\n",
      "Batch 1450, Loss: 0.08604382349221462\n",
      "Batch 1500, Loss: 0.08603399802168474\n",
      "Batch 1550, Loss: 0.08598930708251624\n",
      "Batch 1600, Loss: 0.08593989629801431\n",
      "Batch 1650, Loss: 0.08603097391855116\n",
      "Batch 1700, Loss: 0.08599004193931627\n",
      "Batch 1750, Loss: 0.08592587800031912\n",
      "Batch 1800, Loss: 0.08590674340691576\n",
      "Batch 1850, Loss: 0.08593399871572263\n",
      "Batch 1900, Loss: 0.08592765716773991\n",
      "Batch 1950, Loss: 0.08589000773774738\n",
      "Batch 2000, Loss: 0.08597586985218332\n",
      "Batch 2050, Loss: 0.08588377317530649\n",
      "Batch 2100, Loss: 0.08586698275054476\n",
      "Batch 2150, Loss: 0.08581447064824016\n",
      "Batch 2200, Loss: 0.08578291961279463\n",
      "Batch 2250, Loss: 0.0856335182326224\n",
      "Batch 2300, Loss: 0.08557524310250132\n",
      "Batch 2350, Loss: 0.08559310760493602\n",
      "Batch 2400, Loss: 0.08557860239616395\n",
      "Batch 2450, Loss: 0.08555060284729295\n",
      "Batch 2500, Loss: 0.0855729119958297\n",
      "Batch 2550, Loss: 0.08548919820684615\n",
      "Batch 2600, Loss: 0.0853680819572973\n",
      "Batch 2650, Loss: 0.0853222876289765\n",
      "Batch 2700, Loss: 0.08534796208063775\n",
      "Batch 2750, Loss: 0.0853715706132589\n",
      "Batch 2800, Loss: 0.08532146009374794\n",
      "Batch 2850, Loss: 0.08540844255165075\n",
      "Batch 2900, Loss: 0.08541695039095608\n",
      "Batch 2950, Loss: 0.0853497031575528\n",
      "Batch 3000, Loss: 0.08540805340006148\n",
      "Batch 3050, Loss: 0.08539708104636425\n",
      "Batch 3100, Loss: 0.08537418318520272\n",
      "Batch 3150, Loss: 0.085375805321117\n",
      "Batch 3200, Loss: 0.08535238319861879\n",
      "Batch 3250, Loss: 0.08528671036158106\n",
      "Batch 3300, Loss: 0.0852741696919477\n",
      "Batch 3350, Loss: 0.08531900955377279\n",
      "Batch 3400, Loss: 0.08538273970498363\n",
      "Batch 3450, Loss: 0.08541285643307171\n",
      "Batch 3500, Loss: 0.08544638193250337\n",
      "Batch 3550, Loss: 0.08544881278699724\n",
      "Batch 3600, Loss: 0.08551615813500629\n",
      "Batch 3650, Loss: 0.08558325389840368\n",
      "Batch 3700, Loss: 0.0855934077088735\n",
      "Batch 3750, Loss: 0.08555771617683074\n",
      "Batch 3800, Loss: 0.08553804845441434\n",
      "Batch 3850, Loss: 0.08552333969232273\n",
      "Batch 3900, Loss: 0.08550589004467743\n",
      "Batch 3950, Loss: 0.08543863758929665\n",
      "Batch 4000, Loss: 0.08538974558510416\n",
      "Batch 4050, Loss: 0.08537579731079402\n",
      "Batch 4100, Loss: 0.08543944902167007\n",
      "Batch 4150, Loss: 0.08539292742155341\n",
      "Batch 4200, Loss: 0.08543281725051743\n",
      "Batch 4250, Loss: 0.08537458112784585\n",
      "Batch 4300, Loss: 0.08535889844874187\n",
      "Batch 4350, Loss: 0.08535037915740937\n",
      "Batch 4400, Loss: 0.08533404491873199\n",
      "Batch 4450, Loss: 0.08531301388811331\n",
      "Batch 4500, Loss: 0.08533935985233966\n",
      "Batch 4550, Loss: 0.08533313315794563\n",
      "Batch 4600, Loss: 0.08533744960155781\n",
      "Batch 4650, Loss: 0.0853471365427746\n",
      "Batch 4700, Loss: 0.08531001235275026\n",
      "Batch 4750, Loss: 0.08532226039143939\n",
      "Batch 4800, Loss: 0.08533404790834269\n",
      "Batch 4850, Loss: 0.08534173586153486\n",
      "Batch 4900, Loss: 0.08533570354369288\n",
      "Batch 4950, Loss: 0.08528819405610714\n",
      "Batch 5000, Loss: 0.08525693753511507\n",
      "Batch 5050, Loss: 0.08524428287345091\n",
      "Batch 5100, Loss: 0.08527261488367394\n",
      "Batch 5150, Loss: 0.08525255085694101\n",
      "Batch 5200, Loss: 0.08525547008311465\n",
      "Batch 5250, Loss: 0.08527439966707662\n",
      "Batch 5300, Loss: 0.08530093064118703\n",
      "Batch 5350, Loss: 0.08533727249666662\n",
      "Batch 5400, Loss: 0.08532843905281817\n",
      "Batch 5450, Loss: 0.08535515868496805\n",
      "Batch 5500, Loss: 0.08534672406279918\n",
      "Batch 5550, Loss: 0.08534090632816319\n",
      "Batch 5600, Loss: 0.08533687389440081\n",
      "Batch 5650, Loss: 0.08530793457731291\n",
      "Batch 5700, Loss: 0.08528845068260153\n",
      "Batch 5750, Loss: 0.08528726904250554\n",
      "Batch 5800, Loss: 0.08530638123405003\n",
      "Batch 5850, Loss: 0.08530018414670296\n",
      "Batch 5900, Loss: 0.08532880595183076\n",
      "Batch 5950, Loss: 0.08531706382334071\n",
      "Batch 6000, Loss: 0.08530312720931167\n",
      "Batch 6050, Loss: 0.08528998692819809\n",
      "Batch 6100, Loss: 0.08531127096515223\n",
      "Batch 6150, Loss: 0.08529438204433229\n",
      "Batch 6200, Loss: 0.0853068656854903\n",
      "Batch 6250, Loss: 0.08530307144639504\n",
      "Batch 6300, Loss: 0.08531909060817988\n",
      "Batch 6350, Loss: 0.08530913441650653\n",
      "Batch 6400, Loss: 0.0853155757524722\n",
      "Batch 6450, Loss: 0.08533284029913152\n",
      "Batch 6500, Loss: 0.08532011193660352\n",
      "Reset triggered due to batch size mismatch\n",
      "Output shape: torch.Size([]), Label shape: torch.Size([1])\n",
      "Average loss for epoch 2: 0.08531163418681456\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.L1Loss()\n",
    "loss_scores = [float('inf')]\n",
    "learning_rate = 0.01\n",
    "epochs = 2\n",
    "static_model = static_model.to(device)\n",
    "optimizer = optim.Adam(static_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    static_model.hidden_reset()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(static_train_dataloader):\n",
    "        (input, worthless_input), label = batch\n",
    "        if input.shape[0] != STATIC_BATCH_SIZE:\n",
    "            static_model.batch_reset(input.shape[0])\n",
    "            print(\"Reset triggered due to batch size mismatch\")\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = static_model(input).squeeze()\n",
    "        \n",
    "        # Ensure output and label shapes are compatible for the loss function\n",
    "        if output.shape != label.shape:\n",
    "            print(f\"Output shape: {output.shape}, Label shape: {label.shape}\")\n",
    "        \n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print(f\"Batch {i+1}, Loss: {epoch_loss / (i+1)}\")\n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(static_train_dataloader)\n",
    "    print(f\"Average loss for epoch {epoch+1}: {average_epoch_loss}\")\n",
    "    \n",
    "    if average_epoch_loss < loss_scores[-1]:\n",
    "        torch.save(static_model.state_dict(), \"../other_pickle/Static_Model.pth\")\n",
    "        print(\"Model saved\")\n",
    "    loss_scores.append(average_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_model.load_state_dict(torch.load(\"../other_pickle/Static_Model.pth\"))\n",
    "static_model.eval()\n",
    "for i, batch in list(enumerate(static_train_dataloader))[:10]:\n",
    "    (input, worthless_input), label = batch\n",
    "    output = static_model(input).squeeze()\n",
    "    # loss = loss_function(output, label)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "LAYERS = 8\n",
    "\n",
    "class DynamicLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, batch_size, layers, input, categories=0):\n",
    "        super(DynamicLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.layers_num = layers\n",
    "        \n",
    "        #input is all the embedding vectors plus all the other variables\n",
    "        self.lstm = nn.LSTM(input, hidden_dim, num_layers=layers, batch_first=True) \n",
    "        self.hidden = (torch.zeros(layers,batch_size,hidden_dim),torch.zeros(layers,batch_size,hidden_dim))\n",
    "        \n",
    "        #Squeeeze them into 1 dimension\n",
    "        if categories > 0:\n",
    "            self.hidden2label = nn.Linear(hidden_dim, categories)\n",
    "        else:\n",
    "            self.hidden2label = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, batch_tensor):\n",
    "        lstm_out, self.hidden = self.lstm(batch_tensor)\n",
    "        last_timestep_output = lstm_out[:, -1, :]\n",
    "        sales = self.hidden2label(last_timestep_output)\n",
    "        return sales\n",
    "    \n",
    "    def hidden_reset(self):\n",
    "        #reset the hidden and cell state after each epoch\n",
    "        self.hidden = (torch.zeros(self.layers_num,self.batch_size,self.hidden_dim),\n",
    "                       torch.zeros(self.layers_num,self.batch_size,self.hidden_dim))\n",
    "    def batch_reset(self,batch_size):\n",
    "        self.hidden = (torch.zeros(self.layers_num,batch_size,self.hidden_dim),\n",
    "                       torch.zeros(self.layers_num,batch_size,self.hidden_dim))\n",
    "    def flatten_parameters(self):\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "dynamic_model = DynamicLSTM(HIDDEN_SIZE, STATIC_BATCH_SIZE, LAYERS, dynamic_size, categories=9)\n",
    "dynamic_model = dynamic_model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Batch 10, Loss: 1.319769970832645\n",
      "Batch 20, Loss: 1.402531977834551\n",
      "Batch 30, Loss: 1.464933596904132\n",
      "Batch 40, Loss: 1.4592754595945305\n",
      "Batch 50, Loss: 1.3818568124136457\n",
      "Batch 60, Loss: 1.3636398481749858\n",
      "Batch 70, Loss: 1.3132020866374143\n",
      "Batch 80, Loss: 1.3108535315241712\n",
      "Batch 90, Loss: 1.3094457392159218\n",
      "Batch 100, Loss: 1.3350681369344393\n",
      "Batch 110, Loss: 1.329348893167229\n",
      "Batch 120, Loss: 1.3456242269003726\n",
      "Batch 130, Loss: 1.366603802401159\n",
      "Batch 140, Loss: 1.4289747565367938\n",
      "Batch 150, Loss: 1.5224066097331046\n",
      "Batch 160, Loss: 1.5296784347681953\n",
      "Batch 170, Loss: 1.5473424931930428\n",
      "Batch 180, Loss: 1.535560604618781\n",
      "Batch 190, Loss: 1.5372189313142395\n",
      "Batch 200, Loss: 1.5588628976616474\n",
      "Batch 210, Loss: 1.5526475355553524\n",
      "Batch 220, Loss: 1.5599478513441942\n",
      "Batch 230, Loss: 1.5665123795053804\n",
      "Batch 240, Loss: 1.5615688519629691\n",
      "Batch 250, Loss: 1.5590981741086294\n",
      "Batch 260, Loss: 1.569677791923514\n",
      "Batch 270, Loss: 1.5643889242132076\n",
      "Batch 280, Loss: 1.566394245782407\n",
      "Batch 290, Loss: 1.5484054326818253\n",
      "Batch 300, Loss: 1.536587805039059\n",
      "Batch 310, Loss: 1.5383354163133987\n",
      "Batch 320, Loss: 1.5309422669778807\n",
      "Batch 330, Loss: 1.535168357222227\n",
      "Batch 340, Loss: 1.5331908950102726\n",
      "Batch 350, Loss: 1.528485619321586\n",
      "Batch 360, Loss: 1.5238777215257235\n",
      "Batch 370, Loss: 1.5211441544082336\n",
      "Batch 380, Loss: 1.5131181359042833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[156], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdynamic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Ensure output and label shapes are compatible for the loss function\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m label\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[145], line 24\u001b[0m, in \u001b[0;36mDynamicLSTM.forward\u001b[1;34m(self, batch_tensor)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_tensor):\n\u001b[1;32m---> 24\u001b[0m     lstm_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     last_timestep_output \u001b[38;5;241m=\u001b[39m lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     26\u001b[0m     sales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden2label(last_timestep_output)\n",
      "File \u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_scores = [float('inf')]\n",
    "learning_rate = 0.1\n",
    "epochs = 1\n",
    "dynamic_model = dynamic_model.to(device)\n",
    "optimizer = optim.Adam(dynamic_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    dynamic_model.hidden_reset()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(dynamic_train_dataloader):\n",
    "        (input, worthless_input), label = batch\n",
    "        if input.shape[0] != DYNAMIC_BATCH_SIZE:\n",
    "            dynamic_model.batch_reset(input.shape[0])\n",
    "            print(\"Reset triggered due to batch size mismatch\")\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = dynamic_model(input).squeeze()\n",
    "        \n",
    "        # Ensure output and label shapes are compatible for the loss function\n",
    "        if output.shape != label.shape:\n",
    "            print(f\"Output shape: {output.shape}, Label shape: {label.shape}\")\n",
    "        \n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Batch {i+1}, Loss: {epoch_loss / (i+1)}\")\n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(dynamic_train_dataloader)\n",
    "    print(f\"Average loss for epoch {epoch+1}: {average_epoch_loss}\")\n",
    "    \n",
    "    if average_epoch_loss < loss_scores[-1]:\n",
    "        torch.save(dynamic_model.state_dict(), \"../other_pickle/Dynamic_Model.pth\")\n",
    "        print(\"Model saved\")\n",
    "    loss_scores.append(average_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04],\n",
      "        [2.4551e-10, 1.5086e-04, 3.8396e-01, 5.2203e-01, 8.1895e-02, 9.0988e-03,\n",
      "         1.2079e-03, 1.2944e-03, 3.6838e-04]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>) tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dynamic_model.load_state_dict(torch.load(\"../other_pickle/Dynamic_Model.pth\"))\n",
    "dynamic_model.eval()\n",
    "for i, batch in list(enumerate(dynamic_train_dataloader))[:100]:\n",
    "    (input, worthless_input), label = batch\n",
    "    output = dynamic_model(input).squeeze()\n",
    "    # loss = loss_function(output, label)\n",
    "    print(F.softmax(output, dim=1), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
