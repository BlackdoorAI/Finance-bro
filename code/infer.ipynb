{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from infer_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_reload = [\"infer_functions\"]\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    __import__(module_name)\n",
    "    module = sys.modules[module_name]\n",
    "    globals().update({name: getattr(module, name) for name in dir(module) if not name.startswith('_')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_tensor_dataset = create_tensor_dataset(\"static\", 4, limit=100, categories= 0, averages=True)\n",
    "dynamic_tensor_dataset = create_tensor_dataset(\"dynamic\", 4, limit=100, categories= 0, averages=True, use_profile=True, use_multiples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catting AAPL.csv\n",
      "Catting ABT.csv\n",
      "Catting AEP.csv\n",
      "Catting AMD.csv\n",
      "Catting AMGN.csv\n",
      "Catting AMZN.csv\n",
      "Catting APH.csv\n",
      "Catting BA.csv\n",
      "Catting BKR.csv\n",
      "Catting BMY.csv\n",
      "Catting BSX.csv\n",
      "Catting CDW.csv\n",
      "Catting CI.csv\n",
      "Catting CL.csv\n",
      "Catting COP.csv\n",
      "Catting COR.csv\n",
      "Catting CPNG.csv\n",
      "Catting CSGP.csv\n",
      "Catting CTSH.csv\n",
      "Catting CVS.csv\n",
      "Catting DHR.csv\n",
      "Catting ECL.csv\n",
      "Catting ED.csv\n",
      "Catting EFX.csv\n",
      "Catting EL.csv\n",
      "Catting ET.csv\n",
      "Catting FAST.csv\n",
      "Catting FIS.csv\n",
      "Catting GILD.csv\n",
      "Catting HES.csv\n",
      "Catting HSY.csv\n",
      "Catting IBM.csv\n",
      "Catting INTC.csv\n",
      "Catting IR.csv\n",
      "Catting KDP.csv\n",
      "Catting KLAC.csv\n",
      "Catting KMB.csv\n",
      "Catting KMI.csv\n",
      "Catting KO.csv\n",
      "Catting LHX.csv\n",
      "Catting LIN.csv\n",
      "Catting LRCX.csv\n",
      "Catting LYB.csv\n",
      "Catting MA.csv\n",
      "Catting MCK.csv\n",
      "Catting MCO.csv\n",
      "Catting MLM.csv\n",
      "Catting MMM.csv\n",
      "Catting MPC.csv\n",
      "Catting MSFT.csv\n",
      "Catting MSI.csv\n",
      "Catting NEM.csv\n",
      "Catting NOC.csv\n",
      "Catting NSC.csv\n",
      "Catting NUE.csv\n",
      "Catting OKE.csv\n",
      "Catting ON.csv\n",
      "Catting ORLY.csv\n",
      "Catting OTIS.csv\n",
      "Catting PFE.csv\n",
      "Catting PG.csv\n",
      "Catting PH.csv\n",
      "Catting PM.csv\n",
      "Catting PWR.csv\n",
      "Catting QCOM.csv\n",
      "Catting RTX.csv\n",
      "Catting SBUX.csv\n",
      "Catting SCCO.csv\n",
      "Catting SHW.csv\n",
      "Catting SPGI.csv\n",
      "Catting SYK.csv\n",
      "Catting SYY.csv\n",
      "Catting TDG.csv\n",
      "Catting TEL.csv\n",
      "Catting TMO.csv\n",
      "Catting TSLA.csv\n",
      "Catting TXN.csv\n",
      "Catting URI.csv\n",
      "Catting WM.csv\n",
      "Catting XYL.csv\n",
      "Catting ZTS.csv\n",
      "tensor(False) tensor(False) tensor(False)\n"
     ]
    }
   ],
   "source": [
    "with open(r\"..\\other_pickle\\measures.json\", \"r\") as file:\n",
    "    measures = json.load(file)\n",
    "combined_tensor_dataset = create_tensor_dataset(\"together\", 5, measures, limit =800, categories= 0, verbose=True, averages=False, use_profile=True, use_multiples=False, ghost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in combined_tensor_dataset:\n",
    "    print(inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "with open(r\"..\\other_pickle\\measures.json\", \"r\") as file:\n",
    "    measures = json.load(file)\n",
    "\n",
    "static_size = len(measures[\"static\"])\n",
    "dynamic_size = len(measures[\"dynamic\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "STATIC_BATCH_SIZE = 20\n",
    "DYNAMIC_BATCH_SIZE = 5\n",
    "COMBINED_BATCH_SIZE = 32\n",
    "\n",
    "# static_train_dataloader = torch.utils.data.DataLoader(static_tensor_dataset, batch_size=STATIC_BATCH_SIZE, shuffle=True)\n",
    "# dynamic_train_dataloader = torch.utils.data.DataLoader(dynamic_tensor_dataset, batch_size=DYNAMIC_BATCH_SIZE, shuffle=False)\n",
    "# combined_train_dataloader = torch.utils.data.DataLoader(combined_tensor_dataset, batch_size=COMBINED_BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "LAYERS = 8\n",
    "\n",
    "static_model = StaticLSTM(HIDDEN_SIZE, STATIC_BATCH_SIZE, LAYERS, static_size, categories=6)\n",
    "static_model = static_model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = split_tensor_dataset(static_tensor_dataset)\n",
    "grid_search(StaticLSTM, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()\n",
    "loss_scores = [float('inf')]\n",
    "learning_rate = 0.01\n",
    "epochs = 2\n",
    "static_model = static_model.to(device)\n",
    "optimizer = optim.Adam(static_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    static_model.hidden_reset()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(static_train_dataloader):\n",
    "        (input, worthless_input), label = batch\n",
    "        if input.shape[0] != STATIC_BATCH_SIZE:\n",
    "            static_model.batch_reset(input.shape[0])\n",
    "            print(\"Reset triggered due to batch size mismatch\")\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = static_model(input).squeeze()\n",
    "        \n",
    "        # Ensure output and label shapes are compatible for the loss function\n",
    "        if output.shape != label.shape:\n",
    "            print(f\"Output shape: {output.shape}, Label shape: {label.shape}\")\n",
    "        \n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print(f\"Batch {i+1}, Loss: {epoch_loss / (i+1)}\")\n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(static_train_dataloader)\n",
    "    print(f\"Average loss for epoch {epoch+1}: {average_epoch_loss}\")\n",
    "    \n",
    "    if average_epoch_loss < loss_scores[-1]:\n",
    "        torch.save(static_model.state_dict(), \"../other_pickle/Static_Model.pth\")\n",
    "        print(\"Model saved\")\n",
    "    loss_scores.append(average_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_model.load_state_dict(torch.load(\"../other_pickle/Static_Model.pth\"))\n",
    "static_model.eval()\n",
    "for i, batch in list(enumerate(static_train_dataloader))[:10]:\n",
    "    (input, worthless_input), label = batch\n",
    "    output = static_model(input).squeeze()\n",
    "    # loss = loss_function(output, label)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_reload = [\"infer_functions\"]\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    __import__(module_name)\n",
    "    module = sys.modules[module_name]\n",
    "    globals().update({name: getattr(module, name) for name in dir(module) if not name.startswith('_')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-13 00:30:45,479] A new study created in memory with name: no-name-e34ca1de-900c-4d97-9128-2ef98a39edd0\n",
      "[I 2024-06-13 00:32:03,103] Trial 0 finished with value: 0.10038082953418623 and parameters: {'hidden_dim': 437, 'layers': 4, 'batch_size': 16, 'lr': 0.003038154129716284, 'epochs': 2}. Best is trial 0 with value: 0.10038082953418623.\n",
      "[I 2024-06-13 00:32:05,642] Trial 1 finished with value: 0.10009098962899576 and parameters: {'hidden_dim': 165, 'layers': 2, 'batch_size': 64, 'lr': 0.00740147025891154, 'epochs': 1}. Best is trial 1 with value: 0.10009098962899576.\n",
      "[I 2024-06-13 00:32:19,027] Trial 2 finished with value: 0.5972372979827296 and parameters: {'hidden_dim': 331, 'layers': 2, 'batch_size': 16, 'lr': 0.009466467695336572, 'epochs': 1}. Best is trial 1 with value: 0.10009098962899576.\n",
      "[I 2024-06-13 00:33:34,037] Trial 3 finished with value: 0.20244354824980024 and parameters: {'hidden_dim': 328, 'layers': 4, 'batch_size': 16, 'lr': 0.007598251470058156, 'epochs': 3}. Best is trial 1 with value: 0.10009098962899576.\n",
      "[I 2024-06-13 00:33:47,606] Trial 4 finished with value: 0.11244786788586701 and parameters: {'hidden_dim': 324, 'layers': 4, 'batch_size': 64, 'lr': 0.006656967446449591, 'epochs': 1}. Best is trial 1 with value: 0.10009098962899576.\n",
      "[I 2024-06-13 00:34:08,201] Trial 5 finished with value: 0.17311995911862704 and parameters: {'hidden_dim': 267, 'layers': 4, 'batch_size': 16, 'lr': 0.0014153255897160892, 'epochs': 1}. Best is trial 1 with value: 0.10009098962899576.\n",
      "[I 2024-06-13 00:34:16,231] Trial 6 finished with value: 0.0963741911125318 and parameters: {'hidden_dim': 163, 'layers': 2, 'batch_size': 64, 'lr': 0.007554072845705796, 'epochs': 3}. Best is trial 6 with value: 0.0963741911125318.\n",
      "[I 2024-06-13 00:34:36,359] Trial 7 finished with value: 0.13462883259464126 and parameters: {'hidden_dim': 203, 'layers': 3, 'batch_size': 16, 'lr': 0.005985982512284994, 'epochs': 2}. Best is trial 6 with value: 0.0963741911125318.\n",
      "[I 2024-06-13 00:34:52,389] Trial 8 finished with value: 0.11244041210086576 and parameters: {'hidden_dim': 208, 'layers': 3, 'batch_size': 48, 'lr': 0.003700128902912334, 'epochs': 3}. Best is trial 6 with value: 0.0963741911125318.\n",
      "[I 2024-06-13 00:35:25,152] Trial 9 finished with value: 0.09587821616399922 and parameters: {'hidden_dim': 277, 'layers': 4, 'batch_size': 16, 'lr': 0.008790942024600675, 'epochs': 1}. Best is trial 9 with value: 0.09587821616399922.\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = split_tensor_dataset(combined_tensor_dataset)\n",
    "best_parameters = grid_search(DynamicLSTM, x_train, y_train, n_trials=10, categories=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 288, 'layers': 3, 'batch_size': 32, 'lr': 0.0014625720043832183, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Batch 10, Loss: 0.7624322595160065\n",
      "Batch 20, Loss: 0.386967732861853\n",
      "Batch 30, Loss: 0.2613999674617259\n",
      "Batch 40, Loss: 0.1975048067858554\n",
      "Batch 50, Loss: 0.1597081456687775\n",
      "Batch 60, Loss: 0.13536079461954834\n",
      "Batch 70, Loss: 0.11740619922441936\n",
      "Batch 80, Loss: 0.1038365854815854\n",
      "Batch 90, Loss: 0.09343729232504773\n",
      "Batch 100, Loss: 0.08504591803135755\n",
      "Batch 110, Loss: 0.07825082887840876\n",
      "Batch 120, Loss: 0.07402816352052223\n",
      "Reset triggered due to batch size mismatch\n",
      "Average loss for epoch 1: 0.07182994614602504\n",
      "Model saved\n",
      "Epoch 2/1\n",
      "Batch 10, Loss: 0.014380470689310521\n",
      "Batch 20, Loss: 0.011746353080158343\n",
      "Batch 30, Loss: 0.010758333578077409\n",
      "Batch 40, Loss: 0.009818680875900012\n",
      "Batch 50, Loss: 0.009923647432612622\n",
      "Batch 60, Loss: 0.01098262778718081\n",
      "Batch 70, Loss: 0.011287995123552563\n",
      "Batch 80, Loss: 0.011420591634955174\n",
      "Batch 90, Loss: 0.011205457416902365\n",
      "Batch 100, Loss: 0.010976625398587904\n",
      "Batch 110, Loss: 0.010882718150887172\n",
      "Batch 120, Loss: 0.012331125424967263\n",
      "Reset triggered due to batch size mismatch\n",
      "Average loss for epoch 2: 0.012108937684938393\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# loss_function = nn.CrossEntropyLoss()\n",
    "combined_train_dataloader = DataLoader(combined_tensor_dataset, batch_size=best_parameters[\"batch_size\"], shuffle=False)\n",
    "loss_function = nn.HuberLoss(delta=1.0)\n",
    "loss_scores = [float('inf')]\n",
    "learning_rate = best_parameters[\"lr\"]\n",
    "combined_model = DynamicLSTM(best_parameters[\"hidden_dim\"], best_parameters[\"batch_size\"], best_parameters[\"layers\"], input=input_size)\n",
    "combined_model = combined_model.to(device).double()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=learning_rate)\n",
    "#best_parameters[\"epochs\"]\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch+1}/{best_parameters['epochs']}\")\n",
    "    combined_model.hidden_reset()\n",
    "    combined_model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(combined_train_dataloader):\n",
    "        (input, worthless_input), label = batch\n",
    "        if input.shape[0] != best_parameters[\"batch_size\"]:\n",
    "            combined_model.batch_reset(input.shape[0])\n",
    "            print(\"Reset triggered due to batch size mismatch\")\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = combined_model(input).squeeze()\n",
    "        \n",
    "        # Ensure output and label shapes are compatible for the loss function\n",
    "        if output.shape != label.shape:\n",
    "            print(f\"Output shape: {output.shape}, Label shape: {label.shape}\")\n",
    "        \n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Batch {i+1}, Loss: {epoch_loss / (i+1)}\")\n",
    "    \n",
    "    average_epoch_loss = epoch_loss / len(combined_train_dataloader)\n",
    "    print(f\"Average loss for epoch {epoch+1}: {average_epoch_loss}\")\n",
    "    \n",
    "    if average_epoch_loss < loss_scores[-1]:\n",
    "        torch.save(combined_model.state_dict(), \"../other_pickle/Combined_Model.pth\")\n",
    "        print(\"Model saved\")\n",
    "    loss_scores.append(average_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.load_state_dict(torch.load(\"../other_pickle/Combined_Model.pth\"))\n",
    "combined_model.eval()\n",
    "for i, batch in list(enumerate(combined_train_dataloader))[:100]:\n",
    "    (input, worthless_input), label = batch\n",
    "    output = combined_model(input).squeeze()\n",
    "    # loss = loss_function(output, label)\n",
    "    # print(F.softmax(output, dim=1), label)\n",
    "    print(output, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
