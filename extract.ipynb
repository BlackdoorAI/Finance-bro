{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "from fredapi import Fred\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pydash\n",
    "import pickle\n",
    "import yahoo_fin.stock_info as si\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "fred = Fred(api_key='0c34c4dd2fd6943f6549f1c990a8a0f0') \n",
    "\n",
    "async def fetch(url, url_headers, semaphore, client, timeout, max_retries, start_retry_delay):\n",
    "    async with semaphore:\n",
    "        for attempt in range(1,max_retries):\n",
    "            try:\n",
    "                response = await client.get(url, timeout=timeout, headers= url_headers)\n",
    "                response.raise_for_status()\n",
    "                return response  # Successful request, exit the loop\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                    headers = response.headers\n",
    "                    #Sometimes a retry-after header is returned\n",
    "                    retry_after = headers.get('Retry-After')\n",
    "                    if retry_after != None:\n",
    "                        #Just for debugging\n",
    "                        print(retry_after)\n",
    "                        await asyncio.sleep(retry_after.astype(int))\n",
    "                        continue\n",
    "                    print(f\"Error response {e.response.status_code} for {url}\")\n",
    "            except httpx.TimeoutException as e:\n",
    "                print(f\"Timeout reached: {e}\")\n",
    "                print(f\"Retrying in {attempt*start_retry_delay} seconds...\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except httpx.RequestError as e:\n",
    "                print(f\"An error occurred: {e}.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0\n",
    "                \n",
    "\n",
    "def fred_info(ids:list, start:str, end:str):\n",
    "    #start and end are datatime objects\n",
    "    start = start.strftime('%Y-%m-%d')\n",
    "    end = end.strftime('%Y-%m-%d')\n",
    "    frame = pd.DataFrame()\n",
    "    for id in ids:\n",
    "        series = fred.get_series(id,observation_start=start, observation_end=end)\n",
    "        frame[id] = series\n",
    "    frame = frame.reset_index()\n",
    "    frame[\"index\"] = frame[\"index\"].astype(str)\n",
    "    frame = frame.ffill()\n",
    "    return frame.bfill()\n",
    "\n",
    "\n",
    "#Kinda obsolete\n",
    "async def fred_fetch(ids:list, start:str, end:str):\n",
    "    fred_data = await asyncio.to_thread(fred_info,ids,start,end)\n",
    "    return fred_data\n",
    "\n",
    "async def yahoo_fetch(ticker, start_year, end_year, semaphore, max_retries, start_retry_delay):\n",
    "    async with semaphore:\n",
    "        for attempt in range(1,max_retries):\n",
    "            try:\n",
    "                response = await asyncio.to_thread(si.get_data,ticker,start_year, end_year)\n",
    "                return response  # Successful request, exit the loop\n",
    "            except requests.exceptions.ConnectionError as ce:\n",
    "                print(\"Yahoo connection error.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except Exception as e:\n",
    "                print(f\"Yahoo error:{e}\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            # except HTTPError as e:\n",
    "            #         headers = response.headers\n",
    "            #         #Sometimes a retry-after header is returned\n",
    "            #         retry_after = headers.get('Retry-After')\n",
    "            #         if retry_after != None:\n",
    "            #             #Just for debugging\n",
    "            #             print(retry_after)\n",
    "            #             await asyncio.sleep(retry_after.astype(int))\n",
    "            #             continue\n",
    "            #         print(f\"Error response {e.response.status_code}.\")\n",
    "            # except Timeout as e:\n",
    "            #     print(f\"Yahoo Timeout reached: {e}\")\n",
    "            #     print(f\"Retrying in {attempt*start_retry_delay} seconds...\")\n",
    "            #     await asyncio.sleep(attempt*start_retry_delay)\n",
    "            # except RequestException as e:\n",
    "            #     print(f\"A Yahoo error occurred: {e}.\")\n",
    "            #     await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decompose the measure into its constituents\n",
    "measure_conversion = {\"Assets\":{\"replace\":[[\"AccruedAssets\"]],\"add\": [[\"AssetsNoncurrent\", \"AssetsCurrent\"]]},\n",
    "                    \"Liabilities\":{\"replace\":[[\"AccruedLiabilities\"]],\"add\":[[\"LiabilitiesCurrent\", \"LiabilitiesNoncurrent\"]]},\n",
    "                    \"AssetsCurrent\":{},\n",
    "                    \"LiabilitiesCurrent\":{},\n",
    "                    \"revenues\": {}\n",
    "}\n",
    "\n",
    "#Lookup table for the undeprecated version of a measure\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\deprecated_to_current.json\", \"r\") as file:\n",
    "    deprecate_conversion = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "#The first entry date into the EDGARD database\n",
    "START = datetime.strptime('1993-01-01', r\"%Y-%m-%d\")\n",
    "\n",
    "#Manually figure out which measure is used with some company\n",
    "def checkout(name, data):\n",
    "    compdict = {}\n",
    "    for key,value in data.items():\n",
    "        compdict[key] = value[\"description\"]\n",
    "\n",
    "    with open(f\"C:\\Programming\\Python\\Finance\\EDGAR\\{name}.json\", \"w\") as file:\n",
    "        json.dump(compdict, file, indent =1)\n",
    "\n",
    "#Run a list of possible names for the measure\n",
    "def runlist(dict, nameslist:list, ticker, debug=False):\n",
    "    idx = 0\n",
    "    while (idx<len(nameslist)):\n",
    "        try:\n",
    "            data = dict[nameslist[idx]]\n",
    "            return data\n",
    "        except KeyError:\n",
    "            idx +=1\n",
    "    print(f\"{nameslist} not available for {ticker}\")\n",
    "    if debug:\n",
    "        checkout(ticker, dict)\n",
    "    return False\n",
    "\n",
    "#Obsolete fuck if flatten doesn't work\n",
    "def searchdict(dict, nameslist:list, ticker):\n",
    "    for key in [\"dei\", \"us-gaap\", \"ifrs-full\", \"invest\"]:\n",
    "        try:\n",
    "            data = runlist(dict[key], nameslist, ticker) \n",
    "            if  data != False:\n",
    "                return data\n",
    "        except KeyError:\n",
    "            continue\n",
    "    print(f\"{nameslist} not available for {ticker}\")\n",
    "    return False\n",
    "    # raise KeyError(f\"{ticker} doesnt have any of {nameslist}\")\n",
    "\n",
    "#The above shit is useless, flatten the whole fucking thing \n",
    "#Removes the top layer of the dict and returns the flatened version\n",
    "def flatten(d):\n",
    "    dictionary = {}\n",
    "    for key, value in d[\"facts\"].items():\n",
    "        dictionary.update(value)\n",
    "    return dictionary\n",
    "\n",
    "def getcik(ticker):\n",
    "    #Convert the ticker into the proper cik\n",
    "    for key,value in cikdata.items():\n",
    "        if value[\"ticker\"] == ticker:\n",
    "            cik = value[\"cik_str\"]\n",
    "            break\n",
    "    return str(cik).zfill(10)\n",
    "\n",
    "#Headers for EDGAR call\n",
    "headers = {\n",
    "    \"User-Agent\":\"ficakc@seznam.cz\",\n",
    "    \"Accept-Encoding\":\"gzip, deflate\",\n",
    "}\n",
    "\n",
    "TIMEOUT = 8\n",
    "RETRIES = 2\n",
    "START_RETRY_DELAY = 1\n",
    "# cik_url =  \"https://www.sec.gov/files/company_tickers.json\"\n",
    "# cikdata = requests.get(cik_url, headers=headers).json()\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\cik.json\",\"r\") as file:\n",
    "    cikdata = json.load(file)\n",
    "    file.close()\n",
    "    \n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\apple.json\",\"r\") as file:\n",
    "    Apple = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def sync_companyfacts(ticker:str):\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    data  = httpx.get(data_url, headers= headers)\n",
    "    return data\n",
    "    \n",
    "async def companyfacts(ticker:str, client, semaphore):\n",
    "    #Get all the financial data for a ticker\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    facts = await fetch(data_url, headers, semaphore, client, TIMEOUT,RETRIES,START_RETRY_DELAY)\n",
    "    return facts\n",
    "\n",
    "def endtodatetime(dataframe):\n",
    "    dataframe.loc[:,\"end\"] = pd.datetime(dataframe[\"end\"])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "class Stock:\n",
    "    def __init__(self, ticker:str):\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = getcik(self.ticker)\n",
    "        \n",
    "    async def async_init(self,client, semaphore, standard_measures):\n",
    "        #Get all of the data for the company, ALL of it \n",
    "        data = await companyfacts(self.ticker, client, semaphore)\n",
    "        #If the response wasn't recieved, skips the rest of the code \n",
    "        if type(data) != int:\n",
    "            #Flatten the dict to avoid the shitstorm\n",
    "            self.data = flatten(data.json())\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        #Get the share amount \n",
    "        self.share_name_list = [\"EntityCommonStockSharesOutstanding\", \"CommonStockSharesOutstanding\", \"WeightedAverageNumberOfSharesOutstandingBasic\", \"WeightedAverageNumberOfDilutedSharesOutstanding\"]\n",
    "        meta = copy.deepcopy(self.data)\n",
    "        #searches the company dict for the first occurence of something in the names list\n",
    "        share_info = runlist(meta, self.share_name_list, self.ticker, debug=True)\n",
    "        share_date= datetime.strptime(share_info[\"units\"][\"shares\"][0][\"end\"], r\"%Y-%m-%d\")\n",
    "        #Get the earliest date with all the info about the company\n",
    "        start_dates = []\n",
    "        for measure in standard_measures:\n",
    "            stuff = copy.deepcopy(self.data)\n",
    "            if measure in deprecate_conversion:\n",
    "                fact = runlist(stuff,[deprecate_conversion[measure]],self.ticker, debug=True)\n",
    "                if fact ==False:\n",
    "                    continue\n",
    "                date = datetime.strptime(fact[\"units\"][\"USD\"][0][\"end\"],r\"%Y-%m-%d\")\n",
    "            else:\n",
    "                fact =runlist(stuff,[measure],self.ticker, debug=True)\n",
    "                if fact ==False:\n",
    "                    continue\n",
    "                date = datetime.strptime(fact[\"units\"][\"USD\"][0][\"end\"],r\"%Y-%m-%d\")\n",
    "            start_dates.append(date)\n",
    "        self.start_year = max([share_date] + start_dates)\n",
    "        self.end_year = datetime.now().date()\n",
    "        return 1\n",
    "    async def price_init(self,semaphore):\n",
    "        #Get the price and set the self.price\n",
    "        self.fullprice = await yahoo_fetch(self.ticker,self.start_year, self.end_year, semaphore, RETRIES, START_RETRY_DELAY)\n",
    "        if type(self.fullprice) == int:\n",
    "            return 0\n",
    "        self.fullprice = self.fullprice.reset_index()\n",
    "        Price = self.fullprice[[self.fullprice.columns[0],\"close\", \"adjclose\"]].copy()\n",
    "        Price[\"end\"] = Price[\"index\"].astype(str)\n",
    "        Price.drop(columns=[\"index\"],inplace=True)\n",
    "        date_range = pd.date_range(start=self.start_year, end=self.end_year).astype(str)\n",
    "        self.date_range = pd.DataFrame(date_range, columns=['end'])\n",
    "        Price = pd.merge(self.date_range, Price, on = [\"end\"],how=\"left\" )\n",
    "        self.price = Price.ffill().bfill()\n",
    "        return 1 \n",
    "    def fact(self,measure,simple=True):\n",
    "        #Propagate the 0 \n",
    "        if self.data == 0:\n",
    "            return   \n",
    "        try:\n",
    "            if measure in deprecate_conversion:\n",
    "                measure = deprecate_conversion[measure]\n",
    "                # frame = pd.concat([frame, frame_undep], axis=0).reset_index(drop=True)\n",
    "            point_list = self.data[\"us-gaap\"][measure][\"units\"][\"USD\"]\n",
    "            frame = pd.DataFrame(point_list)\n",
    "            frame = frame.drop_duplicates(subset='end', keep='last')\n",
    "            frame[measure] = frame[\"val\"]\n",
    "            if simple:\n",
    "                frame = frame[[\"end\", measure]]\n",
    "            #If the measure is deprecated switch to the undeprecated version\n",
    "            frame = pd.merge(self.date_range,frame,on=\"end\",how=\"left\")\n",
    "            frame = frame.ffill().bfill()\n",
    "            return frame\n",
    "        except KeyError:\n",
    "            print(f\"Measure {measure} not available for {self.ticker}.\")\n",
    "    def shares(self,simple=True):\n",
    "        #Propagate the 0 \n",
    "        if self.data == 0:\n",
    "            return 0\n",
    "        if simple:\n",
    "            meta = copy.deepcopy(self.data)\n",
    "            share_count = pd.DataFrame(runlist(meta,self.share_name_list)[\"units\"][\"shares\"][0], index=[0])[[\"end\",\"val\"]]\n",
    "        share_count[\"shares\"] = share_count[\"val\"]\n",
    "        share_count.drop(columns=[\"val\"], inplace = True)\n",
    "        share_count = share_count.drop_duplicates(subset=\"end\", keep=\"last\")\n",
    "        share_count = pd.merge(self.date_range, share_count, on=[\"end\"], how=\"left\")\n",
    "        return share_count.ffill()\n",
    "    \n",
    "\n",
    "#Initializes and appends the stock object\n",
    "async def async_task(ticker, client, semaphore_edgar, semaphore_yahoo, measures):\n",
    "    # Measures are used to get the date when all the financial info is available\n",
    "    stock = Stock(ticker)\n",
    "    print(f\"Currently pinging {ticker}\")\n",
    "    successful_edgar = await stock.async_init(client,semaphore_edgar,measures)\n",
    "    if successful_edgar:\n",
    "        print(f\"Price pinging {ticker}$\")\n",
    "        succesful_price = await stock.price_init(semaphore_yahoo)\n",
    "    else:\n",
    "        succesful_price = False\n",
    "    with open(f'C:\\Programming\\Python\\Finance\\EDGAR\\companies\\{ticker}.pkl', 'wb') as file:\n",
    "        pickle.dump(stock,file)\n",
    "    del stock\n",
    "    #Return (ticker, availability of data, availability of price)\n",
    "    return (ticker, successful_edgar, succesful_price)\n",
    "\n",
    "\n",
    "\n",
    "def acquire_frame(ticker, measures, indicator_frame):\n",
    "    #Get a dataframe from the saved data of some stock \n",
    "    try:\n",
    "        with open(f'C:\\Programming\\Python\\Finance\\EDGAR\\companies\\{ticker}.pkl', 'rb') as file:\n",
    "            stock = pickle.load(file)\n",
    "            file.close()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{ticker} is not available for loading\")\n",
    "    try:\n",
    "        #Price and shares oustanding \n",
    "        shares = stock.shares().copy()\n",
    "        stock_price = stock.price.copy()\n",
    "        df = pd.merge(shares, stock_price, left_on=[\"end\"], right_on=[\"end\"], how = \"left\")\n",
    "        frames_list = [stock.fact(measure) for measure in measures]\n",
    "        for frame in frames_list:\n",
    "            df = pd.merge(df,frame, on=[\"end\"], how=\"left\")\n",
    "    except AttributeError:\n",
    "        return\n",
    "    #Economic indicators \n",
    "    df = pd.merge(df, indicator_frame, left_on =[\"end\"], right_on=[\"index\"], how=\"left\")\n",
    "    return df\n",
    "    \n",
    "#Get the success rate for the api call\n",
    "def success_rate(company_frames_availability):\n",
    "    edgar_success = 0\n",
    "    yahoo_success = 0\n",
    "    for ticker, edgar, yahoo in company_frames_availability:\n",
    "        edgar_success += edgar\n",
    "        yahoo_success += yahoo\n",
    "    try:\n",
    "        edgar_success = edgar_success/len(company_frames_availability)\n",
    "        yahoo_success = yahoo_success/len(company_frames_availability)\n",
    "        print(f\"Edgar success rate: {edgar_success}\")\n",
    "        print(f\"Yahoo success rate: {yahoo_success}\")\n",
    "    except ZeroDivisionError:\n",
    "        print(\"FULL\")\n",
    "#Function to call again for missing data\n",
    "def ticker_fill(company_frames_availability):\n",
    "    ticker_list = []\n",
    "    for ticker, edgar, yahoo in company_frames_availability:\n",
    "        if edgar:\n",
    "            continue\n",
    "        else:\n",
    "            ticker_list.append(ticker)\n",
    "    return ticker_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\"TB3MS\", \"DCOILWTICO\"]\n",
    "indicator_frame = fred_info(indicators, START, datetime.now().date())\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\other_pickle\\indicator_frame.pkl\", \"wb\") as file:\n",
    "    pickle.dump(indicator_frame, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIG FUCKING RESET!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#GATHER THE FIRST companies_num companies ciks and pass them to the gather with the tasks\n",
    "company_frames_availability = []\n",
    "for company, values in cikdata.items():\n",
    "    company_frames_availability.append((values[\"ticker\"],0,0))\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\other_pickle\\frame_availability.pkl\", \"wb\") as file:\n",
    "    pickle.dump(company_frames_availability, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL\n"
     ]
    }
   ],
   "source": [
    "#write out measures based on importance in descending order\n",
    "# tracemalloc.start()\n",
    "measures = [\"Assets\", \"Liabilities\", \"AssetsCurrent\", \"LiabilitiesCurrent\"]\n",
    "#Load out the indicators \n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\other_pickle\\indicator_frame.pkl\", \"rb\") as file:\n",
    "    indicator_frame = pickle.load(file)\n",
    "#Load the info about the companies that we already have (ticker,edgar,yahoo)\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\other_pickle\\frame_availability.pkl\", \"rb\") as file:\n",
    "    company_frames_availability = pickle.load(file)\n",
    "\n",
    "edgar_client =  httpx.AsyncClient()\n",
    "sem_edgar = asyncio.Semaphore(9)\n",
    "#Separate sem for yahoo to spread the work and connections\n",
    "sem_yahoo = asyncio.Semaphore(9)\n",
    "\n",
    "#Create tasks to ge the first companies_num companies by valuation\n",
    "companies_num = 5\n",
    "ticker_list = company_frames_availability[:companies_num]\n",
    "ticker_list = ticker_fill(ticker_list)\n",
    "tasks = []\n",
    "for ticker in ticker_list:\n",
    "    tasks.append(async_task(ticker, edgar_client, sem_edgar, sem_yahoo, measures))\n",
    "\n",
    "company_frames_availability = await asyncio.gather(*tasks)\n",
    "success_rate(company_frames_availability)\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\other_pickle\\frame_availability.pkl\", \"wb\") as file:\n",
    "    pickle.dump(company_frames_availability, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_frames_tuples = [(ticker,acquire_frame(ticker, measures, indicator_frame)) for ticker,value_edg,value_yah in company_frames_availability if value_edg and value_yah]\n",
    "company_frames_dict = {}\n",
    "for key,value in company_frames_tuples:\n",
    "    company_frames_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAPL', 1, 1), ('MSFT', 1, 1), ('GOOGL', 1, 1), ('AMZN', 1, 1), ('NVDA', 1, 1), ('META', 1, 1), ('TSLA', 1, 1), ('BRK-B', 1, 1), ('LLY', 1, 1), ('TSM', 0, False)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\companies\\AAPL.pkl\", \"br\") as file:\n",
    "    company =  pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# frame = apple.fact(\"Assets\")\n",
    "# frame.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple.data[\"facts\"][\"dei\"][\"EntityCommonStockSharesOutstanding\"][\"units\"][\"shares\"]\n",
    "frame  = fred_info([\"TB3MS\", \"DCOILWTICO\"], '2015-02-24', '2017-02-24')\n",
    "frame.head(40)\n",
    "# print(frame)\n",
    "\n",
    "frame[\"index\"] = frame[\"index\"].astype(str)\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\FRED.json\", \"w\") as file:\n",
    "    json.dump(frame.to_dict(orient=\"records\"), file, indent=1)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated fucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "tickers = [\"META\"]\n",
    "for ticker in tickers:\n",
    "    data = sync_companyfacts(ticker).json()\n",
    "    data = data[\"facts\"][\"us-gaap\"]\n",
    "    for key,value in data.items():\n",
    "        del value[\"units\"]\n",
    "        if not key in dictionary:\n",
    "            dictionary[key] = value\n",
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\deprecated.json\", \"w\")as file:\n",
    "    json.dump(dictionary, file, indent= 1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# meta = sync_companyfacts(\"META\").json()\n",
    "# dictionary = amazon[\"facts\"][\"dei\"].keys()\n",
    "# dictionary  = [i for i in dictionary]\n",
    "# with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\amazontotal.json\", \"w\") as file:\n",
    "#         json.dump(dictionary, file, indent =1)\n",
    "#         file.close()\n",
    "\n",
    "\n",
    "# print(amazon[\"facts\"].keys())\n",
    "checkout(\"META\",\"meta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Stock.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m measures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssetsCurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiabilitiesCurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m Apple \u001b[38;5;241m=\u001b[39m \u001b[43mStock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maapl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m stock \u001b[38;5;241m=\u001b[39m Apple\n\u001b[0;32m      4\u001b[0m shares \u001b[38;5;241m=\u001b[39m stock\u001b[38;5;241m.\u001b[39mshares()\n",
      "\u001b[1;31mTypeError\u001b[0m: Stock.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "measures = [\"Assets\", \"Liabilities\", \"AssetsCurrent\", \"LiabilitiesCurrent\"]\n",
    "Apple = Stock(\"aapl\", measures)\n",
    "stock = Apple\n",
    "shares = stock.shares()\n",
    "stock_num = stock.price\n",
    "if isinstance(shares, int) or isinstance(stock_num, int): \n",
    "    pass\n",
    "    # break\n",
    "df = pd.merge(shares.copy(), stock_num.copy(), on=[\"end\"], how = \"left\")\n",
    "frames_list = [stock.fact(measure) for measure in measures]\n",
    "for frame in frames_list:\n",
    "    df = pd.merge(df,frame, on=[\"end\"], how=\"left\")\n",
    "df.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Get all the measure names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\stock.json\", \"w\") as file:\n",
    "#     json.dump(df.to_dict(orient='records'), file, indent=1)\n",
    "#     file.close()\n",
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\shares.json\", \"w\") as file:\n",
    "    json.dump(stock.shares().copy().to_dict(orient='records'), file, indent=1)\n",
    "    file.close()\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\assets.json\", \"w\") as file:\n",
    "    json.dump(Apple[\"facts\"][\"us-gaap\"][\"Assets\"][\"units\"][\"USD\"], file, indent=1)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = {}\n",
    "for key,value in Apple[\"facts\"][\"us-gaap\"].items() :\n",
    "    measures[key.ljust(100)] = value[\"label\"]\n",
    "\n",
    "measures[\"METADATA\".ljust(200,\"/\")] = \"\"\n",
    "\n",
    "for key,value in Apple[\"facts\"][\"dei\"].items():\n",
    "    measures[key.ljust(100)] = value[\"label\"]\n",
    "\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\measures.json\",\"w\") as file:\n",
    "    json.dump(measures, file, indent=1)\n",
    "\n",
    "\n",
    "#create price reference list:\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\price.json\", \"w\") as file:\n",
    "    json.dump(Apple.price.to_dict(orient='records'), file, indent=1)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Apple[\"facts\"][\"us-gaap\"][\"AssetsCurrent\"][\"units\"][\"USD\"] :\n",
    "    try:\n",
    "        del key[\"frame\"]\n",
    "        print(key)\n",
    "    except KeyError:\n",
    "        print(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Apple[\"facts\"][\"us-gaap\"][\"Assets\"][\"units\"][\"USD\"][0][\"end\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
