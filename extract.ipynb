{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to info: Stock[\"facts\"][\"us-gaap\"][some_measure][\"units\"][\"USD\"]\n",
    "\n",
    "Path to meta info: Stock[\"facts\"][\"dei\"][some_meausere]\n",
    "\n",
    "List of all possible names for shares outstanding:\n",
    "    CommonStockSharesOutstanding\n",
    "    EntityCommonStockSharesOutstanding\n",
    "\n",
    "Decide if to ping individual concepts instead of getting all the data\n",
    "Alternative is storing all the files in harddisk instead of memory\n",
    "    The Current strategy is to store the data using pickle and reload it for analysis\n",
    "\n",
    "Three things:\n",
    "    First:\n",
    "        Graph and correlate measures and ratios\n",
    "            Currently forward filling fundamentals data, could be fucked in the future\n",
    "            Include the rate of change of the variables and compare those \n",
    "    Second:\n",
    "        Make a test to validate trading strategies\n",
    "            Add a function that picks eligible stocks at some timeframe and test their performance xdz\n",
    "    Third:\n",
    "        Train a model to predict the long term price\n",
    "            forward filling fundamentals can be really fucked \n",
    "            Consider using averages or linear change from previous value to next for training \n",
    "            Include all the past fundamentals for each datapoint - this could figure out how people value growth.\n",
    "\n",
    "Data to add for regression and AI:\n",
    "    oil price\n",
    "    interest rates \n",
    "        FRED HAS SO MUCH DATA HOLY SHIT \n",
    "        pick yer fucking poison mate\n",
    "    \n",
    "\n",
    "Measures:\n",
    "    Price/BookValuepershare = MarketCap/BookValue = EntityCommonStockSharesOutstanding*SPrice(Assets-Liabilities)\n",
    "    Price/Earnings - all the possible earnings metrics\n",
    "    Current ratio = AssetsCurrent/LiabilitiesCurrent\n",
    "    ///////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    Revenue\n",
    "    Operating Income \n",
    "    Free cash flow \n",
    "    EBITDA \n",
    "    Dividends - \"PaymentsOfDividendsCommonStock\"\n",
    "\n",
    "How does market cap influence these correlations?\n",
    "\n",
    "Where do you find price data?\n",
    "    Yahoo-fin.\n",
    "\n",
    "\n",
    "Methods to figure out relationships:\n",
    "    Kendall's Tau\n",
    "    Spearman's Rank Correlation\n",
    "\n",
    "    cross variable correlation loop through all variables \n",
    "    Polynomial regression\n",
    "    LSTM\n",
    "\n",
    "\n",
    "Todo:\n",
    "    Maybe split the price part into a second async price_init\n",
    "        async is fucking up again\n",
    "\n",
    "    Make a filler function to get data that wasnt available \n",
    "\n",
    "    Implement additive logic for unavailable measures\n",
    "\n",
    "    Figure out how much of the data was unsuccessful or missing\n",
    "        maybe add a counter if you can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "from fredapi import Fred\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pydash\n",
    "import pickle\n",
    "import yahoo_fin.stock_info as si\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "fred = Fred(api_key='0c34c4dd2fd6943f6549f1c990a8a0f0') \n",
    "\n",
    "async def fetch(url, url_headers, semaphore, client, timeout, max_retries, start_retry_delay):\n",
    "    async with semaphore:\n",
    "        for attempt in range(1,max_retries):\n",
    "            try:\n",
    "                response = await client.get(url, timeout=timeout, headers= url_headers)\n",
    "                response.raise_for_status()\n",
    "                return response  # Successful request, exit the loop\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                    headers = response.headers\n",
    "                    #Sometimes a retry-after header is returned\n",
    "                    retry_after = headers.get('Retry-After')\n",
    "                    if retry_after != None:\n",
    "                        #Just for debugging\n",
    "                        print(retry_after)\n",
    "                        await asyncio.sleep(retry_after.astype(int))\n",
    "                        continue\n",
    "                    print(f\"Error response {e.response.status_code} for {url}\")\n",
    "            except httpx.TimeoutException as e:\n",
    "                print(f\"Timeout reached: {e}\")\n",
    "                print(f\"Retrying in {attempt*start_retry_delay} seconds...\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except httpx.RequestError as e:\n",
    "                print(f\"An error occurred: {e}.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0\n",
    "                \n",
    "\n",
    "def fred_info(ids:list, start:str, end:str):\n",
    "    #start and end are datatime objects\n",
    "    start = start.strftime('%Y-%m-%d')\n",
    "    end = end.strftime('%Y-%m-%d')\n",
    "    frame = pd.DataFrame()\n",
    "    for id in ids:\n",
    "        series = fred.get_series(id,observation_start=start, observation_end=end)\n",
    "        frame[id] = series\n",
    "    frame = frame.reset_index()\n",
    "    frame[\"index\"] = frame[\"index\"].astype(str)\n",
    "    frame = frame.ffill()\n",
    "    return frame.bfill()\n",
    "\n",
    "\n",
    "#Kinda obsolete\n",
    "async def fred_fetch(ids:list, start:str, end:str):\n",
    "    fred_data = await asyncio.to_thread(fred_info,ids,start,end)\n",
    "    return fred_data\n",
    "\n",
    "async def yahoo_fetch(ticker, start_year, end_year, semaphore, max_retries, start_retry_delay):\n",
    "    async with semaphore:\n",
    "        for attempt in range(1,max_retries):\n",
    "            try:\n",
    "                response = await asyncio.to_thread(si.get_data,ticker,start_year, end_year)\n",
    "                return response  # Successful request, exit the loop\n",
    "            except requests.exceptions.ConnectionError as ce:\n",
    "                print(\"Yahoo connection error.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except Exception as e:\n",
    "                print(f\"Yahoo error:{e}\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            # except HTTPError as e:\n",
    "            #         headers = response.headers\n",
    "            #         #Sometimes a retry-after header is returned\n",
    "            #         retry_after = headers.get('Retry-After')\n",
    "            #         if retry_after != None:\n",
    "            #             #Just for debugging\n",
    "            #             print(retry_after)\n",
    "            #             await asyncio.sleep(retry_after.astype(int))\n",
    "            #             continue\n",
    "            #         print(f\"Error response {e.response.status_code}.\")\n",
    "            # except Timeout as e:\n",
    "            #     print(f\"Yahoo Timeout reached: {e}\")\n",
    "            #     print(f\"Retrying in {attempt*start_retry_delay} seconds...\")\n",
    "            #     await asyncio.sleep(attempt*start_retry_delay)\n",
    "            # except RequestException as e:\n",
    "            #     print(f\"A Yahoo error occurred: {e}.\")\n",
    "            #     await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decompose the measure into its constituents\n",
    "measure_conversion = {\"Assets\":[[\"Assets\"],[\"AssetsNoncurrent\", \"AssetsCurrent\"]],\n",
    "                    \"Liabilities\":[[\"Liabilities\"],[\"LiabilitiesCurrent\", \"LiabilitiesNoncurrent\"]],\n",
    "                    \"AssetsCurrent\":[[\"AssetsCurrent\"]],\n",
    "                    \"LiabilitiesCurrent\":[[\"LiabilitiesCurrent\"]],\n",
    "                    \"revenues\": [[\"revenues\"]]\n",
    "}\n",
    "\n",
    "#Lookup table for the undeprecated version of a measure\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\deprecated_to_current.json\", \"r\") as file:\n",
    "    deprecate_conversion = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "#The first entry date into the EDGARD database\n",
    "START = datetime.strptime('1993-01-01', r\"%Y-%m-%d\")\n",
    "\n",
    "def runlist(dict, nameslist):\n",
    "    idx = 0\n",
    "    while (idx<len(nameslist)):\n",
    "        try:\n",
    "            data = dict[nameslist[idx]]\n",
    "            return data\n",
    "        except KeyError:\n",
    "            idx +=1\n",
    "    return False\n",
    "\n",
    "def searchdict(dict, nameslist, ticker):\n",
    "    for key in [\"dei\", \"us-gaap\", \"ifrs-full\", \"invest\"]:\n",
    "        try:\n",
    "            data = runlist(dict[key], nameslist) \n",
    "            if  data != False:\n",
    "                return data\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return False\n",
    "    # raise KeyError(f\"{ticker} doesnt have any of {nameslist}\")\n",
    "\n",
    "\n",
    "def getcik(ticker):\n",
    "    #Convert the ticker into the proper cik\n",
    "    for key,value in cikdata.items():\n",
    "        if value[\"ticker\"] == ticker:\n",
    "            cik = value[\"cik_str\"]\n",
    "            break\n",
    "    return str(cik).zfill(10)\n",
    "\n",
    "#Headers for EDGAR call\n",
    "headers = {\n",
    "    \"User-Agent\":\"ficakc@seznam.cz\",\n",
    "    \"Accept-Encoding\":\"gzip, deflate\",\n",
    "}\n",
    "\n",
    "TIMEOUT = 8\n",
    "RETRIES = 2\n",
    "START_RETRY_DELAY = 1\n",
    "# cik_url =  \"https://www.sec.gov/files/company_tickers.json\"\n",
    "# cikdata = requests.get(cik_url, headers=headers).json()\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\cik.json\",\"r\") as file:\n",
    "    cikdata = json.load(file)\n",
    "    file.close()\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\apple.json\",\"r\") as file:\n",
    "    Apple = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def sync_companyfacts(ticker:str):\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    data  = httpx.get(data_url, headers= headers)\n",
    "    return data\n",
    "    \n",
    "async def companyfacts(ticker:str, client, semaphore):\n",
    "    #Get all the financial data for a ticker\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    facts = await fetch(data_url, headers, semaphore, client, TIMEOUT,RETRIES,START_RETRY_DELAY)\n",
    "    return facts\n",
    "\n",
    "def endtodatetime(dataframe):\n",
    "    dataframe.loc[:,\"end\"] = pd.datetime(dataframe[\"end\"])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "class Stock:\n",
    "    def __init__(self, ticker:str):\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = getcik(self.ticker)\n",
    "        \n",
    "    async def async_init(self,client, semaphore, standard_measures):\n",
    "        #Get all of the data for the company, ALL of it \n",
    "        data = await companyfacts(self.ticker, client, semaphore)\n",
    "        #If the response wasn't recieved, skips the rest of the code \n",
    "        if type(data) != int:\n",
    "            self.data = data.json()\n",
    "        else:\n",
    "            return 0\n",
    "        #Get the share amount \n",
    "        self.share_name_list = [\"EntityCommonStockSharesOutstanding\", \"CommonStockSharesOutstanding\", \"WeightedAverageNumberOfSharesOutstandingBasic\", \"WeightedAverageNumberOfDilutedSharesOutstanding\"]\n",
    "        meta = copy.deepcopy(self.data[\"facts\"])\n",
    "        #searches the company dict for the first occurence of something in the names list\n",
    "        share_info = searchdict(meta, self.share_name_list, self.ticker)\n",
    "        share_date= datetime.strptime(share_info[\"units\"][\"shares\"][0][\"end\"], r\"%Y-%m-%d\")\n",
    "        #Get the earliest date with all the info about the company\n",
    "        start_dates = []\n",
    "        for measure in standard_measures:\n",
    "            if measure in deprecate_conversion:\n",
    "                fact = searchdict(self.data[\"facts\"],deprecate_conversion[measure],self.ticker)\n",
    "                if fact ==False:\n",
    "                    continue\n",
    "                date = datetime.strptime(fact[\"units\"][\"USD\"][0][\"end\"],r\"%Y-%m-%d\")\n",
    "            else:\n",
    "                fact =searchdict(self.data[\"facts\"],measure,self.ticker)\n",
    "                if fact ==False:\n",
    "                    continue\n",
    "                date = datetime.strptime(fact[\"units\"][\"USD\"][0][\"end\"],r\"%Y-%m-%d\")\n",
    "            start_dates.append(date)\n",
    "        self.start_year = max([share_date] + start_dates)\n",
    "        self.end_year = datetime.now().date()\n",
    "        return 1\n",
    "    async def price_init(self,semaphore):\n",
    "        #Get the price and set the self.price\n",
    "        self.fullprice = await yahoo_fetch(self.ticker,self.start_year, self.end_year, semaphore, RETRIES, START_RETRY_DELAY)\n",
    "        if type(self.fullprice) == int:\n",
    "            return 0\n",
    "        self.fullprice = self.fullprice.reset_index()\n",
    "        Price = self.fullprice[[self.fullprice.columns[0],\"close\", \"adjclose\"]].copy()\n",
    "        Price[\"end\"] = Price[\"index\"].astype(str)\n",
    "        Price.drop(columns=[\"index\"],inplace=True)\n",
    "        date_range = pd.date_range(start=self.start_year, end=self.end_year).astype(str)\n",
    "        self.date_range = pd.DataFrame(date_range, columns=['end'])\n",
    "        Price = pd.merge(self.date_range, Price, on = [\"end\"],how=\"left\" )\n",
    "        self.price = Price.ffill().bfill()\n",
    "        return 1 \n",
    "    def fact(self,measure,simple=True):\n",
    "        #Propagate the 0 \n",
    "        if self.data == 0:\n",
    "            return   \n",
    "        try:\n",
    "            if measure in deprecate_conversion:\n",
    "                measure = deprecate_conversion[measure]\n",
    "                # frame = pd.concat([frame, frame_undep], axis=0).reset_index(drop=True)\n",
    "            point_list = self.data[\"facts\"][\"us-gaap\"][measure][\"units\"][\"USD\"]\n",
    "            frame = pd.DataFrame(point_list)\n",
    "            frame = frame.drop_duplicates(subset='end', keep='last')\n",
    "            frame[measure] = frame[\"val\"]\n",
    "            if simple:\n",
    "                frame = frame[[\"end\", measure]]\n",
    "            #If the measure is deprecated switch to the undeprecated version\n",
    "            frame = pd.merge(self.date_range,frame,on=\"end\",how=\"left\")\n",
    "            frame = frame.ffill().bfill()\n",
    "            return frame\n",
    "        except KeyError:\n",
    "            print(f\"Measure {measure} not available for {self.ticker}.\")\n",
    "    def shares(self,simple=True):\n",
    "        #Propagate the 0 \n",
    "        if self.data == 0:\n",
    "            return 0\n",
    "        if simple:\n",
    "            meta = copy.deepcopy(self.data[\"facts\"])\n",
    "            share_count = pd.DataFrame(searchdict(meta,self.share_name_list)[\"units\"][\"shares\"][0], index=[0])[[\"end\",\"val\"]]\n",
    "        share_count[\"shares\"] = share_count[\"val\"]\n",
    "        share_count.drop(columns=[\"val\"], inplace = True)\n",
    "        share_count = share_count.drop_duplicates(subset=\"end\", keep=\"last\")\n",
    "        share_count = pd.merge(self.date_range, share_count, on=[\"end\"], how=\"left\")\n",
    "        return share_count.ffill()\n",
    "    \n",
    "\n",
    "#Initializes and appends the stock object\n",
    "async def async_task(ticker, client, semaphore_edgar, semaphore_yahoo, measures):\n",
    "    # Measures are used to get the date when all the financial info is available\n",
    "    stock = Stock(ticker)\n",
    "    print(f\"Currently pinging {ticker}\")\n",
    "    succesful_edgar = await stock.async_init(client,semaphore_edgar,measures)\n",
    "    if succesful_edgar:\n",
    "        print(f\"Price pinging {ticker}$\")\n",
    "        succesful_price = await stock.price_init(semaphore_yahoo)\n",
    "    with open(f'C:\\Programming\\Python\\Finance\\EDGAR\\companies\\{ticker}.pkl', 'wb') as file:\n",
    "        pickle.dump(stock,file)\n",
    "    del stock\n",
    "    #Return (ticker, availability of data, availability of price)\n",
    "    return (ticker, succesful_edgar, succesful_price)\n",
    "\n",
    "\n",
    "def acquire_frame(ticker, measures, indicator_frame):\n",
    "    #Get a dataframe from the saved data of some stock \n",
    "    try:\n",
    "        with open(f'C:\\Programming\\Python\\Finance\\EDGAR\\companies\\{ticker}.pkl', 'rb') as file:\n",
    "            stock = pickle.load(file)\n",
    "            file.close()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{ticker} is not available for loading\")\n",
    "    try:\n",
    "        #Price and shares oustanding \n",
    "        shares = stock.shares().copy()\n",
    "        stock_price = stock.price.copy()\n",
    "        df = pd.merge(shares, stock_price, left_on=[\"end\"], right_on=[\"end\"], how = \"left\")\n",
    "        frames_list = [stock.fact(measure) for measure in measures]\n",
    "        for frame in frames_list:\n",
    "            df = pd.merge(df,frame, on=[\"end\"], how=\"left\")\n",
    "    except AttributeError:\n",
    "        return\n",
    "    #Economic indicators \n",
    "    df = pd.merge(df, indicator_frame, left_on =[\"end\"], right_on=[\"index\"], how=\"left\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\"TB3MS\", \"DCOILWTICO\"]\n",
    "indicator_frame = fred_info(indicators, START, datetime.now().date())\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\indicator_frame.pkl\", \"wb\") as file:\n",
    "    pickle.dump(indicator_frame, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently pinging AAPL\n",
      "Currently pinging MSFT\n",
      "Currently pinging GOOGL\n",
      "Currently pinging AMZN\n",
      "Currently pinging NVDA\n",
      "Currently pinging META\n",
      "Currently pinging TSLA\n",
      "Currently pinging BRK-B\n",
      "Currently pinging LLY\n",
      "Currently pinging TSM\n",
      "Currently pinging V\n",
      "Currently pinging AVGO\n",
      "Currently pinging JPM\n",
      "Currently pinging UNH\n",
      "Currently pinging NVO\n",
      "Currently pinging SPY\n",
      "Currently pinging WMT\n",
      "Currently pinging LVMUY\n",
      "Currently pinging XOM\n",
      "Currently pinging MA\n",
      "Currently pinging JNJ\n",
      "Currently pinging LTMAY\n",
      "Currently pinging HD\n",
      "Currently pinging PG\n",
      "Currently pinging ASML\n",
      "Currently pinging COST\n",
      "Currently pinging ORCL\n",
      "Currently pinging CVX\n",
      "Currently pinging MRK\n",
      "Currently pinging ABBV\n",
      "Price pinging GOOGL$\n",
      "Price pinging AAPL$\n",
      "Price pinging META$\n",
      "Price pinging BRK-B$\n",
      "Price pinging MSFT$\n",
      "Price pinging LLY$\n",
      "Price pinging TSLA$\n",
      "Price pinging AMZN$\n",
      "Price pinging TSM$\n",
      "Price pinging NVDA$\n",
      "Price pinging AVGO$\n",
      "Price pinging V$\n",
      "Error response 404 for https://data.sec.gov/api/xbrl/companyfacts/CIK0000884394.json\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'succesful_price' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# A list of tuples with the availability\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m company_frames_availability \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[async_task(ticker, edgar_client, sem_edgar, sem_yahoo, measures) \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m ticker_list])\n",
      "Cell \u001b[1;32mIn[24], line 183\u001b[0m, in \u001b[0;36masync_task\u001b[1;34m(ticker, client, semaphore_edgar, semaphore_yahoo, measures)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m stock\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m#Return (ticker, availability of data, availability of price)\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (ticker, succesful_edgar, \u001b[43msuccesful_price\u001b[49m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'succesful_price' where it is not associated with a value"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 404 for https://data.sec.gov/api/xbrl/companyfacts/CIK0000824046.json\n",
      "Price pinging JPM$\n",
      "Price pinging LTMAY$\n",
      "Price pinging XOM$\n",
      "Price pinging WMT$\n",
      "Price pinging UNH$\n",
      "Price pinging ASML$\n",
      "Price pinging HD$\n",
      "Price pinging ORCL$\n",
      "Price pinging CVX$\n",
      "Price pinging COST$\n",
      "Price pinging JNJ$\n",
      "Price pinging MA$\n",
      "Price pinging PG$\n",
      "Price pinging ABBV$\n",
      "Price pinging MRK$\n"
     ]
    }
   ],
   "source": [
    "#write out measures based on importance in descending order\n",
    "# tracemalloc.start()\n",
    "measures = [\"Assets\", \"Liabilities\", \"AssetsCurrent\", \"LiabilitiesCurrent\"]\n",
    "#Load out the indicators \n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\indicator_frame.pkl\", \"rb\") as file:\n",
    "    indicator_frame = pickle.load(file)\n",
    "#Get the first n companies sorted by market cap \n",
    "companies_num = 30\n",
    "comp = 0\n",
    "edgar_client =  httpx.AsyncClient()\n",
    "sem_edgar = asyncio.Semaphore(9)\n",
    "#Separate sem for yahoo to spread the work and connections\n",
    "sem_yahoo = asyncio.Semaphore(9)\n",
    "#GATHER THE FIRST companies_num companies ciks and pass them to the gather with the tasks\n",
    "ticker_list = []\n",
    "for company, values in cikdata.items():\n",
    "    if comp<companies_num:\n",
    "        ticker_list.append(values[\"ticker\"])\n",
    "        comp+=1\n",
    "    else:\n",
    "        break\n",
    "# A list of tuples with the availability\n",
    "company_frames_availability = await asyncio.gather(*[async_task(ticker, edgar_client, sem_edgar, sem_yahoo, measures) for ticker in ticker_list])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure Liabilities not available for AMZN.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can only merge Series or DataFrame objects, a <class 'NoneType'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m company_frames_tuples \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43macquire_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindicator_frame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalue_edg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalue_yah\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompany_frames_availability\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue_edg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue_yah\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m company_frames_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m company_frames_tuples:\n",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m company_frames_tuples \u001b[38;5;241m=\u001b[39m [(ticker,\u001b[43macquire_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindicator_frame\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m ticker,value_edg,value_yah \u001b[38;5;129;01min\u001b[39;00m company_frames_availability \u001b[38;5;28;01mif\u001b[39;00m value_edg \u001b[38;5;129;01mand\u001b[39;00m value_yah]\n\u001b[0;32m      2\u001b[0m company_frames_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m company_frames_tuples:\n",
      "Cell \u001b[1;32mIn[43], line 183\u001b[0m, in \u001b[0;36macquire_frame\u001b[1;34m(ticker, measures, indicator_frame)\u001b[0m\n\u001b[0;32m    181\u001b[0m     frames_list \u001b[38;5;241m=\u001b[39m [stock\u001b[38;5;241m.\u001b[39mfact(measure) \u001b[38;5;28;01mfor\u001b[39;00m measure \u001b[38;5;129;01min\u001b[39;00m measures]\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames_list:\n\u001b[1;32m--> 183\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:152\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    151\u001b[0m     left_df \u001b[38;5;241m=\u001b[39m _validate_operand(left)\n\u001b[1;32m--> 152\u001b[0m     right_df \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_operand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    155\u001b[0m             left_df,\n\u001b[0;32m    156\u001b[0m             right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    167\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2632\u001b[0m, in \u001b[0;36m_validate_operand\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2633\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only merge Series or DataFrame objects, a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2634\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Can only merge Series or DataFrame objects, a <class 'NoneType'> was passed"
     ]
    }
   ],
   "source": [
    "company_frames_tuples = [(ticker,acquire_frame(ticker, measures, indicator_frame)) for ticker,value_edg,value_yah in company_frames_availability if value_edg and value_yah]\n",
    "company_frames_dict = {}\n",
    "for key,value in company_frames_tuples:\n",
    "    company_frames_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8800\\1421013583.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\Programming\\Python\\Finance\\EDGAR\\companies\\AAPL.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"br\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mapple\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mApple_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macquire_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AAPL\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"AccountsPayable\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8800\\1302156518.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(ticker, measures, indicator_frame)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m#Economic indicators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'index'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\companies\\AAPL.pkl\", \"br\") as file:\n",
    "    apple =  pickle.load(file)\n",
    "\n",
    "Apple_frame = acquire_frame(\"AAPL\",[\"AccountsPayable\"],pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple.data[\"facts\"][\"dei\"][\"EntityCommonStockSharesOutstanding\"][\"units\"][\"shares\"]\n",
    "frame  = fred_info([\"TB3MS\", \"DCOILWTICO\"], '2015-02-24', '2017-02-24')\n",
    "frame.head(40)\n",
    "# print(frame)\n",
    "\n",
    "frame[\"index\"] = frame[\"index\"].astype(str)\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\FRED.json\", \"w\") as file:\n",
    "    json.dump(frame.to_dict(orient=\"records\"), file, indent=1)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated fucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "tickers = [\"META\"]\n",
    "for ticker in tickers:\n",
    "    data = sync_companyfacts(ticker).json()\n",
    "    data = data[\"facts\"][\"us-gaap\"]\n",
    "    for key,value in data.items():\n",
    "        del value[\"units\"]\n",
    "        if not key in dictionary:\n",
    "            dictionary[key] = value\n",
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\deprecated.json\", \"w\")as file:\n",
    "    json.dump(dictionary, file, indent= 1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkout(ticker, name):\n",
    "    data = sync_companyfacts(ticker).json()\n",
    "    dict1  = data[\"facts\"][\"us-gaap\"]\n",
    "    dict2  = data[\"facts\"][\"dei\"]\n",
    "    dict = {**dict1, **dict2}\n",
    "    compdict = {}\n",
    "    for key,value in dict.items():\n",
    "        compdict[key] = value[\"description\"]\n",
    "\n",
    "    with open(f\"C:\\Programming\\Python\\Finance\\EDGAR\\{name}.json\", \"w\") as file:\n",
    "        json.dump(compdict, file, indent =1)\n",
    "        file.close()\n",
    "\n",
    "# meta = sync_companyfacts(\"META\").json()\n",
    "# dictionary = amazon[\"facts\"][\"dei\"].keys()\n",
    "# dictionary  = [i for i in dictionary]\n",
    "# with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\amazontotal.json\", \"w\") as file:\n",
    "#         json.dump(dictionary, file, indent =1)\n",
    "#         file.close()\n",
    "\n",
    "\n",
    "# print(amazon[\"facts\"].keys())\n",
    "checkout(\"META\",\"meta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Stock.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m measures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssetsCurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiabilitiesCurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m Apple \u001b[38;5;241m=\u001b[39m \u001b[43mStock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maapl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m stock \u001b[38;5;241m=\u001b[39m Apple\n\u001b[0;32m      4\u001b[0m shares \u001b[38;5;241m=\u001b[39m stock\u001b[38;5;241m.\u001b[39mshares()\n",
      "\u001b[1;31mTypeError\u001b[0m: Stock.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "measures = [\"Assets\", \"Liabilities\", \"AssetsCurrent\", \"LiabilitiesCurrent\"]\n",
    "Apple = Stock(\"aapl\", measures)\n",
    "stock = Apple\n",
    "shares = stock.shares()\n",
    "stock_num = stock.price\n",
    "if isinstance(shares, int) or isinstance(stock_num, int): \n",
    "    pass\n",
    "    # break\n",
    "df = pd.merge(shares.copy(), stock_num.copy(), on=[\"end\"], how = \"left\")\n",
    "frames_list = [stock.fact(measure) for measure in measures]\n",
    "for frame in frames_list:\n",
    "    df = pd.merge(df,frame, on=[\"end\"], how=\"left\")\n",
    "df.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Get all the measure names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\stock.json\", \"w\") as file:\n",
    "#     json.dump(df.to_dict(orient='records'), file, indent=1)\n",
    "#     file.close()\n",
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\shares.json\", \"w\") as file:\n",
    "    json.dump(stock.shares().copy().to_dict(orient='records'), file, indent=1)\n",
    "    file.close()\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\assets.json\", \"w\") as file:\n",
    "    json.dump(Apple[\"facts\"][\"us-gaap\"][\"Assets\"][\"units\"][\"USD\"], file, indent=1)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = {}\n",
    "for key,value in Apple[\"facts\"][\"us-gaap\"].items() :\n",
    "    measures[key.ljust(100)] = value[\"label\"]\n",
    "\n",
    "measures[\"METADATA\".ljust(200,\"/\")] = \"\"\n",
    "\n",
    "for key,value in Apple[\"facts\"][\"dei\"].items():\n",
    "    measures[key.ljust(100)] = value[\"label\"]\n",
    "\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\measures.json\",\"w\") as file:\n",
    "    json.dump(measures, file, indent=1)\n",
    "\n",
    "\n",
    "#create price reference list:\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\price.json\", \"w\") as file:\n",
    "    json.dump(Apple.price.to_dict(orient='records'), file, indent=1)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Apple[\"facts\"][\"us-gaap\"][\"AssetsCurrent\"][\"units\"][\"USD\"] :\n",
    "    try:\n",
    "        del key[\"frame\"]\n",
    "        print(key)\n",
    "    except KeyError:\n",
    "        print(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Apple[\"facts\"][\"us-gaap\"][\"Assets\"][\"units\"][\"USD\"][0][\"end\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
