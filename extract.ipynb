{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "from fredapi import Fred\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import yahoo_fin.stock_info as si\n",
    "# We use normal datetime for fred info and pandas datetime for data\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from itertools import islice\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from scipy.stats import hmean\n",
    "\n",
    "fred = Fred(api_key='0c34c4dd2fd6943f6549f1c990a8a0f0') \n",
    "\n",
    "async def fetch(url, url_headers, semaphore, client, timeout, max_retries, start_retry_delay):\n",
    "        #function to fetch data from some url with retries and error responces\n",
    "        for attempt in range(1,max_retries):\n",
    "            try:\n",
    "                async with semaphore:\n",
    "                    response = await client.get(url, timeout=timeout, headers= url_headers)\n",
    "                    response.raise_for_status()\n",
    "                    return response  # Successful request, exit the loop\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                    headers = response.headers\n",
    "                    #Sometimes a retry-after header is returned\n",
    "                    retry_after = headers.get('Retry-After')\n",
    "                    if retry_after != None:\n",
    "                        #Just for debugging\n",
    "                        print(retry_after)\n",
    "                        await asyncio.sleep(int(retry_after))\n",
    "                        continue\n",
    "                    if e.response.status_code == 404:\n",
    "                        return \"del\"\n",
    "                    print(f\"Error response {e.response.status_code} for {url}\")\n",
    "            except httpx.TimeoutException as e:\n",
    "                print(f\"Timeout reached: {e}\")\n",
    "                print(f\"Retrying in {attempt*start_retry_delay} seconds...\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except httpx.RequestError as e:\n",
    "                print(f\"An error occurred: {e}.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0\n",
    "                \n",
    "\n",
    "def fred_info(ids:list, start:str, end:str):\n",
    "    #Returns a dataframe with all of the indicators together\n",
    "    #start and end are datatime objects\n",
    "    start = start.strftime('%Y-%m-%d')\n",
    "    end = end.strftime('%Y-%m-%d')\n",
    "    series_list = []\n",
    "    for id in ids:\n",
    "        series = fred.get_series(id,observation_start=start, observation_end=end)\n",
    "        series_list.append(series)\n",
    "    frame = pd.concat(series_list, axis=1, join=\"outer\")\n",
    "    frame.columns = ids\n",
    "    return frame\n",
    "\n",
    "async def yahoo_fetch(ticker, start_year, end_year, semaphore, max_retries, start_retry_delay):\n",
    "    #Fetch implemented to get the price data for the company \n",
    "    async with semaphore:\n",
    "        for attempt in range(max_retries +1):\n",
    "            try:\n",
    "                response = await asyncio.to_thread(si.get_data,ticker,min(start_year[\"static\"], start_year[\"dynamic\"]), max(end_year[\"static\"], end_year[\"dynamic\"]))\n",
    "                return response  # Successful request, exit the loop\n",
    "            except requests.exceptions.ConnectionError as ce:\n",
    "                print(\"Yahoo connection error.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except Exception as e:\n",
    "                print(f\"Yahoo error:{e}\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0\n",
    "    \n",
    "TIMEOUT = 8\n",
    "RETRIES = 2\n",
    "START_RETRY_DELAY = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = comp_load(\"AAPL\")\n",
    "data_points = comp.converted_data[\"GrossProfit\"]\n",
    "# with open(\"examples/grossprofit.json\",\"w\") as file:\n",
    "#     json.dump(data_points,file, indent=1)\n",
    "duplicates = set()\n",
    "for i, datapoint in enumerate(data_points):\n",
    "    for j, datapoint2 in enumerate(data_points):\n",
    "        if i != j:  # Avoid comparing the item with itself\n",
    "            if abs((datapoint[\"end\"] - datapoint2[\"end\"]).days) < 5  and abs((datapoint[\"start\"] - datapoint2[\"start\"]).days) <  5 and datapoint[\"val\"] == datapoint2[\"val\"]:\n",
    "                # Convert datapoint2 to a tuple of its relevant attributes\n",
    "                duplicate_tuple = (datapoint2[\"start\"], datapoint2[\"end\"], datapoint2[\"val\"])\n",
    "                duplicates.add(duplicate_tuple)\n",
    "\n",
    "for start, end, val in duplicates:\n",
    "    print(f\"{start.strftime('%Y-%m-%d')} -> {end.strftime('%Y-%m-%d')}, Value: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = comp_load(\"NVDA\")\n",
    "example_save(comp.data[\"GrossProfit\"], \"grossNVDA\")\n",
    "\n",
    "# company_wordsearch(\"UNP\", \"OperatingExpenses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different names\n",
    "measure_conversion = {\"Assets\":[\"EquityAndLiabilities\", \"LiabilitiesAndStockholdersEquity\"],\n",
    "                    \"Liabilities\":[],\n",
    "                    \"AssetsCurrent\":[\"CurrentAssets\"],\n",
    "                    \"LiabilitiesCurrent\":[\"CurrentLiabilities\"],\n",
    "                    \"AssetsNoncurrent\":[\"NoncurrentAssets\"],\n",
    "                    \"LiabilitiesNoncurrent\":[\"NoncurrentLiabilities\"],\n",
    "                    \"Revenues\": [],\n",
    "                    \"AccountsPayableCurrent\": [\"AccountsPayableTradeCurrent\"],\n",
    "                    \"EntityCommonStockSharesOutstanding\": [\"CommonStockSharesOutstanding\",\"NumberOfSharesOutstanding\"],\n",
    "                    \"StockholdersEquity\":[\"Equity\",\"StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest\"],\n",
    "                    \"DerivativeLiabilities\":[\"FairValueLiabilitiesMeasuredOnRecurringBasisDerivativeFinancialInstrumentsLiabilities\"],\n",
    "                    \"DerivativeAssets\":[\"FairValueAssetsMeasuredOnRecurringBasisDerivativeFinancialInstrumentsAssets\"],\n",
    "                    \"ShortTermBorrowings\":[\"DebtCurrent\"],\n",
    "                    \"CostofRevenue\":[\"CostOfGoodsAndServicesSold\"],\n",
    "                    \"CostOfGoodsAndServicesSold\": [\"CostOfRevenue\",\"CostOfGoodsAndServiceExcludingDepreciationDepletionAndAmortization\"],\n",
    "                    \"CostOfGoodsSold\": [\"CostOfGoodsSoldExcludingDepreciationDepletionAndAmortization\"],\n",
    "                    \"Amortization\": [\"FiniteLivedIntangibleAssetsAmortizationExpenseNextTwelveMonths\",\"AmortizationOfIntangibleAssets\"],\n",
    "                    \"IncomeTaxesPaidNet\":[\"IncomeTaxesPaid\"],\n",
    "                    \"NetIncomeLoss\": [\"ProfitLoss\"],\n",
    "                    \"OperatingExpenses\": [\"OperatingCostsAndExpenses\"],\n",
    "                    \"PaymentsToAcquirePropertyPlantAndEquipment\": [\"PaymentsForCapitalImprovements\",\"CapitalExpenditure\",\"PurchasesOfPropertyPlantAndEquipment\", \"PaymentsForPropertyPlantAndEquipment\"],\n",
    "                    \"NetCashProvidedByUsedInOperatingActivities\" : [\"CashAndCashEquivalentsFromOperatingActivities\", \"OperatingActivitiesNetCashInflowsOutflows\",\"CashFlowsFromOperatingActivities\"],\n",
    "                    \"InterestIncome\" : [\"InvestmentIncomeInterest\"],\n",
    "                    \"LongTermDebtCurrent\" : [\"LongTermDebtMaturitiesRepaymentsOfPrincipalInNextTwelveMonths\"]\n",
    "}\n",
    "approximate_measure_conversion = {\n",
    "                    \"Liabilities\":[\"LiabilitiesFairValueDisclosure\"],\n",
    "                    \"AssetsCurrent\":[\"CurrentAssetsOtherThanAssetsOrDisposalGroupsClassifiedAsHeldForSaleOrAsHeldForDistributionToOwners\",],\n",
    "                    \"LiabilitiesCurrent\":[\"CurrentLiabilitiesOtherThanLiabilitiesIncludedInDisposalGroupsClassifiedAsHeldForSale\"],\n",
    "                    \"EntityCommonStockSharesOutstanding\": [\"WeightedAverageNumberOfSharesOutstandingBasic\", \"WeightedAverageNumberOfDilutedSharesOutstanding\"],\n",
    "                    \"PrepaidExpenseAndOtherAssetsCurrent\":[\"OtherAssetsCurrent\"],\n",
    "                    \"CapitalLeaseObligationsNoncurrent\": [\"OperatingLeaseLiabilityNoncurrent\"],\n",
    "                    \"CapitalLeaseObligationsCurrent\": [\"LesseeOperatingLeaseLiabilityPaymentsDueNextTwelveMonths\"],\n",
    "                    \"IntangibleAssetsNetExcludingGoodwill\":[\"FiniteLivedIntangibleAssetsNet\"],\n",
    "                    \"AccountsReceivableNet\": [\"AccountsReceivableNetCurrent\"],\n",
    "                    \"Revenues\": [\"SalesRevenueNet\",\"RevenueFromContractWithCustomerExcludingAssessedTax\"],\n",
    "                    \"CostOfGoodsSold\":[\"CostOfGoodsAndServicesSold\", \"CostOfRevenue\"],\n",
    "                    \"CostofRevenue\":[\"CostOfGoodsSold\"],\n",
    "                    \"CostOfGoodsAndServicesSold\": [\"CostOfGoodsSold\"],\n",
    "                    \"AccountsPayableAndAccruedLiabilitiesCurrent\": [\"AccountsPayableCurrent\"],\n",
    "                    \"AccountsPayableAndAccruedLiabilities\": [\"AccountsPayable\"],\n",
    "                    \"Depreciation\":[\"DepreciationAndAmortization\"],\n",
    "                    \"ShortTermBorrowings\":[\"LongTermDebtMaturitiesRepaymentsOfPrincipalInNextTwelveMonths\",\"LongTermDebtCurrent\"],\n",
    "                    \"AccountsReceivableNet\" :[\"AccountsReceivableGrossCurrent\"],\n",
    "                    \"DepreciationAndAmortization\" : [\"DepreciationDepletionAndAmortization\"],  \n",
    "}\n",
    "additive_conversion = {\"Assets\":[\"AssetsCurrent\", \"AssetsNoncurrent\"],\n",
    "                    \"Liabilities\":[\"LiabilitiesCurrent\", \"LiabilitiesNoncurrent\"],\n",
    "                    \"AccountsPayableAndAccruedLiabilitiesCurrent\":[\"AccountsPayableCurrent\", \"AccruedLiabilitiesCurrent\"],\n",
    "                    \"CashCashEquivalentsAndShortTermInvestments\": [\"CashAndCashEquivalentsAtCarryingValue\",\"ShortTermInvestments\"],\n",
    "                    \"AccountsReceivableNet\": [\"AccountsReceivableNetCurrent\", \"AccountsReceivableNetNoncurrent\"],\n",
    "                    \"CostsAndExpenses\":[\"OperatingExpenses\", \"CostOfGoodsAndServicesSold\"],\n",
    "                    \"GrossProfit\":[\"OperatingIncomeLoss\",\"DepreciationAndAmortization\"],\n",
    "                    \"AccountsPayableAndAccruedLiabilities\": [\"AccruedLiabilities\", \"AccountsPayable\"],\n",
    "                    \"AccountsPayableAndAccruedLiabilitiesCurrent\": [\"AccruedLiabilitiesCurrent\", \"AccountsPayableCurrent\"],\n",
    "                    \"LongTermDebt\": [\"LongTermDebtCurrent\",\"LongTermDebtNoncurrent\"],\n",
    "                    \"DepreciationAndAmortization\": [\"Depreciation\", \"AmortizationOfIntangibleAssets\"],\n",
    "                    \"OperatingIncomeLoss\":[\"NetIncomeLoss\",\"IncomeTaxesPaidNet\", \"DepreciationAndAmortization\",\"InterestExpense\"],\n",
    "                    \"PaymentsToAcquirePropertyPlantAndEquipment\": [\"PaymentsToAcquireProductiveAssets\", \"PaymentsToAcquireOtherPropertyPlantAndEquipment\"],\n",
    "                    \"CapitalExpenditure\":[\"PaymentsToAcquireEquipmentOnLease\",\"PaymentsToAcquireOilAndGasPropertyAndEquipment\",\"PaymentsToAcquireOtherProductiveAssets\",\"PaymentsToAcquireProductiveAssets\"],\n",
    "                    \"Revenue\": [\"GrossProfit\", \"CostOfGoodsAndServicesSold\"],\n",
    "                    \"InterestExpense\": [\"InterestPaidNet\", \"InterestIncome\"],\n",
    "                    \"LongTermDebt\": [\"LongTermDebtMaturitiesRepaymentsOfPrincipalAfterYearFive\",\"LongTermDebtMaturitiesRepaymentsOfPrincipalInYearFive\", \"LongTermDebtMaturitiesRepaymentsOfPrincipalInYearFour\",\"LongTermDebtMaturitiesRepaymentsOfPrincipalInYearThree\",\"LongTermDebtMaturitiesRepaymentsOfPrincipalInYearTwo\",\"LongTermDebtMaturitiesRepaymentsOfPrincipalInNextTwelveMonths\"]\n",
    "}       \n",
    "approximate_additive_conversion = {\n",
    "                    \"LiabilitesCurrent\":[\"ShortTermBorrowings\",\"AccountsPayableAndAccruedLiabilitiesCurrent\",\"TaxesPayableCurrent\", \"DividendsPayableCurrent\",\"OtherLiabilitiesCurrent\"],\n",
    "                    \"AssetsCurrent\":[\"CashCashEquivalentsAndShortTermInvestments\",\"AccountsReceivableNetCurrent\", \"CapitalLeaseObligationsCurrent\",\"InventoryNet\",\"PrepaidExpenseAndOtherAssetsCurrent\"],\n",
    "                    \"LiabilitesNoncurrent\":[\"LongTermDebtNoncurrent\",\"CapitalLeaseObligationsNoncurrent\",\"DeferredTaxLiabilities\",\"PensionAndOtherPostretirementDefinedBenefitPlansLiabilitiesNoncurrent\",\"CapitalLeaseObligationsNoncurrent\",\"DeferredRevenue\",\"OtherLiabilitiesNoncurrent\"],\n",
    "                    \"AssetsNoncurrent\": [\"PropertyPlantAndEquipmentNet\",\"IntangibleAssetsNetExcludingGoodwill\",\"AccountsReceivableNetNoncurrent\",\"OtherAssetsNoncurrent\"], #FINISH WITH GPT-4\n",
    "                    \n",
    "}\n",
    "subtract_conversion = {\"Liabilities\":[\"Assets\",\"StockholdersEquity\"],\n",
    "                        \"LiabilitiesNoncurrent\": [\"Liabilities\",\"LiabilitiesCurrent\"],\n",
    "                        \"AssetsNoncurrent\": [\"Assets\", \"AssetsCurrent\"],\n",
    "                        \"CostOfRevenue\":[\"Revenues\",\"GrossProfit\"],\n",
    "                        \"GrossProfit\": [\"Revenues\",\"CostOfGoodsAndServicesSold\"],\n",
    "                        \"OperatingIncomeLoss\":[\"GrossProfit\", \"OperatingExpenses\"],\n",
    "                        \"Depreciation\":[\"DepreciationDepletionAndAmortization\",\"AmortizationOfIntangibleAssets\"],\n",
    "                        \"CostsAndExpenses\":[\"Revenues\",\"NetIncomeLoss\"],\n",
    "                        \"CostOfGoodsAndServicesSold\": [\"CostsAndExpenses\",\"OperatingExpenses\"],\n",
    "                        \"FreeCashFlow\": [\"NetCashProvidedByUsedInOperatingActivities\", \"PaymentsToAcquirePropertyPlantAndEquipment\"]\n",
    "}\n",
    "optional = [\"DividendsPayableCurrent\",\n",
    "            \"CapitalLeaseObligationsCurrent\",\n",
    "            \"PensionAndOtherPostretirementDefinedBenefitPlansLiabilitiesNoncurrent\",\n",
    "            \"CapitalLeaseObligationsNoncurrent\",\n",
    "            \"DeferredRevenue\",\n",
    "            \"AccountsReceivableNetNoncurrent\",\n",
    "            \"PaymentsToAcquireOtherPropertyPlantAndEquipment\"\n",
    "            ]\n",
    "\n",
    "#used in unitrun\n",
    "valid_units = ['USD','shares','USD/shares', 'Year', 'Entity', 'Segment', 'USD/Contract', 'Job',  'pure', 'USD/Investment', 'Position']\n",
    "all_units = ['Patent', 'USD', 'Restaurant', 'CNY', 'count', 'former_employee', 'State', 'state', 'membership', 'USD/rights', 'companies', 'Contracts', 'JPY/USD', 'EUR/shares', 'Cases', 'CHF/EUR', 'reportable_unit', 'businesses', 'stores', 'USD/warrant', 'employees', 'reportable_segments', 'derivative', 'Property', 'Employees', 'interest_rate_swap', 'USD/EUR', 'positions', 'Store', 'country', 'USD/Investment', 'CNY/shares', 'USD/shares_unit', 'Reporting_Unit', 'MXN/USD', 'item', 'day', 'uSDollarPerTonne', 'Rate', 'BRL', 'reportablesegments', 'LegalMatter', 'business_segment', 'Interest_Rate_Swap', 'JPY/EUR', 'plan', 'INR/shares', 'JPY', 'JPY/shares', 'numberofprojects', 'EUR', 'unit', 'Years', 'Job', 'years', 'USD/Decimal', 'instrument', 'GBP/EUR', 'reportable_segment', 'percentage', 'Contract', 'Plaintiff', 'TWD/shares', 'TWD', 'Account', 't', 'businessSegment', 'CHF', 'year', 'Positions', 'Projects', 'acquisitions', 'TWD/EUR', 'shares', 'GBP/shares', 'classAction', 'Interest_Rate_Swaps', 'reporting_unit', 'Investment', 'segement', 'Wells', 'segments', 'warehouse', 'AUD/EUR', 'Ground', 'project', 'Segments', 'reportableSegments', 'USD/Contract', 'GBP', 'Derivative', 'case_filed', 'BusinessSegment', 'DKK/shares', 'Acquisition', 'position', 'CNY/USD', 'location', 'defendant', 'operatingSegment', 'Year', 'Operating_segments', 'Y', 'company', 'Business', 'Tonne', 'plaintiff', 'businesscombinations', 'Derivatives', 'Reportable_Segments', 'HKD', 'MYR', 'units', 'operating_segments', 'employee', 'CHF/shares', 'EUR/USD', 'AED', 'patent', 'USD/Right', 'numberofyears', 'Segment', 'customer', 'subsidiaries', 'MXN/shares', 'legalmatter', 'NumberofBusinesses', 'acre', 'DKK', 'Option', 'sqft', 'Entity', 'Business_Segments', 'Employee', 'Person', 'claims', 'states', 'MXN', 'CAD/EUR', 'CAD', 'Location', 'account', 'reportableSegment', 'securities', 'Project', 'VEF/USD', 'business_unit', 'Country', 'Acquistions', 'Operating_Segment', 'Position', 'current_employees', 'Land', 'segment', 'CAD/shares', 'business', 'AUD', 'entity', 'acquisition', 'legal_action', 'countries', 'claim', 'D', 'Day', 'lawsuit', 'USD/PartnershipUnit', 'security', 'Percent', 'ZAR', 'INR/EUR', 'individuals', 'Stock_options', 'USD/shares', 'INR', 'pure', 'Businesses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datapoint reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timediff(a,b):\n",
    "    return abs((a-b).days)\n",
    "\n",
    "def reshape(measure,datapoint_list, annual = False):\n",
    "    #Reshapes the datapoint list so that its indexed by end and each item retains its attrs\n",
    "    #Designed to be used after data is converted to datetime\n",
    "    #True if it's static\n",
    "    dynamic = True\n",
    "    if \"start\" not in datapoint_list[0]: \n",
    "        dynamic = False\n",
    "    elif pd.isna(datapoint_list[0][\"start\"]):\n",
    "        dynamic = False\n",
    "    if dynamic ==False:\n",
    "        reshaped_data = {}\n",
    "        for item in datapoint_list:\n",
    "            date = item[\"end\"]\n",
    "            if date not in reshaped_data:\n",
    "                reshaped_data[date] = []\n",
    "            reshaped_data[date].append({\n",
    "                \"val\": item[\"val\"],\n",
    "                # \"accn\": item[\"accn\"],\n",
    "                # \"fy\": item[\"fy\"],\n",
    "                # \"fp\": item[\"fp\"],\n",
    "                # \"form\": item[\"form\"],\n",
    "                \"filed\": item[\"filed\"],\n",
    "                # \"frame\": item.get(\"frame\") \n",
    "            })\n",
    "        return reshaped_data, dynamic\n",
    "    else:\n",
    "        if annual:\n",
    "        #We need the yearly values\n",
    "            connected = []\n",
    "            for datapoint in datapoint_list:\n",
    "                if datapoint[\"form\"] in [\"8-K\",\"10-K/A\", \"10-K\", \"20-F\", \"6-K\"] and timediff(datapoint[\"start\"], datapoint[\"end\"])>340:\n",
    "                    connected.append(datapoint)\n",
    "        else:      \n",
    "            #We need to get quartertly data for all of the quarters \n",
    "            #This should create data that is connected, whenever there is a link missing, we construct it\n",
    "            #The data is sorted by the end date \n",
    "            #When two points are used to infer data the later filing date is asigned to the new point \n",
    "            for datapoint in datapoint_list:\n",
    "                datapoint[\"dur\"] = timediff(datapoint[\"end\"],datapoint[\"start\"])\n",
    "            connected = []\n",
    "            wanted_end = datapoint_list[0][\"end\"]\n",
    "            total_end = datapoint_list[-1][\"end\"]\n",
    "            while(wanted_end < total_end):\n",
    "                for datapoint in datapoint_list:\n",
    "                    missing = True\n",
    "                    if timediff(wanted_end,datapoint[\"end\"]) < 10 and datapoint[\"dur\"] < 100:\n",
    "                        connected.append(datapoint)\n",
    "                        wanted_end = datapoint[\"end\"] + pd.Timedelta(days=91)\n",
    "                        missing = False\n",
    "                #missing \n",
    "                #Find the interval of possible points to use to infer\n",
    "                useful_ends = (wanted_end -pd.Timedelta(days=100),wanted_end + pd.Timedelta(days=370))\n",
    "                pieces =[]\n",
    "                for datapoint in datapoint_list:\n",
    "                    if useful_ends[0]<datapoint[\"end\"] <useful_ends[1]:\n",
    "                        pieces.append(datapoint)\n",
    "                synthesised = False\n",
    "                candidates = []\n",
    "                for i,piece1 in enumerate(pieces,start=1):\n",
    "                    for piece2 in pieces[i:]:\n",
    "                        if abs(piece1[\"dur\"] - piece2[\"dur\"]) <100: #If the periods have a difference representing a quarter\n",
    "                            if piece1[\"dur\"] > piece2[\"dur\"]: #Piece one is the longer duration \n",
    "                                if timediff(piece1[\"end\"],piece2[\"end\"]) <6: #If they match by their ends \n",
    "                                    candidates.append({\"end\": piece2[\"start\"], \"start\":piece1[\"start\"], \"val\": piece1[\"val\"]-piece2[\"val\"], \"filed\": max([piece1[\"filed\"], piece2[\"filed\"]])})\n",
    "\n",
    "                                elif (timediff(piece1[\"start\"], piece2[\"start\"])) < 6: #If they match by their starts\n",
    "                                    candidates.append({\"end\": piece1[\"end\"], \"start\": piece2[\"end\"], \"val\": piece1[\"val\"]-piece2[\"val\"], \"filed\": max([piece1[\"filed\"], piece2[\"filed\"]])})\n",
    "\n",
    "                            elif piece1[\"dur\"] <piece2[\"dur\"]:\n",
    "                                if timediff(piece1[\"end\"],piece2[\"end\"]) <6: #If they match by their ends \n",
    "                                    candidates.append({\"end\": piece1[\"start\"], \"start\":piece2[\"start\"], \"val\": piece2[\"val\"]-piece1[\"val\"], \"filed\": max([piece1[\"filed\"], piece2[\"filed\"]])})\n",
    "\n",
    "                                elif (timediff(piece1[\"start\"], piece2[\"start\"])) < 6: #If they match by their starts\n",
    "                                    candidates.append({\"end\": piece2[\"end\"], \"start\": piece1[\"end\"], \"val\": piece2[\"val\"]-piece1[\"val\"], \"filed\": max([piece1[\"filed\"], piece2[\"filed\"]])})\n",
    "                            if candidates != []:\n",
    "                                filed = candidates[0][\"filed\"]\n",
    "                                index=0\n",
    "                                for i, candidate in enumerate(candidates[1:],start=1):\n",
    "                                    if candidate[\"filed\"] < filed:\n",
    "                                        filed = candidate[\"filed\"]\n",
    "                                        index = i\n",
    "                                diff = candidates[index]\n",
    "                                if timediff(diff[\"end\"], wanted_end) <10: #If the ends match we have the point\n",
    "                                    diff[\"special\"] = \"synth\"\n",
    "                                    connected.append(diff)\n",
    "                                    synthesised = True\n",
    "                                    wanted_end = diff[\"end\"] + pd.Timedelta(days=91)\n",
    "                if synthesised == False and missing == True:\n",
    "                    connected.append({\"end\": wanted_end, \"start\": wanted_end + pd.Timedelta(days=91), \"val\": np.nan, \"filed\": pd.Timestamp(year=1993, month=1, day=1)})\n",
    "                    wanted_end = wanted_end + pd.Timedelta(days=91)\n",
    "        #After getting the connected data, treat it the same as static so that you can use the same method and have consistency\n",
    "        reshaped_data = {}\n",
    "        for item in connected:\n",
    "            date = item[\"end\"]\n",
    "            if date not in reshaped_data:\n",
    "                reshaped_data[date] = []\n",
    "            reshaped_data[date].append({\n",
    "                \"val\": item[\"val\"],\n",
    "                \"start\": item[\"start\"],\n",
    "                # \"accn\": item[\"accn\"],\n",
    "                # \"fy\": item[\"fy\"],\n",
    "                # \"fp\": item[\"fp\"],\n",
    "                # \"form\": item[\"form\"],\n",
    "                \"filed\": item[\"filed\"],\n",
    "                \"special\": item.get(\"special\")\n",
    "                # \"frame\": item.get(\"frame\") \n",
    "            })\n",
    "        return reshaped_data, dynamic\n",
    "\n",
    "\n",
    "\n",
    "# apple = comp_load(\"AAPL\")\n",
    "# data= apple.converted_data[\"GrossProfit\"]\n",
    "# # point_list, unit = unitrun(data[\"units\"], apple.ticker)\n",
    "# reshaped, dynamic = reshape(\"GrossProfit\", data, annual=True)\n",
    "# # print(reshaped)\n",
    "# prev_key = pd.Timestamp(year=1993, month=1, day=1)\n",
    "# for key,value in list(reshaped.items())[1:]:\n",
    "#         # difff = timediff(key,prev_key)\n",
    "#         # if difff == 98:\n",
    "#         #     print(value)\n",
    "#         # print(difff)\n",
    "#         # prev_key = key\n",
    "#         print(value[\"GrossProfit\"][0])\n",
    "#         print(timediff(value[\"GrossProfit\"][0][\"filed\"],key))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = comp_load(\"AAPL\")\n",
    "#\"NetCashProvidedByUsedInOperatingActivities\", \"PaymentsToAcquirePropertyPlantAndEquipment\"\n",
    "frame, unit =recursive_fact(comp,\"NetCashProvidedByUsedInOperatingActivities\", dynamic_tolerance=pd.DateOffset(years=1))\n",
    "frame.to_csv(\"examples/doubleinit.csv\")\n",
    "# print(comp.start_year)\n",
    "# print(comp.end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTS_PATH = r\"C:\\Edgar_data\"\n",
    "SUBMISSIONS_PATH = r\"C:\\Submissions_data\"\n",
    "\n",
    "#Unavailable stuff\n",
    "with open(r'other_pickle\\unavailable.json', 'r') as file:\n",
    "        Unavailable_Measures = json.load(file)\n",
    "\n",
    "#Lookup table for the undeprecated version of a measure\n",
    "with open(r\"other_pickle\\deprecated_to_current.json\", \"r\") as file:\n",
    "    deprecate_conversion = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "#Categories\n",
    "with open(r\"categories\\categories.json\", \"r\") as file:\n",
    "    categories = json.load(file)\n",
    "\n",
    "#Make sure the necessary folders exist\n",
    "for category, num_range in categories.items():\n",
    "    os.makedirs(f\"companies_data\\{category}\", exist_ok=True)\n",
    "    os.makedirs(f\"companies_data_missing\\{category}\", exist_ok=True)\n",
    "\n",
    "for path in [\"checkout\", \"companies\", \"companies_data\", \"companies_data_missing\", \"units-checkout\"]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open(\"categories\\category_measures.json\", \"r\") as file:\n",
    "    category_measures = json.load(file)\n",
    "\n",
    "#The first entry date into the EDGAR database\n",
    "START = datetime.strptime('1993-01-01', r\"%Y-%m-%d\")\n",
    "\n",
    "#Easily load a company into a variable\n",
    "def comp_load(ticker):\n",
    "    with open(f\"companies\\{ticker}.pkl\", \"rb\")as file:\n",
    "        company = pickle.load(file)\n",
    "    return company\n",
    "\n",
    "def example_save(data, name):\n",
    "    with open(f\"examples/{name}.json\", \"w\") as file:\n",
    "        json.dump(data,file,indent=1)\n",
    "    print(\"Saved\")\n",
    "    \n",
    "#Manually figure out which measure is used with some company\n",
    "def company_wordsearch(ticker, word):\n",
    "    with open(f\"companies\\{ticker}.pkl\", \"rb\")as file:\n",
    "        company = pickle.load(file)\n",
    "    data = company.data\n",
    "    compdict = {}\n",
    "    for key,value in data.items():\n",
    "        compdict[key] = value[\"description\"]\n",
    "\n",
    "    matching_elements  ={}\n",
    "    for name, desc in compdict.items():\n",
    "        if word.lower() in name.lower():   \n",
    "            matching_elements[name] = desc \n",
    "    with open(f\"checkout\\{ticker}.json\", \"w\") as file:\n",
    "        json.dump(matching_elements, file, indent =1)\n",
    "    formatted_json = json.dumps(matching_elements, indent=4)\n",
    "    formatted_with_newlines = formatted_json.replace('\\n', '\\n\\n')\n",
    "    print(formatted_with_newlines)  \n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def closest_date(dates, target_date, ticker, fallback=False):\n",
    "    left, right = 0, len(dates) - 1\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = left + (right - left) // 2\n",
    "        \n",
    "        if dates[mid] < target_date:\n",
    "            left = mid + 1\n",
    "        elif dates[mid] > target_date:\n",
    "            right = mid - 1\n",
    "        else:\n",
    "            # Exact match\n",
    "            if fallback:\n",
    "                # Ensure mid-1 is within bounds\n",
    "                return dates[mid], dates[mid-1] if mid-1 >= 0 else None\n",
    "            return dates[mid]\n",
    "    \n",
    "    if left > 0:\n",
    "        if fallback:\n",
    "            # Ensure left-2 is within bounds for the second closest date\n",
    "            second_closest = dates[left-2] if left-2 >= 0 else None\n",
    "            return dates[left-1], second_closest\n",
    "        return dates[left-1]\n",
    "    else:\n",
    "        # print(f\"All dates are greater for {ticker}\")\n",
    "        return (None, None) if fallback else None\n",
    "\n",
    "def unitrun(dict, ticker, all=False, debug=False):\n",
    "    if all:\n",
    "        unit_list = all_units\n",
    "    else:\n",
    "        unit_list = valid_units\n",
    "    for unit in unit_list:\n",
    "        try:\n",
    "            return dict[unit], unit \n",
    "        except KeyError:\n",
    "            continue\n",
    "    print(f\"No unit available for {ticker}\")\n",
    "    if debug: \n",
    "        units = []\n",
    "        for key,value in dict.items():\n",
    "            units.append(key)\n",
    "        units = list(set(units))\n",
    "        with open(f\"units-checkout\\\\{ticker}.json\", \"w\") as file:\n",
    "            json.dump(units,file)\n",
    "    return False, False\n",
    "\n",
    "#Removes the top layer of the dict and returns the flattened version\n",
    "#We are then left with only the measures, no layer on top \n",
    "def flatten(data, ticker):\n",
    "    #Flatten the dataset \n",
    "    #Also removes platforms from the data and keeps staircases\n",
    "    flat_data = {}\n",
    "    for key, value in data[\"facts\"].items():\n",
    "        flat_data.update(value)\n",
    "    # missing_count = 0\n",
    "    # total_count = 0\n",
    "    filtered_count = 0\n",
    "    duplicate_count = 0\n",
    "    for measure, datapoints_units in flat_data.items():\n",
    "        datapoints, unit = unitrun(datapoints_units[\"units\"], ticker, all=True)\n",
    "        if datapoints == False:\n",
    "            continue\n",
    "        if len(datapoints) <3:\n",
    "            continue \n",
    "        filtered = []\n",
    "        duplicates = []\n",
    "        end_prev = datapoints[0][\"end\"]\n",
    "        val_prev = datapoints[0][\"val\"]\n",
    "        # Always add the first element; comparison starts from the second element\n",
    "        filtered.append(datapoints[0])\n",
    "        if \"start\" in datapoints[0] and \"start\" in datapoints[-1]:\n",
    "            start_prev = datapoints[0].get(\"start\", None)\n",
    "            for i in range(1, len(datapoints)):\n",
    "                end = datapoints[i][\"end\"]\n",
    "                val = datapoints[i][\"val\"]\n",
    "                # try:\n",
    "                start = datapoints[i].get(\"start\", None)\n",
    "                # except KeyError:\n",
    "                #     print(measure)\n",
    "                #     print(datapoints[i])\n",
    "                #     print(datapoints)\n",
    "                #     break\n",
    "                if not (end == end_prev and val == val_prev and start == start_prev): #or datapoints[i][\"form\"] in [\"8-K\",\"10-K/A\"]: #or end == end_next\n",
    "                    filtered.append(datapoints[i]) \n",
    "                    filtered_count +=1\n",
    "                else:\n",
    "                    duplicates.append(datapoints[i]) \n",
    "                    duplicate_count +=1\n",
    "                end_prev = end\n",
    "                val_prev = val\n",
    "                start_prev = start\n",
    "        else:\n",
    "            for i in range(1, len(datapoints)):\n",
    "                end = datapoints[i][\"end\"]\n",
    "                val = datapoints[i][\"val\"]\n",
    "                if not (end == end_prev and val == val_prev): #or datapoints[i][\"form\"] in [\"8-K\",\"10-K/A\"]: #or end == end_next\n",
    "                    filtered.append(datapoints[i]) \n",
    "                    filtered_count +=1\n",
    "                else:\n",
    "                    duplicates.append(datapoints[i]) \n",
    "                    duplicate_count +=1\n",
    "                end_prev = end\n",
    "                val_prev = val\n",
    "        flat_data[measure][\"units\"][unit] = filtered\n",
    "        #For each company, measure, for each unfiltered datapoint if we dont have it we add one\n",
    "        # for datapoint_duplicate in duplicates:\n",
    "        #     gotem = False\n",
    "        #     total_count +=1\n",
    "        #     for datapoint_filtered in filtered:\n",
    "        #         if datapoint_duplicate[\"val\"] == datapoint_filtered[\"val\"] and datapoint_duplicate[\"end\"] == datapoint_filtered[\"end\"]:\n",
    "        #             gotem = True\n",
    "        #     if gotem == False:\n",
    "        #         # if \"start\" in datapoint_unfiltered.keys():\n",
    "        #         missing_count +=1\n",
    "\n",
    "    print(f\"{ticker}:{(duplicate_count/(filtered_count+duplicate_count))*100}%\")\n",
    "    # print(f\"{ticker}:{(1-missing_count/total_count)*100}%\")\n",
    "    return flat_data\n",
    "\n",
    "def getcik(ticker):\n",
    "    #Convert the ticker into the proper cik\n",
    "    for key,value in cikdata.items():\n",
    "        if value[\"ticker\"] == ticker:\n",
    "            cik = value[\"cik_str\"]\n",
    "            break\n",
    "    return str(cik).zfill(10)\n",
    "\n",
    "#Headers for EDGAR call\n",
    "headers = {\n",
    "    \"User-Agent\":\"ficakc@seznam.cz\",\n",
    "    \"Accept-Encoding\":\"gzip, deflate\",\n",
    "}\n",
    "\n",
    "# cik_url =  \"https://www.sec.gov/files/company_tickers.json\"\n",
    "# cikdata = requests.get(cik_url, headers=headers).json()\n",
    "\n",
    "with open(r\"other_pickle\\cik.json\",\"r\") as file:\n",
    "    cikdata = json.load(file)\n",
    "    file.close()\n",
    "    \n",
    "def sync_companyfacts(ticker:str):\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    data  = httpx.get(data_url, headers= headers)\n",
    "    return data\n",
    "    \n",
    "async def companyfacts(ticker:str, client, semaphore):\n",
    "    #Get all the financial data for a ticker\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    facts = await fetch(data_url, headers, semaphore, client, TIMEOUT,RETRIES,START_RETRY_DELAY)\n",
    "    return facts\n",
    "\n",
    "async def companysubmissions(ticker:str, client, semaphore):\n",
    "    #Get all the financial data for a ticker\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    facts = await fetch(data_url, headers, semaphore, client, TIMEOUT,RETRIES,START_RETRY_DELAY)\n",
    "    return facts\n",
    "\n",
    "\n",
    "class Stock:\n",
    "    def __init__(self, ticker:str, standard_measures):\n",
    "        self.initialized_measures = {\"static\":[], \"dynamic\":[]}\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = getcik(self.ticker)\n",
    "        try:\n",
    "            with open(os.path.join(FACTS_PATH, f\"CIK{self.cik}.json\"), \"r\") as file:\n",
    "                self.data = flatten(json.load(file), self.ticker)\n",
    "            with open(os.path.join(SUBMISSIONS_PATH, f\"CIK{self.cik}.json\"), \"r\") as file:\n",
    "                data = json.load(file)\n",
    "                self.sic = int(data[\"sic\"])\n",
    "                self.sic_desc = data[\"sicDescription\"]\n",
    "                self.foreign = False\n",
    "                for form in [\"20-F\", \"40-F\", \"6-K\"]:\n",
    "                    if form in data[\"filings\"][\"recent\"][\"form\"]:\n",
    "                        self.foreign = True\n",
    "            \n",
    "            self.success = self.time_init(standard_measures)\n",
    "        except FileNotFoundError:\n",
    "            self.success = \"del\"\n",
    "\n",
    "    def time_init(self, standard_measures, static_start_threshold = 1, static_end_threshold = 1, dynamic_start_threshold = 1, dynamic_end_threshold = 1):\n",
    "        start_thresholds = {\"static\": static_start_threshold, \"dynamic\":dynamic_start_threshold}\n",
    "        end_thresholds = {\"static\": static_end_threshold, \"dynamic\": dynamic_end_threshold}\n",
    "        #Also serves as a filter for the companies with wrong currencies\n",
    "        #Check to see if already initialized with the measures\n",
    "        missing = False\n",
    "        self.start_year = {}\n",
    "        self.end_year = {}\n",
    "        self.date_range = {}\n",
    "        dates = {\"static\":[], \"dynamic\":[]}\n",
    "        needed_measures = []\n",
    "        #Both static and dynamic time init\n",
    "        for motion in [\"static\", \"dynamic\"]:\n",
    "            for measure in standard_measures[motion]:\n",
    "                if not measure in self.initialized_measures[motion]:\n",
    "                    missing = True\n",
    "                    break\n",
    "            if not missing:\n",
    "                #We do this because even if initialized with the same measures, thresholds could be different\n",
    "                start_index = math.ceil(start_thresholds[motion]*len(start_dates[motion])) -1 \n",
    "                end_index = math.ceil(end_thresholds[motion]*len(end_dates[motion])) -1\n",
    "                self.start_year[motion] = pd.to_datetime(start_dates[start_index], format=r\"%Y-%m-%d\")\n",
    "                self.end_year[motion] = pd.to_datetime(end_dates[end_index], format=r\"%Y-%m-%d\")\n",
    "                self.date_range[motion] = pd.date_range(start=self.start_year[motion], end=self.end_year[motion])\n",
    "                return 1\n",
    "            # Get all the constituent measures needed through recursivefact\n",
    "            for measure in standard_measures[motion]:\n",
    "                #Make a copy.deepcopy if not working \n",
    "                if measure in deprecate_conversion:\n",
    "                    measure = deprecate_conversion[measure]\n",
    "                value, unit = recursive_fact(self, measure, approx = True, printout =False, date_gather= True)\n",
    "                if value.empty != True:\n",
    "                    if unit == \"del\":\n",
    "                        print(\"returned delete\")\n",
    "                        return \"del\"\n",
    "                    dates[motion].append(value.attrs[\"date\"]) \n",
    "                    # attrs[\"measures\"] is list \n",
    "                    needed_measures += value.attrs[\"measures\"]\n",
    "                else:\n",
    "                    continue\n",
    "            if dates[motion] == []:\n",
    "                return \"del\"\n",
    "            \n",
    "            start_dates, end_dates = map(lambda x: sorted(list(x)),zip(*dates[motion]))\n",
    "            start_index = math.ceil(start_thresholds[motion]*len(start_dates)) -1 \n",
    "            end_index = math.ceil(end_thresholds[motion]*len(end_dates)) -1\n",
    "            self.start_dates = start_dates\n",
    "            self.end_dates = end_dates\n",
    "            self.start_year[motion] = pd.to_datetime(start_dates[start_index], format=r\"%Y-%m-%d\")\n",
    "            self.end_year[motion] = pd.to_datetime(end_dates[end_index], format=r\"%Y-%m-%d\")\n",
    "            self.date_range[motion] = pd.date_range(start=self.start_year[motion], end=self.end_year[motion])\n",
    "\n",
    "        #remove duplicates\n",
    "        needed_measures = list(set(needed_measures))\n",
    "        self.measures_and_dates = zip(standard_measures,start_dates,end_dates)\n",
    "        self.initialized_measures = standard_measures\n",
    "\n",
    "        #Change the strings to datetime for the initialized measures\n",
    "        # Step 1: Flatten batches into a single DataFrame with an identifier\n",
    "        all_data = []\n",
    "        for measure in needed_measures:\n",
    "            if measure in deprecate_conversion:\n",
    "                measure = deprecate_conversion[measure]\n",
    "            datapoints, unit = unitrun(self.data[measure][\"units\"], self.ticker)\n",
    "            for datapoint in datapoints:\n",
    "                datapoint[\"batch_name\"] = measure  # Add identifier\n",
    "                all_data.append(datapoint)\n",
    "\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df['filed'] = pd.to_datetime(df['filed'], format='%Y-%m-%d')\n",
    "        df['end'] = pd.to_datetime(df['end'], format='%Y-%m-%d')\n",
    "        df['start'] = pd.to_datetime(df['start'], format='%Y-%m-%d', errors='coerce')\n",
    "        separated_batches = {name: df[df['batch_name'] == name].drop(columns=['batch_name']).to_dict('records') for name in needed_measures}\n",
    "        self.converted_data = separated_batches\n",
    "        return 1 \n",
    "    \n",
    "    def date_reset(self):\n",
    "        self.initialized_measures = {\"static\":[], \"dynamic\":[]}\n",
    "\n",
    "    async def async_init(self,client, semaphore, standard_measures):\n",
    "        #Get all of the data for the company, ALL of it \n",
    "        data = await companysubmissions(self.ticker, client, semaphore)\n",
    "        #If the response wasn't recieved, skips the rest of the code \n",
    "        if type(data) == str:\n",
    "            return \"del\"\n",
    "        elif type(data) != int:\n",
    "            data = data.json()\n",
    "            self.sic = data[\"sic\"]\n",
    "            self.sicDescription = data[\"sicDescription\"]\n",
    "            return 1\n",
    "        \n",
    "    async def price_init(self,semaphore):\n",
    "        #Get the price and set the self.price\n",
    "        self.fullprice = await yahoo_fetch(self.ticker,self.start_year, self.end_year, semaphore, RETRIES, START_RETRY_DELAY)\n",
    "        if type(self.fullprice) == int:\n",
    "            return 0\n",
    "        Price = self.fullprice[[\"close\", \"adjclose\"]].copy()\n",
    "        # Price = Price.reindex(self.date_range)\n",
    "        self.price = Price.ffill().bfill()\n",
    "        return 1 \n",
    "    \n",
    "    def fact(self, measure, row_delta = pd.Timedelta(days=1), column_delta = pd.Timedelta(days=365),static_tolerance=pd.Timedelta(days =0), dynamic_row_delta=pd.Timedelta(days=1), dynamic_tolerance=pd.Timedelta(days=91), lookbehind =5, annual=False, date_gather=False):\n",
    "        \"\"\"  \n",
    "        If date_gather, then it returns a dataframe to allow recursive_fact to get the date.\n",
    "        Returns a dataframe that has rows indexed row_delta away, with lookbehind columns that are column_delta away.\n",
    "        If the data is dynamic then the row and column deltas are fixed.\n",
    "        Dynamic tolerance is how much into the future the price we are predicting is.\n",
    "        \"\"\"\n",
    "        #Propagate the 0 \n",
    "        if self.data == 0:\n",
    "            return 0\n",
    "        try:\n",
    "            #If the measure is deprecated switch to the undeprecated version\n",
    "            if measure in deprecate_conversion:\n",
    "                measure = deprecate_conversion[measure]\n",
    "                # frame = pd.concat([frame, frame_undep], axis=0).reset_index(drop=True)\n",
    "            if date_gather:\n",
    "                try:\n",
    "                    data= self.data[measure]\n",
    "                except KeyError:\n",
    "                    # return pd.DataFrame(), None\n",
    "                    return None\n",
    "                point_list, unit = unitrun(data[\"units\"], self.ticker)\n",
    "                if point_list == False:\n",
    "                    # return pd.DataFrame(), \"del\"\n",
    "                    return \"del\"\n",
    "                # frame = pd.DataFrame({'A': [1]})\n",
    "                # frame.attrs[\"date\"] = (point_list[0][\"end\"], point_list[-1][\"end\"])\n",
    "                # frame.attrs[\"measures\"] = [measure]\n",
    "                # # #Use the previous method to gather dates\n",
    "                # return frame, unit\n",
    "                return (point_list[0][\"end\"], point_list[-1][\"end\"])\n",
    "\n",
    "            \n",
    "            #Get the index dates for the datpoints for measure\n",
    "            frame_list = []\n",
    "            if measure in self.converted_data:\n",
    "                data= self.converted_data[measure]\n",
    "            else:\n",
    "                # print(f\"{self.ticker}: Data not converted or available for {measure}\")\n",
    "                return pd.DataFrame(), None\n",
    "            reshaped, dynamic = reshape(measure, data, annual)\n",
    "            if dynamic:\n",
    "                motion = \"dynamic\"\n",
    "            else:\n",
    "                motion = \"static\"\n",
    "            if dynamic == True:\n",
    "                row_delta = dynamic_row_delta\n",
    "                column_delta = pd.Timedelta(days=95)\n",
    "            tolerance = dynamic_tolerance * int(dynamic) + static_tolerance * (int(not dynamic))\n",
    "            #Vectorized version\n",
    "            dates = list(reshaped.keys())\n",
    "            #Generate all the needed row_dates in advance\n",
    "            # num_of_rows = (self.end_year[motion] - self.start_year[motion]- column_delta*(lookbehind-1)).days/row_delta.days\n",
    "            # row_dates = [[self.end_year[motion] +row_delta*l -column_delta*i for i in range(0,lookbehind)] for l in num_of_rows]\n",
    "\n",
    "            base_dates = pd.date_range(start=self.end_year[motion], end=self.start_year[motion], freq=-row_delta)\n",
    "            i_values = np.arange(lookbehind)  # An array [0, 1, ..., lookbehind-1]\n",
    "            adjustments = i_values * column_delta\n",
    "            # Broadcasted subtraction: for each date in base_dates, subtract each value in adjustments\n",
    "            row_dates = np.subtract.outer(base_dates, adjustments)\n",
    "            dimensions = row_dates.shape\n",
    "            flat_row_dates = row_dates.flatten()\n",
    "            date_series = pd.Series(flat_row_dates)\n",
    "            # Use apply to run closest_date on each date\n",
    "            results_series = date_series.apply(lambda x: closest_date(tuple(dates), x, self.ticker, fallback=True))  # m rows and n columns, replace with actual values\n",
    "            row_indexes = results_series.to_numpy().reshape(dimensions)\n",
    "            row_indexes = pd.DataFrame(row_indexes, index =base_dates)\n",
    "            #each item is a tuple of dates \n",
    "\n",
    "            frame_dict = {row_index:[] for row_index in base_dates}\n",
    "            for row_index, row in row_indexes.iterrows():\n",
    "                barrier = row_index + tolerance\n",
    "                for index_tuple in row: \n",
    "                    index, fallback_index = index_tuple\n",
    "                    nearest_filed = pd.Timestamp.min\n",
    "                    uptodate = {\"val\": np.nan}\n",
    "                    if index!= None:\n",
    "                        for value in reshaped[index]:  \n",
    "                            if nearest_filed < value[\"filed\"] <= barrier:\n",
    "                                uptodate = value\n",
    "                                nearest_filed = value[\"filed\"]\n",
    "                    if np.isnan(uptodate[\"val\"]):\n",
    "                        if fallback_index != None:\n",
    "                            for value in reshaped[fallback_index]:  \n",
    "                                if nearest_filed < value[\"filed\"] <= barrier:\n",
    "                                    uptodate = value\n",
    "                                    nearest_filed = value[\"filed\"]\n",
    "                    frame_dict[row_index].append(uptodate[\"val\"])\n",
    "\n",
    "\n",
    "            frame = pd.DataFrame.from_dict(frame_dict, orient='index')\n",
    "            frame = frame.iloc[::-1]\n",
    "            frame.columns = [f\"{measure}-{i}\" for i in range(0,lookbehind)]\n",
    "            return frame, \"Some_unit\"   \n",
    "        except KeyError as e:\n",
    "            print(f\"in fact keyerror: {e}\")\n",
    "            print(f\"Fact {measure} not available for {self.ticker}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"in fact: {e}\")\n",
    "\n",
    "def recursive_fact(comp, measure, depth=0, approx = True, row_delta = pd.Timedelta(days=1), column_delta = pd.Timedelta(days=365),static_tolerance=pd.Timedelta(days =0), dynamic_row_delta=pd.Timedelta(days=1), dynamic_tolerance=pd.Timedelta(days=91), lookbehind =5 , annual=False, printout =False, date_gather= False):\n",
    "    #The branch closes with a LookupError that gets propagated backward, means that one of the parts is missing\n",
    "    #Deletes are propagated backwards and returned\n",
    "    #If there is a loop the recursion will terminate after 4 cycles max\n",
    "    #Date gather uses the frame.attrs[\"date\"] to get the max date of all the parts that make up the measure\n",
    "    #Date gather also gets the needed measures via frame.attrs[\"measures\"]\n",
    "    #   - Date gather skips the addition of the frames so you can just use attrs[\"date\"]\n",
    "    #Printout is for debugging\n",
    "    #If approximate is set to False then the measures will be missing instead \n",
    "    if depth>4:\n",
    "        return pd.DataFrame(), None\n",
    "    if printout:\n",
    "        print(f\"Entering recursive with {measure}\")\n",
    "    value, unit = comp.fact(measure, row_delta, column_delta, static_tolerance, dynamic_row_delta,  dynamic_tolerance, lookbehind, annual, date_gather)\n",
    "    if unit == \"del\":\n",
    "        return value,unit\n",
    "    #value will be false if measure is unavailable\n",
    "    if value.empty != True:\n",
    "        return value,unit\n",
    "    #Just a different name\n",
    "    if measure in measure_conversion:\n",
    "        for replacement in measure_conversion[measure]:\n",
    "            value, unit = recursive_fact(comp, replacement, depth+1, approx,row_delta, column_delta, static_tolerance,  dynamic_row_delta, dynamic_tolerance, lookbehind, annual, printout,date_gather)\n",
    "            if value.empty == True:\n",
    "                pass\n",
    "            else:\n",
    "                #We dont need to use date attrs here, because this just passes on the date without change\n",
    "                return value, unit\n",
    "    #Not stated, but can be derived by adding\n",
    "    if measure in additive_conversion:\n",
    "        values = []\n",
    "        if additive_conversion[measure] != []:\n",
    "            abort = False\n",
    "            for part in additive_conversion[measure]:\n",
    "                if abort == False:\n",
    "                    add_value, unit = recursive_fact(comp,part,depth+1,approx,row_delta, column_delta,static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual, printout,date_gather)\n",
    "                    if unit == \"del\":\n",
    "                        return add_value, unit\n",
    "                    if add_value.empty == True:\n",
    "                        if part not in optional:\n",
    "                            abort = True\n",
    "                        continue\n",
    "                    values.append(add_value)\n",
    "            if not abort:\n",
    "                #Get all the possible indexes for the frame\n",
    "                frame1 = values[0]\n",
    "                for idx, value in enumerate(values[1:], start=1):\n",
    "                    frame1, value = frame1.align(value, join=\"outer\", axis=0)\n",
    "                    values[idx] = value\n",
    "                    \n",
    "                result_frame = pd.DataFrame(index=frame1.index)\n",
    "                for i, col in enumerate(frame1.columns):\n",
    "                    result_frame[f'{measure} {i - lookbehind +1}'] = frame1[col]  # Initialize with frame1's columns\n",
    "                for value in values:\n",
    "                    for i, col in enumerate(value.columns):\n",
    "                        result_frame[f'{measure} {i - lookbehind +1}'] = result_frame[f'{measure} {i - lookbehind +1}'].add(value[col])\n",
    "\n",
    "                return result_frame, unit\n",
    "    #Not stated, but can be inferred by subtracting\n",
    "    if measure in subtract_conversion:\n",
    "        add, unit = recursive_fact(comp,subtract_conversion[measure][0],depth+1, approx,row_delta, column_delta, static_tolerance,dynamic_row_delta, dynamic_tolerance, lookbehind, annual,printout,date_gather)\n",
    "        #Things in subtract should have the same unit so one check is ok.\n",
    "        if unit == \"del\":\n",
    "            return add,unit \n",
    "        sub, unit = recursive_fact(comp,subtract_conversion[measure][1],depth+1, approx,row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual,printout,date_gather)\n",
    "        if add.empty != True and sub.empty !=True:\n",
    "            #Get all the possible indexes for the frame\n",
    "            values = [add, sub]\n",
    "            add, sub = add.align(sub, join=\"outer\", axis=0)\n",
    "            result_frame = pd.DataFrame(index=add.index)\n",
    "            for i, col in enumerate(add.columns):\n",
    "                result_frame[f'{measure} {i - lookbehind +1}'] = add[col]  # Initialize with add's columns\n",
    "            for i, col in enumerate(sub.columns):\n",
    "                result_frame[f'{measure} {i - lookbehind +1}'] = result_frame[f'{measure} {i - lookbehind +1}'].sub(sub[col])\n",
    "\n",
    "            return result_frame,unit\n",
    "    if approx:\n",
    "        if measure in approximate_measure_conversion:\n",
    "            for replacement in approximate_measure_conversion[measure]:\n",
    "                value, unit = recursive_fact(comp,replacement,depth+1,approx, row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual, printout,date_gather)\n",
    "                if value.empty != True:\n",
    "                    print(f\"approximate_conversion {measure} to {replacement}\")\n",
    "                    return value, unit\n",
    "        if measure in approximate_additive_conversion:\n",
    "            values = []\n",
    "            if approximate_additive_conversion[measure] != []:\n",
    "                abort = False\n",
    "                for part in approximate_additive_conversion[measure]:\n",
    "                    if abort == False:\n",
    "                        add_value, unit = recursive_fact(comp,part,depth+1,approx,row_delta, column_delta, dynamic_row_delta,static_tolerance, dynamic_tolerance, lookbehind, annual, printout,date_gather)\n",
    "                        if unit == \"del\":\n",
    "                            return add_value, unit\n",
    "                        if add_value.empty == True:\n",
    "                            if part not in optional:\n",
    "                                abort = True\n",
    "                            continue\n",
    "                        values.append(add_value)\n",
    "                if not abort:\n",
    "                    #Get all the possible indexes for the frame\n",
    "                    frame1 = values[0]\n",
    "                    for idx, value in enumerate(values[1:], start=1):\n",
    "                        frame1, value = frame1.align(value, join=\"outer\", axis=0)\n",
    "                        values[idx] = value\n",
    "                        \n",
    "                    result_frame = pd.DataFrame(index=frame1.index)\n",
    "                    for i, col in enumerate(frame1.columns):\n",
    "                        result_frame[f'{measure} {i - lookbehind +1}'] = frame1[col]  # Initialize with frame1's columns\n",
    "                    for value in values:\n",
    "                        for i, col in enumerate(value.columns):\n",
    "                            result_frame[f'{measure} {i - lookbehind +1}'] = result_frame[f'{measure} {i - lookbehind +1}'].add(value[col])\n",
    "                    print(f\"appoximate_addition {measure}\")\n",
    "                    return result_frame, unit\n",
    "    if printout:\n",
    "        print(f\"{measure} not available for {comp.ticker}\")\n",
    "    return pd.DataFrame(), None\n",
    "\n",
    "def path_selector(comp, measure, path,row_delta = pd.Timedelta(days=1), column_delta = pd.Timedelta(days=365),static_tolerance=pd.Timedelta(days =0), dynamic_row_delta=pd.Timedelta(days=1), dynamic_tolerance=pd.Timedelta(days=91), lookbehind =5 , annual=False):\n",
    "    \"\"\"\n",
    "    Takes in the desired path to the data and outputs the data\n",
    "    New recursive_fact\n",
    "    \"\"\"\n",
    "    if path == ():              \n",
    "        value, unit = comp.fact(measure, row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual)\n",
    "        #We need to account for measure conversion after add and sub\n",
    "        if value.empty == True:\n",
    "            if measure in measure_conversion:\n",
    "                for replacement in measure_conversion[measure]:\n",
    "                    value, unit = comp.fact(replacement, row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual)\n",
    "                    if value.empty != True:\n",
    "                        break\n",
    "        return value, unit\n",
    "    #Add path\n",
    "    elif path[0] == \"add\":\n",
    "        values = []\n",
    "        if additive_conversion[measure] != []:\n",
    "            for part in additive_conversion[measure]:\n",
    "                add_value, unit = path_selector(comp, part, (), row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual)\n",
    "                values.append(add_value)\n",
    "            #Get all the possible indexes for the frame\n",
    "            frame1 = values[0]\n",
    "            for idx, value in enumerate(values[1:], start=1):\n",
    "                frame1, value = frame1.align(value, join=\"outer\", axis=0)\n",
    "                values[idx] = value\n",
    "            result_frame = pd.DataFrame(index=frame1.index)\n",
    "            for i, col in enumerate(frame1.columns):\n",
    "                result_frame[f'{measure} {i - lookbehind +1}'] = frame1[col]  # Initialize with frame1's columns\n",
    "            for value in values:\n",
    "                for i, col in enumerate(value.columns):\n",
    "                    result_frame[f'{measure} {i - lookbehind +1}'] = result_frame[f'{measure} {i - lookbehind +1}'].add(value[col])\n",
    "            return result_frame, unit\n",
    "        \n",
    "    elif path[0] == \"sub\":\n",
    "        add, unit = path_selector(comp, subtract_conversion[measure][0], (), row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual)\n",
    "        sub, unit = path_selector(comp, subtract_conversion[measure][1], (), row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual)\n",
    "        #Get all the possible indexes for the frame\n",
    "        values = [add, sub]\n",
    "        add, sub = add.align(sub, join=\"outer\", axis=0)\n",
    "        result_frame = pd.DataFrame(index=add.index)\n",
    "        for i, col in enumerate(add.columns):\n",
    "            result_frame[f'{measure} {i - lookbehind +1}'] = add[col]  # Initialize with add's columns\n",
    "        for i, col in enumerate(sub.columns):\n",
    "            result_frame[f'{measure} {i - lookbehind +1}'] = result_frame[f'{measure} {i - lookbehind +1}'].sub(sub[col])\n",
    "        return result_frame,unit\n",
    "    \n",
    "    elif path[0] == \"approx_add\":\n",
    "        values = []\n",
    "        if approximate_additive_conversion[measure] != []:\n",
    "            for part in approximate_additive_conversion[measure]:\n",
    "                add_value, unit = path_selector(comp, part, (), row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual)\n",
    "                values.append(add_value)\n",
    "            #Get all the possible indexes for the frame\n",
    "            frame1 = values[0]\n",
    "            for idx, value in enumerate(values[1:], start=1):\n",
    "                frame1, value = frame1.align(value, join=\"outer\", axis=0)\n",
    "                values[idx] = value\n",
    "            result_frame = pd.DataFrame(index=frame1.index)\n",
    "            for i, col in enumerate(frame1.columns):\n",
    "                result_frame[f'{measure} {i - lookbehind +1}'] = frame1[col]  # Initialize with frame1's columns\n",
    "            for value in values:\n",
    "                for i, col in enumerate(value.columns):\n",
    "                    result_frame[f'{measure} {i - lookbehind +1}'] = result_frame[f'{measure} {i - lookbehind +1}'].add(value[col])\n",
    "            return result_frame, unit\n",
    "    else:\n",
    "        path_selector(comp, path[0], path[1:], row_delta, column_delta, static_tolerance, dynamic_row_delta, dynamic_tolerance, lookbehind, annual, approx)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "def recursive_date_gather(comp, measure, depth=0, current_path= [], path_date = None, blind = False, approx = True, printout =False):\n",
    "    #path = (\"first_step\", \"second_step\"...)\n",
    "    #The whole thing return the best path with the date that you get\n",
    "    #Individual calls do not return anything, the tree stops whenever there is a return\n",
    "    if path_date is None and depth==0:\n",
    "        path_date = {\"del\": False}\n",
    "    date_list = []\n",
    "    if depth>4:\n",
    "        return None\n",
    "    if printout:\n",
    "        print(f\"Entering recursive with {measure}\")\n",
    "    dates = comp.fact(measure, date_gather = True)\n",
    "    if dates == \"del\":\n",
    "        path_date[\"del\"] = True\n",
    "    if dates != None:\n",
    "        if not blind:\n",
    "            #We leave this empty becuase it represents the call to the current path, imagine adding teh measure intput to the star of all paths \n",
    "            path_date[tuple(current_path)] = dates\n",
    "        date_list.append(dates)\n",
    "        \n",
    "    if measure in measure_conversion:\n",
    "        for replacement in measure_conversion[measure]:\n",
    "            #we do not add anything to the current path because its handled \n",
    "            dates = recursive_date_gather(comp, replacement, depth+1, current_path + [replacement], path_date, blind, approx, printout)\n",
    "            if dates != None:\n",
    "                date_list.append(dates)\n",
    "\n",
    "    if measure in additive_conversion:\n",
    "        start_list = []\n",
    "        end_list = []\n",
    "        if additive_conversion[measure] != []:\n",
    "            abort = False\n",
    "            for part in additive_conversion[measure]:\n",
    "                if abort == False:\n",
    "                    dates = recursive_date_gather(comp, part,depth+1, current_path + [\"add\"],path_date, True, approx, printout) #set blind to true to avoid adding false paths\n",
    "                    if dates == None:\n",
    "                        if part not in optional:\n",
    "                            abort = True\n",
    "                        continue\n",
    "                    else:\n",
    "                        start, end = dates\n",
    "                        start_list.append(start)\n",
    "                        end_list.append(end)\n",
    "            if not abort:\n",
    "                start = max(start_list)\n",
    "                end = min(end_list)\n",
    "                if not blind:\n",
    "                    path_date[tuple(current_path + [\"add\"])] = (start, end)\n",
    "                date_list.append((start,end))\n",
    "                        \n",
    "    if measure in subtract_conversion:\n",
    "        dates_add = recursive_date_gather(comp,subtract_conversion[measure][0],depth+1, current_path + [\"sub\"], path_date, True, approx, printout)\n",
    "        dates_sub = recursive_date_gather(comp,subtract_conversion[measure][1],depth+1, current_path + [\"sub\"], path_date, True, approx, printout)\n",
    "        if dates_add != None and dates_sub != None:\n",
    "            dates = (max(dates_add[0], dates_sub[0]), min(dates_add[1], dates_sub[1]))\n",
    "            if not blind:\n",
    "                path_date[tuple(current_path + [\"sub\"])] = dates\n",
    "            date_list.append(dates)\n",
    "\n",
    "    if approx:\n",
    "        if measure in approximate_measure_conversion:\n",
    "            for replacement in approximate_measure_conversion[measure]:\n",
    "                dates = recursive_date_gather(comp, replacement, depth+1, current_path + [replacement], path_date, blind, approx, printout)\n",
    "\n",
    "        if measure in approximate_additive_conversion:\n",
    "            start_list = []\n",
    "            end_list = []\n",
    "            if approximate_additive_conversion[measure] != []:\n",
    "                abort = False\n",
    "                for part in approximate_additive_conversion[measure]:\n",
    "                    if abort == False:\n",
    "                        dates = recursive_date_gather(comp, part,depth+1,current_path + [\"approx_add\"],path_date, True, approx, printout) #set blind to true to avoid adding false paths\n",
    "                        if dates == None:\n",
    "                            if part not in optional:\n",
    "                                abort = True\n",
    "                            continue\n",
    "                        else:\n",
    "                            start, end = dates\n",
    "                            start_list.append(start)\n",
    "                            end_list.append(end)\n",
    "                if not abort:\n",
    "                    start = max(start_list)\n",
    "                    end = min(end_list)\n",
    "                    if not blind:\n",
    "                        path_date[tuple(current_path + [\"approx_add\"])] = (start, end)\n",
    "                    date_list.append((start,end))\n",
    "    if printout:\n",
    "        print(f\"{measure} not available for {comp.ticker}\")\n",
    "    if depth ==0:\n",
    "        if path_date[\"del\"] == True:\n",
    "            return \"del\"\n",
    "        #some logic to find the best path here\n",
    "        best_interval = pd.Timedelta(days=0)\n",
    "        best_path = None\n",
    "        best_dates = None\n",
    "        for path, dates in list(path_date.items())[1:]: #skip the dele\n",
    "            interval = pd.to_datetime(dates[1]) - pd.to_datetime(dates[0])\n",
    "            if interval > best_interval:\n",
    "                best_interval = interval\n",
    "                best_dates = dates\n",
    "                best_path = path\n",
    "        #We can also use the path from here to get the needed measures.\n",
    "        return (best_path,best_dates)\n",
    "    #Find the best dates\n",
    "    if date_list == []:\n",
    "        return None\n",
    "    start_dates, end_dates = [item[0] for item in date_list], [item[1] for item in date_list]\n",
    "    first_start = min(start_dates)\n",
    "    index = start_dates.index(first_start)\n",
    "    return date_list[index]\n",
    "\n",
    "def get_category(sic):\n",
    "    for category, number_ranges in categories.items():\n",
    "        for number_range in number_ranges:\n",
    "            if number_range[0]<=sic<=number_range[-1]:\n",
    "                return category\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "#(comp, measure, depth=0, approx = True, row_delta = pd.Timedelta(days=1), column_delta = pd.Timedelta(days=365),static_tolerance=pd.Timedelta(days =0), dynamic_row_delta=pd.Timedelta(days=1), dynamic_tolerance=pd.Timedelta(days=91), lookbehind =5 , annual=False, printout =False, date_gather= False\n",
    "def acquire_frame(comp, measures:dict, indicator_frame, static_start_threshold = 1, static_end_threshold = 1, dynamic_start_threshold = 1, dynamic_end_threshold = 1, approx= True, threshold = 1, row_delta = pd.Timedelta(days=1), column_delta = pd.Timedelta(days=365), static_tolerance=pd.Timedelta(days =0), dynamic_row_delta=pd.Timedelta(days=1), dynamic_tolerance=pd.Timedelta(days=91),  lookbehind =5, annual=False, printout =False):\n",
    "    #Get a dataframe from the saved data of some stock \n",
    "    #Returns 0 in all the columns where data is missing\n",
    "    comp.time_init(measures, static_start_threshold, static_end_threshold , dynamic_start_threshold , dynamic_end_threshold)\n",
    "    catg = get_category(comp.sic)\n",
    "    frames_list = []\n",
    "    unit_list = []\n",
    "    df = {}\n",
    "    for motion in [\"static\",\"dynamic\"]:\n",
    "        for measure in measures[motion]:\n",
    "            data, unit = recursive_fact(comp, measure, 0, approx, row_delta , column_delta , static_tolerance, dynamic_row_delta,dynamic_tolerance, lookbehind, annual, printout)\n",
    "            if data.empty != True:\n",
    "                data.name = measure\n",
    "                frames_list.append(data)\n",
    "                unit_list.append(unit)\n",
    "            else:\n",
    "                frames_list.append(pd.DataFrame(data={measure: np.nan}, index=comp.date_range))\n",
    "                unit_list.append(\"missing\")\n",
    "                catandticker =  comp.ticker +\":\"+ comp.sic_desc\n",
    "                if catandticker in Unavailable_Measures[catg]:\n",
    "                    Unavailable_Measures[catg][catandticker] += [measure]\n",
    "                else:\n",
    "                    Unavailable_Measures[catg][catandticker] = [measure] \n",
    "\n",
    "        df[motion] = pd.concat(frames_list, axis =1, join=\"outer\")\n",
    "    #If units are necessary\n",
    "    # columns_multiindex = pd.MultiIndex.from_tuples([(col, unit) for col, unit in zip(df.columns, unit_list)],names=['Variable', 'Unit'])\n",
    "    # df.columns = columns_multiindex\n",
    "    #Economic indicators \n",
    "    # indicator_frame = indicator_frame.reindex(comp.date_range)\n",
    "    # indicator_frame = indicator_frame.ffill().bfill()\n",
    "    # df = df.join(indicator_frame, how=\"left\")\n",
    "        df[motion] = df[motion].join(comp.price, how= \"left\")\n",
    "        df[motion].attrs[\"units\"] = unit_list\n",
    "        df[motion].attrs[\"category\"] = catg\n",
    "    return df\n",
    "\n",
    "#Initializes and appends the stock object\n",
    "async def async_task(ticker, client, semaphore_edgar, semaphore_yahoo, measures):\n",
    "    # Measures are used to get the date when all the financial info is available\n",
    "    print(f\"Loading {ticker}\")\n",
    "    stock = Stock(ticker, measures)\n",
    "    if stock.success == \"del\":\n",
    "        return (ticker, \"del\")\n",
    "    # successful_sic = await stock.async_init(client,semaphore_edgar,measures)\n",
    "    # if successful_sic == \"del\":\n",
    "    #     return (ticker, \"del\")\n",
    "    if stock.success:\n",
    "        print(f\"Price pinging {ticker}$\")\n",
    "        succesful_price = await stock.price_init(semaphore_yahoo)\n",
    "    else:\n",
    "        succesful_price = 0\n",
    "    with open(f'companies\\{ticker}.pkl', 'wb') as file:\n",
    "        pickle.dump(stock,file)\n",
    "    success = stock.success\n",
    "    del stock\n",
    "    #Return (ticker, availability of data, availability of price)\n",
    "    print(f\"||Done {ticker}||\")\n",
    "    return (ticker, [success, succesful_price])\n",
    "\n",
    "#Get the success rate for the api call\n",
    "def success_rate(availability_list):\n",
    "    edgar_success = 0\n",
    "    yahoo_success = 0\n",
    "    for i in availability_list:\n",
    "        ticker, available = i\n",
    "        if available != \"del\":\n",
    "            edgar_success += available[0]\n",
    "            yahoo_success += available[1]\n",
    "        else:\n",
    "            edgar_success += 1\n",
    "            yahoo_success += 1\n",
    "    try:\n",
    "        edgar_success = edgar_success/len(availability_list)\n",
    "        yahoo_success = yahoo_success/len(availability_list)\n",
    "        print(f\"Edgar success rate: {edgar_success}\")\n",
    "        print(f\"Yahoo success rate: {yahoo_success}\")\n",
    "    except ZeroDivisionError:\n",
    "        print(\"No list to analyze\")\n",
    "            \n",
    "#Function to call again for missing data\n",
    "def ticker_fill(company_frames_availability):\n",
    "    ticker_list = []\n",
    "    for ticker, available in company_frames_availability.items():\n",
    "        if available[0] and available[1]:\n",
    "            continue\n",
    "        else:\n",
    "            ticker_list.append(ticker)\n",
    "    return ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sub',)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FreeCashFlow -4</th>\n",
       "      <th>FreeCashFlow -3</th>\n",
       "      <th>FreeCashFlow -2</th>\n",
       "      <th>FreeCashFlow -1</th>\n",
       "      <th>FreeCashFlow 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-09-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-21</th>\n",
       "      <td>1.943500e+10</td>\n",
       "      <td>2.428700e+10</td>\n",
       "      <td>2.564400e+10</td>\n",
       "      <td>3.021800e+10</td>\n",
       "      <td>2.083800e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>1.943500e+10</td>\n",
       "      <td>2.428700e+10</td>\n",
       "      <td>2.564400e+10</td>\n",
       "      <td>3.021800e+10</td>\n",
       "      <td>2.083800e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-23</th>\n",
       "      <td>1.943500e+10</td>\n",
       "      <td>2.428700e+10</td>\n",
       "      <td>2.564400e+10</td>\n",
       "      <td>3.021800e+10</td>\n",
       "      <td>2.083800e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-24</th>\n",
       "      <td>1.943500e+10</td>\n",
       "      <td>2.428700e+10</td>\n",
       "      <td>2.564400e+10</td>\n",
       "      <td>3.021800e+10</td>\n",
       "      <td>2.083800e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-25</th>\n",
       "      <td>1.943500e+10</td>\n",
       "      <td>2.428700e+10</td>\n",
       "      <td>2.564400e+10</td>\n",
       "      <td>3.021800e+10</td>\n",
       "      <td>2.083800e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2649 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FreeCashFlow -4  FreeCashFlow -3  FreeCashFlow -2  \\\n",
       "2016-09-24              NaN              NaN              NaN   \n",
       "2016-09-25              NaN              NaN              NaN   \n",
       "2016-09-26              NaN              NaN              NaN   \n",
       "2016-09-27              NaN              NaN              NaN   \n",
       "2016-09-28              NaN              NaN              NaN   \n",
       "...                     ...              ...              ...   \n",
       "2023-12-21     1.943500e+10     2.428700e+10     2.564400e+10   \n",
       "2023-12-22     1.943500e+10     2.428700e+10     2.564400e+10   \n",
       "2023-12-23     1.943500e+10     2.428700e+10     2.564400e+10   \n",
       "2023-12-24     1.943500e+10     2.428700e+10     2.564400e+10   \n",
       "2023-12-25     1.943500e+10     2.428700e+10     2.564400e+10   \n",
       "\n",
       "            FreeCashFlow -1  FreeCashFlow 0  \n",
       "2016-09-24              NaN             NaN  \n",
       "2016-09-25              NaN             NaN  \n",
       "2016-09-26              NaN             NaN  \n",
       "2016-09-27              NaN             NaN  \n",
       "2016-09-28              NaN             NaN  \n",
       "...                     ...             ...  \n",
       "2023-12-21     3.021800e+10    2.083800e+10  \n",
       "2023-12-22     3.021800e+10    2.083800e+10  \n",
       "2023-12-23     3.021800e+10    2.083800e+10  \n",
       "2023-12-24     3.021800e+10    2.083800e+10  \n",
       "2023-12-25     3.021800e+10    2.083800e+10  \n",
       "\n",
       "[2649 rows x 5 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = comp_load(\"AAPL\")\n",
    "path_date= recursive_date_gather(comp, \"FreeCashFlow\")\n",
    "print(path_date[0])\n",
    "data, unit = path_selector(comp,\"FreeCashFlow\", path_date[0])\n",
    "data.head(-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[191], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m comp \u001b[38;5;241m=\u001b[39m comp_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother_pickle\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmeasures.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m     fund_measures \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#{\"dynamic\":[\"FreeCashFlow\"], \"static\":[ \"Assets\", \"Liabilities\",\"\"]}\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[191], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m comp \u001b[38;5;241m=\u001b[39m comp_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother_pickle\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmeasures.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m     fund_measures \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#{\"dynamic\":[\"FreeCashFlow\"], \"static\":[ \"Assets\", \"Liabilities\",\"\"]}\u001b[39;00m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "comp = comp_load(\"AAPL\")\n",
    "with open(\"other_pickle\\measures.json\", \"r\") as file:\n",
    "    fund_measures = json.load(file)\n",
    "#{\"dynamic\":[\"FreeCashFlow\"], \"static\":[ \"Assets\", \"Liabilities\",\"\"]}\n",
    "comp.date_reset()\n",
    "aframe= acquire_frame(comp, fund_measures, None, static_tolerance=pd.Timedelta(days=365))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\"TB3MS\", \"DCOILWTICO\"]\n",
    "indicator_frame = fred_info(indicators, START, datetime.now().date())\n",
    "with open(\"other_pickle\\indicator_frame.pkl\", \"wb\") as file:\n",
    "    pickle.dump(indicator_frame, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out measures based on importance in descending order\n",
    "with open(\"other_pickle\\measures.json\", \"r\") as file:\n",
    "    fund_measures = json.load(file)\n",
    "#Load out the indicators \n",
    "with open(\"other_pickle\\indicator_frame.pkl\", \"rb\") as file:\n",
    "    indicator_frame = pickle.load(file)\n",
    "\n",
    "#Load the info about the companies that we already have (ticker,edgar,yahoo)\n",
    "with open(r\"other_pickle\\frame_availability.pkl\", \"rb\") as file:\n",
    "    company_frames_availability = pickle.load(file)\n",
    "\n",
    "#Load the unavailable data so far \n",
    "with open(r'other_pickle\\unavailable.json', 'r') as file:\n",
    "        Unavailable_Measures = json.load(file)\n",
    "\n",
    "edgar_client =  httpx.AsyncClient()\n",
    "sem_edgar = asyncio.Semaphore(9)\n",
    "#Separate sem for yahoo to spread the work and connections\n",
    "sem_yahoo = asyncio.Semaphore(9)\n",
    "\n",
    "#Create tasks to ge the first companies_num companies by valuation\n",
    "# company_frames_availability = {\"TSM\":(0,0)}\n",
    "companies_num = 100\n",
    "ticker_dict = dict(islice(company_frames_availability.items(), companies_num))\n",
    "ticker_list = ticker_fill(ticker_dict)\n",
    "tasks = []\n",
    "for ticker in ticker_list:\n",
    "    tasks.append(async_task(ticker, edgar_client, sem_edgar, sem_yahoo, fund_measures))\n",
    "\n",
    "availability_list = await asyncio.gather(*tasks)\n",
    "\n",
    "for ping in availability_list:\n",
    "    ticker, avail = ping\n",
    "    company_frames_availability[ticker] = avail\n",
    "    if avail == \"del\":\n",
    "         del company_frames_availability[ticker]\n",
    "\n",
    "success_rate(availability_list)\n",
    "\n",
    "print(company_frames_availability)\n",
    "with open(r\"other_pickle\\frame_availability.pkl\", \"wb\") as file:\n",
    "    pickle.dump(company_frames_availability, file)\n",
    "\n",
    "#Save the unavailable measures for later use\n",
    "with open(r'other_pickle\\unavailable.json', 'w') as file:\n",
    "        json.dump(Unavailable_Measures,file, indent= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIG RESET!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#GATHER THE FIRST companies_num companies ciks and pass them to the gather with the tasks\n",
    "company_frames_availability = {}\n",
    "for company, values in cikdata.items():\n",
    "    company_frames_availability[values[\"ticker\"]]=[0,0]\n",
    "print(company_frames_availability)\n",
    "with open(r\"other_pickle\\frame_availability.pkl\", \"wb\") as file:\n",
    "    pickle.dump(company_frames_availability, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNAVAILABLE RESET\n",
    "with open(r\"categories\\categories.json\", \"r\") as file:\n",
    "      categories = json.load(file)\n",
    "Unavailable_Measures = {}\n",
    "for category, num_range in categories.items():\n",
    "    Unavailable_Measures[category] = {}\n",
    "with open(r'other_pickle\\unavailable.json', 'w') as file:\n",
    "        json.dump(Unavailable_Measures,file, indent = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = comp_load(\"TSM\")\n",
    "print(comp)\n",
    "for i,(ticker, availability) in enumerate(company_frames_availability.items()):\n",
    "    edgar, yahoo = availability\n",
    "    if edgar == 1 and yahoo ==1:\n",
    "        comp = comp_load(ticker)\n",
    "        print(comp.foreign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEs = []\n",
    "companies_count = 0\n",
    "missing_tickers = []\n",
    "#company_frames_availability\n",
    "for i,(ticker, availability) in enumerate({\"GE\": (1,1)}.items()):\n",
    "    print(ticker)\n",
    "    edgar, yahoo = availability\n",
    "    if edgar == 1 and yahoo ==1:\n",
    "        companies_count +=1\n",
    "        comp = comp_load(ticker)\n",
    "        if comp.foreign == False:\n",
    "            earnings, unit = recursive_fact(comp, \"NetIncomeLoss\", lookbehind =1, annual=True)\n",
    "            shares, unit = recursive_fact(comp, \"EntityCommonStockSharesOutstanding\", lookbehind =1, static_tolerance=pd.DateOffset(years =1))\n",
    "            try:\n",
    "                shares = shares[shares.columns[0]]\n",
    "                earnings = earnings[earnings.columns[0]]\n",
    "                price = comp.price[\"close\"]\n",
    "                EPS = earnings / shares\n",
    "                PE = price / EPS\n",
    "                PE = PE.dropna()\n",
    "                if PE.empty == True:\n",
    "                    missing_tickers.append(ticker)\n",
    "                PE = PE[PE >= 0]\n",
    "                # PEs.append(PE.mean())\n",
    "                if np.isnan(PE.mean()):\n",
    "                    print(PE)\n",
    "                # print(\"|Done|\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "for i in PEs:\n",
    "    print(i)\n",
    "print(missing_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure = []\n",
    "for i in PEs:\n",
    "    if type(i) == np.float64:\n",
    "        if 100>i:\n",
    "            pure.append(i)\n",
    "mean = np.float64(0)\n",
    "for i in pure:\n",
    "    mean = mean + i\n",
    "mean = mean/len(pure)\n",
    "print(mean)\n",
    "print(len(pure))\n",
    "print(len(PEs))\n",
    "\n",
    "companies_count = 0\n",
    "#company_frames_availability\n",
    "for ticker, availability in company_frames_availability.items():\n",
    "    edgar, yahoo = availability\n",
    "    if edgar == 1 and yahoo ==1:\n",
    "        companies_count +=1\n",
    "print(companies_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"other_pickle\\indicator_frame.pkl\", \"rb\") as file:\n",
    "    indicator_frame = pickle.load(file)\n",
    "\n",
    "#Load the info about the companies that we already have (ticker,edgar,yahoo)\n",
    "with open(r\"other_pickle\\frame_availability.pkl\", \"rb\") as file:\n",
    "    company_frames_availability = pickle.load(file)\n",
    "\n",
    "with open(r'other_pickle\\unavailable.json', 'r') as file:\n",
    "        Unavailable_Measures = json.load(file)\n",
    "\n",
    "with open(r\"other_pickle\\measures.json\", \"r\") as file:\n",
    "    measures = json.load(file)\n",
    "measures = [\"Assets\", \"Liabilities\", \"AssetsCurrent\", \"AssetsNoncurrent\"]\n",
    "for ticker, availability in company_frames_availability.items():\n",
    "    edgar, yahoo = availability\n",
    "    if edgar and yahoo:\n",
    "        try:\n",
    "            with open(f'companies\\{ticker}.pkl', 'rb') as file:\n",
    "                comp = pickle.load(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{ticker} is not available for loading\")\n",
    "        catg = get_category(comp.sic)\n",
    "        frame = acquire_frame(comp, measures, indicator_frame, approx=True, threshold=0.8, lookbehind =5) #category_measures[catg]\n",
    "        #Save it again because of time_init and also save some memory\n",
    "        with open(f'companies\\{ticker}.pkl', 'wb') as file:\n",
    "            pickle.dump(comp,file)\n",
    "            del comp\n",
    "        if \"missing\" in frame.attrs[\"units\"]:\n",
    "            frame.to_csv(f\"companies_data_missing\\{catg}\\{ticker}.csv\")\n",
    "            print(f\"{ticker} missing\")\n",
    "        frame.to_csv(f\"companies_data\\{catg}\\{ticker}.csv\")\n",
    "        print(f\"{ticker} saved.\")\n",
    "\n",
    "with open(r'other_pickle\\unavailable.json', 'w') as file:\n",
    "        json.dump(Unavailable_Measures,file, indent= 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XBRL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elementframe = pd.read_csv(\"ListOfElements.csv\")\n",
    "# Elementframe = Elementframe[Elementframe[\"approvalStatus\"].apply(lambda rem: rem==\"Final\")]\n",
    "# Elementframe.to_csv(\"ListOfElements.csv\")\n",
    "def xbrl_wordsearch(word, length):\n",
    "    matches = Elementframe[Elementframe[\"elementName\"].apply(lambda x: x if (len(x)<length) else \"\").str.contains(word, na=False)]\n",
    "    if matches.empty:\n",
    "        print(\"No match\")\n",
    "        return\n",
    "    print(json.dumps(matches[[\"elementName\",\"definition\",\"Deprecated\"]].to_dict(orient=\"records\"), indent = 2))\n",
    "\n",
    "with open(r\"companies\\MSFT.pkl\", \"rb\") as file:\n",
    "    company = pickle.load(file)\n",
    "\n",
    "with open(r\"units.json\", \"w\") as file:\n",
    "    json.dump(compdict,file, indent= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple.data[\"facts\"][\"dei\"][\"EntityCommonStockSharesOutstanding\"][\"units\"][\"shares\"]\n",
    "frame  = fred_info([\"TB3MS\", \"DCOILWTICO\"], '2015-02-24', '2017-02-24')\n",
    "frame.head(40)\n",
    "# print(frame)\n",
    "\n",
    "frame[\"index\"] = frame[\"index\"].astype(str)\n",
    "with open(\"FRED.json\", \"w\") as file:\n",
    "    json.dump(frame.to_dict(orient=\"records\"), file, indent=1)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "tickers = [\"META\"]\n",
    "for ticker in tickers:\n",
    "    data = sync_companyfacts(ticker).json()\n",
    "    data = data[\"facts\"][\"us-gaap\"]\n",
    "    for key,value in data.items():\n",
    "        del value[\"units\"]\n",
    "        if not key in dictionary:\n",
    "            dictionary[key] = value\n",
    "\n",
    "with open(\"deprecated.json\", \"w\")as file:\n",
    "    json.dump(dictionary, file, indent= 1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Get all the measure names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"MSFT\"\n",
    "unit_list =[]\n",
    "for ticker, availability in company_frames_availability.items():\n",
    "    edgar, yahoo = availability\n",
    "    if edgar and yahoo:\n",
    "        with open(f\"companies\\{ticker}.pkl\", \"rb\")as file:\n",
    "            company = pickle.load(file)\n",
    "            data = company.data\n",
    "            compdict = {}\n",
    "            for name,value in data.items():\n",
    "                units = value[\"units\"]\n",
    "                unit_list += list(units.keys())\n",
    "\n",
    "print(set(unit_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = comp_load(\"MSFT\")\n",
    "for i in list(comp.data.keys()):\n",
    "    print(i)\n",
    "comp_data = comp.data['Revenues'][\"units\"][\"USD\"]\n",
    "filtered = []\n",
    "end_prev = comp_data[0][\"end\"]\n",
    "end_next = comp_data[2][\"end\"]\n",
    "# Always add the first element; comparison starts from the second element\n",
    "filtered.append(comp_data[0])\n",
    "for i in range(1, len(comp_data) - 1):\n",
    "    end = comp_data[i][\"end\"]\n",
    "    end_next = comp_data[i + 1][\"end\"]\n",
    "    # Check if current 'end' matches either the previous or next 'end'\n",
    "    if end == end_prev or end == end_next:\n",
    "        filtered.append(comp_data[i])\n",
    "    # Update 'end_prev' for the next iteration\n",
    "    end_prev = end\n",
    "# Always add the last element; its 'end' was compared in the last iteration\n",
    "filtered.append(comp_data[-1])\n",
    "\n",
    "form_and_date = []\n",
    "for datapoint in comp_data:\n",
    "    form_and_date.append({datapoint[\"fp\"]: datapoint[\"end\"]})\n",
    "\n",
    "val = []\n",
    "for datapoint in comp_data:\n",
    "    val.append(datapoint[\"val\"])\n",
    "cleaner = []\n",
    "for datapoint in comp_data:\n",
    "    cleaner.append((datapoint[\"end\"], datapoint[\"val\"], datapoint[\"form\"]))\n",
    "\n",
    "\n",
    "#changes the data, so keep last\n",
    "for datapoint in comp_data:\n",
    "    del datapoint[\"form\"]\n",
    "    del datapoint[\"accn\"]\n",
    "    del datapoint[\"fy\"]\n",
    "\n",
    "\n",
    "with open(\"examples\\clearer.json\", \"w\") as file:\n",
    "    json.dump(comp_data,file, indent=1)\n",
    "\n",
    "with open(\"examples\\quintuple_fuckers_filtered.json\", \"w\") as file:\n",
    "    json.dump(filtered,file, indent=1)\n",
    "with open(\"examples\\quintuple_fuckers_unfiltered.json\", \"w\") as file:\n",
    "    json.dump(comp_data, file, indent =1)\n",
    "with open(r\"examples\\forms.json\", \"w\") as file:\n",
    "    json.dump(form_and_date, file, indent =0)\n",
    "with open(r\"examples\\cleaner.json\", \"w\") as file:\n",
    "    json.dump(cleaner, file, indent =0)\n",
    "with open(r\"examples\\value.json\", \"w\") as file:\n",
    "    json.dump(val, file, indent =0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
