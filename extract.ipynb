{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to info: Stock[\"facts\"][\"us-gaap\"][some_measure][\"units\"][\"USD\"]\n",
    "\n",
    "Path to meta info: Stock[\"facts\"][\"dei\"][some_meausere]\n",
    "\n",
    "List of all possible names for shares outstanding:\n",
    "    CommonStockSharesOutstanding\n",
    "    EntityCommonStockSharesOutstanding\n",
    "\n",
    "Decide if to ping individual concepts instead of getting all the data\n",
    "Alternative is storing all the files in harddisk instead of memory\n",
    "    The Current strategy is to store the data using pickle and reload it for analysis\n",
    "\n",
    "Three things:\n",
    "    First:\n",
    "        Graph and correlate measures and ratios\n",
    "            Currently forward filling fundamentals data, could be fucked in the future\n",
    "            Include the rate of change of the variables and compare those \n",
    "    Second:\n",
    "        Make a test to validate trading strategies\n",
    "            Add a function that picks eligible stocks at some timeframe and test their performance xdz\n",
    "    Third:\n",
    "        Train a model to predict the long term price\n",
    "            forward filling fundamentals can be really fucked \n",
    "            Consider using averages or linear change from previous value to next for training \n",
    "            Include all the past fundamentals for each datapoint - this could figure out how people value growth.\n",
    "\n",
    "Data to add for regression and AI:\n",
    "    oil price\n",
    "    interest rates \n",
    "        FRED HAS SO MUCH DATA HOLY SHIT \n",
    "        pick yer fucking poison mate\n",
    "    \n",
    "\n",
    "Measures:\n",
    "    Price/BookValuepershare = MarketCap/BookValue = EntityCommonStockSharesOutstanding*SPrice(Assets-Liabilities)\n",
    "    Price/Earnings - all the possible earnings metrics\n",
    "    Current ratio = AssetsCurrent/LiabilitiesCurrent\n",
    "    ///////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    Revenue\n",
    "    Operating Income \n",
    "    Free cash flow \n",
    "    EBITDA \n",
    "    Dividends - \"PaymentsOfDividendsCommonStock\"\n",
    "\n",
    "How does market cap influence these correlations?\n",
    "\n",
    "Where do you find price data?\n",
    "    Yahoo-fin.\n",
    "\n",
    "\n",
    "Methods to figure out relationships:\n",
    "    Kendall's Tau\n",
    "    Spearman's Rank Correlation\n",
    "\n",
    "    cross variable correlation loop through all variables \n",
    "    Polynomial regression\n",
    "    LSTM\n",
    "\n",
    "\n",
    "Todo:\n",
    "    You are using the calls one after another in the same task, you can run them concurrently\n",
    "    Investigate why not forward filling\n",
    "    Figure out the start time\n",
    "        Maybe split the price part into a second async price_init\n",
    "    Implement additive logic for unavailable measures\n",
    "    Gitignore is fucking up again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "from fredapi import Fred\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import yahoo_fin.stock_info as si\n",
    "from datetime import datetime\n",
    "import tracemalloc\n",
    "import copy\n",
    "\n",
    "fred = Fred(api_key='0c34c4dd2fd6943f6549f1c990a8a0f0') \n",
    "\n",
    "async def fetch(url, url_headers, semaphore, client, timeout, max_retries, start_retry_delay):\n",
    "    async with semaphore:\n",
    "        for attempt in range(1,max_retries):\n",
    "            try:\n",
    "                response = await client.get(url, timeout=timeout, headers= url_headers)\n",
    "                response.raise_for_status()\n",
    "                return response  # Successful request, exit the loop\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                    headers = response.headers\n",
    "                    #Sometimes a retry-after header is returned\n",
    "                    retry_after = headers.get('Retry-After')\n",
    "                    if retry_after != None:\n",
    "                        #Just for debugging\n",
    "                        print(retry_after)\n",
    "                        await asyncio.sleep(retry_after.astype(int))\n",
    "                        continue\n",
    "                    print(f\"Error response {e.response.status_code}.\")\n",
    "            except httpx.TimeoutException as e:\n",
    "                print(f\"Timeout reached: {e}\")\n",
    "                print(f\"Retrying in {attempt*start_retry_delay} seconds...\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except httpx.RequestError as e:\n",
    "                print(f\"An error occurred: {e}.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0\n",
    "                \n",
    "\n",
    "def fred_info(ids:list, start:str, end:str):\n",
    "    #start and end are datatime objects\n",
    "    start = start.strftime('%Y-%m-%d')\n",
    "    end = end.strftime('%Y-%m-%d')\n",
    "    frame = pd.DataFrame()\n",
    "    for id in ids:\n",
    "        series = fred.get_series(id,observation_start=start, observation_end=end)\n",
    "        frame[id] = series\n",
    "    frame = frame.reset_index()\n",
    "    frame[\"index\"] = frame[\"index\"].astype(str)\n",
    "    frame = frame.ffill()\n",
    "    return frame.bfill()\n",
    "\n",
    "async def fred_fetch(ids:list, start:str, end:str):\n",
    "    fred_data = await asyncio.to_thread(fred_info,ids,start,end)\n",
    "    return fred_data\n",
    "\n",
    "async def yahoo_fetch(ticker, start_year, end_year, semaphore, max_retries, start_retry_delay):\n",
    "    async with semaphore:\n",
    "        for attempt in range(1,max_retries):\n",
    "            try:\n",
    "                response = await asyncio.to_thread(si.get_data,ticker,start_year, end_year)\n",
    "                response.raise_for_status()\n",
    "                return response  # Successful request, exit the loop\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                    headers = response.headers\n",
    "                    #Sometimes a retry-after header is returned\n",
    "                    retry_after = headers.get('Retry-After')\n",
    "                    if retry_after != None:\n",
    "                        #Just for debugging\n",
    "                        print(retry_after)\n",
    "                        await asyncio.sleep(retry_after.astype(int))\n",
    "                        continue\n",
    "                    print(f\"Error response {e.response.status_code}.\")\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"Yahoo Timeout reached: {e}\")\n",
    "                print(f\"Retrying in {attempt*start_retry_delay} seconds...\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"A Yahoo error occurred: {e}.\")\n",
    "                await asyncio.sleep(attempt*start_retry_delay)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decompose the measure into its constituents\n",
    "measure_conversion = {\"Assets\":[[\"AssetsNoncurrent\", \"AssetsCurrent\"]],\n",
    "                    \"Liabilities\":[[\"LiabilitiesCurrent\", \"LiabilitiesNoncurrent\"]],\n",
    "                    \"AssetsCurrent\":[[\"AssetsCurrent\"]],\n",
    "                    \"LiabilitiesCurrent\":[[\"LiabilitiesCurrent\"]],\n",
    "                    \"revenues\": [[\"revenues\"]]\n",
    "}\n",
    "\n",
    "#Lookup table for the undeprecated version of a measure\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\deprecated_to_current.json\", \"r\") as file:\n",
    "    deprecate_conversion = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "#The first entry date into the EDGARD database\n",
    "START = datetime.strptime('1993-01-01', r\"%Y-%m-%d\")\n",
    "\n",
    "def runlist(dict, nameslist):\n",
    "    idx = 0\n",
    "    while (idx<len(nameslist)):\n",
    "        try:\n",
    "            data = dict[nameslist[idx]]\n",
    "            return data\n",
    "        except KeyError:\n",
    "            idx +=1\n",
    "    return False\n",
    "\n",
    "def searchdict(dict, nameslist):\n",
    "    for key in [\"dei\", \"us-gaap\"]:\n",
    "        data = runlist(dict[key], nameslist) \n",
    "        if  data != False:\n",
    "            return data\n",
    "    raise KeyError(f\"Company doesnt have any of {nameslist}\")\n",
    "\n",
    "\n",
    "def getcik(ticker):\n",
    "    #Convert the ticker into the proper cik\n",
    "    for key,value in cikdata.items():\n",
    "        if value[\"ticker\"] == ticker:\n",
    "            cik = value[\"cik_str\"]\n",
    "            break\n",
    "    return str(cik).zfill(10)\n",
    "\n",
    "#Headers for EDGAR call\n",
    "headers = {\n",
    "    \"User-Agent\":\"ficakc@seznam.cz\",\n",
    "    \"Accept-Encoding\":\"gzip, deflate\",\n",
    "}\n",
    "\n",
    "TIMEOUT = 8\n",
    "RETRIES = 2\n",
    "START_RETRY_DELAY = 1\n",
    "# cik_url =  \"https://www.sec.gov/files/company_tickers.json\"\n",
    "# cikdata = requests.get(cik_url, headers=headers).json()\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\cik.json\",\"r\") as file:\n",
    "    cikdata = json.load(file)\n",
    "    file.close()\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\apple.json\",\"r\") as file:\n",
    "    Apple = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def sync_companyfacts(ticker:str):\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    data  = httpx.get(data_url, headers= headers)\n",
    "    return data\n",
    "    \n",
    "async def companyfacts(ticker:str, client, semaphore):\n",
    "    #Get all the financial data for a ticker\n",
    "    cik = getcik(ticker)\n",
    "    data_url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    facts = await fetch(data_url, headers, semaphore, client, TIMEOUT,RETRIES,START_RETRY_DELAY)\n",
    "    return facts\n",
    "\n",
    "def endtodatetime(dataframe):\n",
    "    dataframe.loc[:,\"end\"] = pd.datetime(dataframe[\"end\"])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "class Stock:\n",
    "    def __init__(self, ticker:str):\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = getcik(self.ticker)\n",
    "        \n",
    "    async def async_init(self,client, semaphore, standard_measures):\n",
    "        #Get all of the data for the company, ALL of it \n",
    "        data = await companyfacts(self.ticker, client, semaphore)\n",
    "        #If the response wasn't recieved, skips the rest of the code \n",
    "        if type(data) != int:\n",
    "            self.data = data.json()\n",
    "        else:\n",
    "            return 0\n",
    "        #Get the share amount \n",
    "        self.share_name_list = [\"EntityCommonStockSharesOutstanding\", \"CommonStockSharesOutstanding\"]\n",
    "        meta = copy.deepcopy(self.data[\"facts\"])\n",
    "        #searches the company dict for the first occurence of something in the names list\n",
    "        share_info = searchdict(meta, self.share_name_list)\n",
    "        share_date= datetime.strptime(share_info[\"units\"][\"shares\"][0][\"end\"], r\"%Y-%m-%d\")\n",
    "        #Get the earliest date with all the info about the company\n",
    "        self.start_year = max([share_date] + [datetime.strptime((self.data[\"facts\"][\"us-gaap\"][deprecate_conversion[measure]][\"units\"][\"USD\"][0][\"end\"], r\"%Y-%m-%d\") if deprecate_conversion[measure] in self.data[\"facts\"][\"us-gaap\"] else START)if measure in deprecate_conversion else (datetime.strptime(self.data[\"facts\"][\"us-gaap\"][measure][\"units\"][\"USD\"][0][\"end\"], r\"%Y-%m-%d\") if measure in self.data[\"facts\"][\"us-gaap\"] else START) for measure in standard_measures])\n",
    "        self.end_year = datetime.now().date()\n",
    "        return 1\n",
    "    async def price_init(self):\n",
    "        #Get the price and set the self.price\n",
    "        self.fullprice = await yahoo_fetch(self.ticker,self.start_year, self.end_year)\n",
    "        self.fullprice = self.fullprice.reset_index()\n",
    "        Price = self.fullprice[[self.fullprice.columns[0],\"close\", \"adjclose\"]].copy()\n",
    "        Price[\"end\"] = Price[\"index\"].astype(str)\n",
    "        Price.drop(columns=[\"index\"],inplace=True)\n",
    "        date_range = pd.date_range(start=self.start_year, end=self.end_year).astype(str)\n",
    "        self.date_range = pd.DataFrame(date_range, columns=['end'])\n",
    "        Price = pd.merge(self.date_range, Price, on = [\"end\"],how=\"left\" )\n",
    "        self.price = Price.ffill().bfill()\n",
    "        return 1 \n",
    "    def fact(self,measure,simple=True):\n",
    "        #Propagate the 0 \n",
    "        if self.data == 0:\n",
    "            return   \n",
    "        try:\n",
    "            if measure in deprecate_conversion:\n",
    "                measure = deprecate_conversion[measure]\n",
    "                # frame = pd.concat([frame, frame_undep], axis=0).reset_index(drop=True)\n",
    "            point_list = self.data[\"facts\"][\"us-gaap\"][measure][\"units\"][\"USD\"]\n",
    "            frame = pd.DataFrame(point_list)\n",
    "            frame = frame.drop_duplicates(subset='end', keep='last')\n",
    "            frame[measure] = frame[\"val\"]\n",
    "            if simple:\n",
    "                frame = frame[[\"end\", measure]]\n",
    "            #If the measure is deprecated switch to the undeprecated version\n",
    "            frame = pd.merge(self.date_range,frame,on=\"end\",how=\"left\")\n",
    "            frame = frame.ffill().bfill()\n",
    "            return frame\n",
    "        except KeyError:\n",
    "            print(f\"Measure {measure} not available for {self.ticker}.\")\n",
    "    def shares(self,simple=True):\n",
    "        #Propagate the 0 \n",
    "        if self.data == 0:\n",
    "            return 0\n",
    "        if simple:\n",
    "            meta = copy.deepcopy(self.data[\"facts\"])\n",
    "            share_count = pd.DataFrame(searchdict(meta,self.share_name_list)[\"units\"][\"shares\"][0], index=[0])[[\"end\",\"val\"]]\n",
    "        share_count[\"shares\"] = share_count[\"val\"]\n",
    "        share_count.drop(columns=[\"val\"], inplace = True)\n",
    "        share_count = share_count.drop_duplicates(subset=\"end\", keep=\"last\")\n",
    "        share_count = pd.merge(self.date_range, share_count, on=[\"end\"], how=\"left\")\n",
    "        return share_count.ffill()\n",
    "    \n",
    "\n",
    "#Initializes and appends the stock object\n",
    "async def async_task(ticker, client, semaphore, measures):\n",
    "    # Measures are used to get the date when all the financial info is available\n",
    "    stock = Stock(ticker)\n",
    "    print(f\"Currently pinging {ticker}\")\n",
    "    succesful = await stock.async_init(client,semaphore,measures)\n",
    "    if not succesful:\n",
    "        return (ticker, False)\n",
    "    with open(f'C:\\Programming\\Python\\Finance\\EDGAR\\companies\\{ticker}.pkl', 'wb') as file:\n",
    "        pickle.dump(stock,file)\n",
    "        file.close()\n",
    "    #Returns availability of data for a specific company\n",
    "    return (ticker,True)\n",
    "\n",
    "\n",
    "def acquire_frame(ticker, measures, indicator_frame):\n",
    "    with open(f'C:\\Programming\\Python\\Finance\\EDGAR\\companies\\{ticker}.pkl', 'rb') as file:\n",
    "        stock = pickle.load(file)\n",
    "        file.close()\n",
    "    try:\n",
    "        #Price and shares oustanding \n",
    "        shares = stock.shares().copy()\n",
    "        stock_price = stock.price.copy()\n",
    "        df = pd.merge(shares, stock_price, left_on=[\"end\"], right_on=[\"end\"], how = \"left\")\n",
    "        frames_list = [stock.fact(measure) for measure in measures]\n",
    "        for frame in frames_list:\n",
    "            df = pd.merge(df,frame, on=[\"end\"], how=\"left\")\n",
    "    except AttributeError:\n",
    "        return\n",
    "    #Economic indicators \n",
    "    df = pd.merge(df, indicator_frame, left_on =[\"end\"], right_on=[\"index\"], how=\"left\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently pinging AAPL\n",
      "Currently pinging MSFT\n",
      "Currently pinging GOOGL\n",
      "Currently pinging AMZN\n",
      "Currently pinging NVDA\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8800\\3347442248.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcomp\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mcompany_frames_availablity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0masync_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mticker_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mcompany_frames_tuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macquire_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompany_frames_availablity\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mcompany_frames_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompany_frames_tuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcompany_frames_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8800\\3347442248.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;31m#write out measures based on importance in descending order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8800\\120158389.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(ticker, measures, indicator_frame)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m#Economic indicators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'index'"
     ]
    }
   ],
   "source": [
    "#write out measures based on importance in descending order\n",
    "# tracemalloc.start()\n",
    "measures = [\"Assets\", \"Liabilities\", \"AssetsCurrent\", \"LiabilitiesCurrent\"]\n",
    "#write out the indicators \n",
    "indicators = [\"TB3MS\", \"DCOILWTICO\"]\n",
    "indicator_frame = asyncio.run(fred_fetch(indicators, START, datetime.now().date()))\n",
    "#Get the first n companies sorted by market cap \n",
    "companies_num = 5\n",
    "comp = 0\n",
    "edgar_client =  httpx.AsyncClient()\n",
    "sem_edgar = asyncio.Semaphore(9)\n",
    "#Separate sem and client for yahoo to spread the work and connections\n",
    "yahoo_client = httpx.AsyncClient()\n",
    "sem_yahoo = asyncio.Semaphore(9)\n",
    "#GATHER THE FIRST companies_num companies ciks and pass them to the gather with the tasks\n",
    "ticker_list = []\n",
    "for company, values in cikdata.items():\n",
    "    if comp<companies_num:\n",
    "        ticker_list.append(values[\"ticker\"])\n",
    "        comp+=1\n",
    "    else:\n",
    "        break\n",
    "company_frames_availablity = await asyncio.gather(*[async_task(ticker, edgar_client, sem_edgar, measures) for ticker in ticker_list])\n",
    "company_frames_tuples = [(ticker,acquire_frame(ticker, measures,pd.DataFrame())) for ticker,value in company_frames_availablity if value]\n",
    "company_frames_dict = {}\n",
    "for key,value in company_frames_tuples:\n",
    "    company_frames_dict[key] = value\n",
    "\n",
    "# for i in range(len(company_frames_tuples)):\n",
    "#     if company_frames_tuples[i] == None:\n",
    "#         del company_frames_tuples[i]\n",
    "# for ticker, df in company_frames_tuples:\n",
    "#     company_frames_dict[ticker] = df\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8800\\1421013583.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\Programming\\Python\\Finance\\EDGAR\\companies\\AAPL.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"br\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mapple\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mApple_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macquire_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AAPL\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"AccountsPayable\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8800\\1302156518.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(ticker, measures, indicator_frame)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m#Economic indicators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ficak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'index'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\companies\\AAPL.pkl\", \"br\") as file:\n",
    "    apple =  pickle.load(file)\n",
    "\n",
    "Apple_frame = acquire_frame(\"AAPL\",[\"AccountsPayable\"],pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple.data[\"facts\"][\"dei\"][\"EntityCommonStockSharesOutstanding\"][\"units\"][\"shares\"]\n",
    "frame  = fred_info([\"TB3MS\", \"DCOILWTICO\"], '2015-02-24', '2017-02-24')\n",
    "frame.head(40)\n",
    "# print(frame)\n",
    "\n",
    "frame[\"index\"] = frame[\"index\"].astype(str)\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\FRED.json\", \"w\") as file:\n",
    "    json.dump(frame.to_dict(orient=\"records\"), file, indent=1)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated fucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "tickers = [\"AAPL\", \"MSFT\"]\n",
    "for ticker in tickers:\n",
    "    data = sync_companyfacts(ticker).json()\n",
    "    data = data[\"facts\"][\"us-gaap\"]\n",
    "    for key,value in data.items():\n",
    "        del value[\"units\"]\n",
    "        if not key in dictionary:\n",
    "            dictionary[key] = value\n",
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\deprecated.json\", \"w\")as file:\n",
    "    json.dump(dictionary, file, indent= 1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkout(ticker, name):\n",
    "    data = sync_companyfacts(ticker).json()\n",
    "    dict1  = data[\"facts\"][\"us-gaap\"]\n",
    "    dict2  = data[\"facts\"][\"dei\"]\n",
    "    dict = {**dict1, **dict2}\n",
    "    complist = []\n",
    "    for key in dict:\n",
    "        complist.append(key)\n",
    "\n",
    "    with open(f\"C:\\Programming\\Python\\Finance\\EDGAR\\{name}.json\", \"w\") as file:\n",
    "        json.dump(complist, file, indent =1)\n",
    "        file.close()\n",
    "\n",
    "# amazon = sync_companyfacts(ticker).json()\n",
    "dictionary = amazon[\"facts\"][\"dei\"].keys()\n",
    "dictionary  = [i for i in dictionary]\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\amazontotal.json\", \"w\") as file:\n",
    "        json.dump(dictionary, file, indent =1)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "# print(amazon[\"facts\"].keys())\n",
    "# checkout(\"AMZN\",\"amazon_liabl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Stock.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m measures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssetsCurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiabilitiesCurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m Apple \u001b[38;5;241m=\u001b[39m \u001b[43mStock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maapl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m stock \u001b[38;5;241m=\u001b[39m Apple\n\u001b[0;32m      4\u001b[0m shares \u001b[38;5;241m=\u001b[39m stock\u001b[38;5;241m.\u001b[39mshares()\n",
      "\u001b[1;31mTypeError\u001b[0m: Stock.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "measures = [\"Assets\", \"Liabilities\", \"AssetsCurrent\", \"LiabilitiesCurrent\"]\n",
    "Apple = Stock(\"aapl\", measures)\n",
    "stock = Apple\n",
    "shares = stock.shares()\n",
    "stock_num = stock.price\n",
    "if isinstance(shares, int) or isinstance(stock_num, int): \n",
    "    pass\n",
    "    # break\n",
    "df = pd.merge(shares.copy(), stock_num.copy(), on=[\"end\"], how = \"left\")\n",
    "frames_list = [stock.fact(measure) for measure in measures]\n",
    "for frame in frames_list:\n",
    "    df = pd.merge(df,frame, on=[\"end\"], how=\"left\")\n",
    "df.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Get all the measure names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\stock.json\", \"w\") as file:\n",
    "#     json.dump(df.to_dict(orient='records'), file, indent=1)\n",
    "#     file.close()\n",
    "\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\shares.json\", \"w\") as file:\n",
    "    json.dump(stock.shares().copy().to_dict(orient='records'), file, indent=1)\n",
    "    file.close()\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\assets.json\", \"w\") as file:\n",
    "    json.dump(Apple[\"facts\"][\"us-gaap\"][\"Assets\"][\"units\"][\"USD\"], file, indent=1)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = {}\n",
    "for key,value in Apple[\"facts\"][\"us-gaap\"].items() :\n",
    "    measures[key.ljust(100)] = value[\"label\"]\n",
    "\n",
    "measures[\"METADATA\".ljust(200,\"/\")] = \"\"\n",
    "\n",
    "for key,value in Apple[\"facts\"][\"dei\"].items():\n",
    "    measures[key.ljust(100)] = value[\"label\"]\n",
    "\n",
    "\n",
    "with open(r\"C:\\Programming\\Python\\Finance\\EDGAR\\measures.json\",\"w\") as file:\n",
    "    json.dump(measures, file, indent=1)\n",
    "\n",
    "\n",
    "#create price reference list:\n",
    "with open(\"C:\\Programming\\Python\\Finance\\EDGAR\\price.json\", \"w\") as file:\n",
    "    json.dump(Apple.price.to_dict(orient='records'), file, indent=1)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Apple[\"facts\"][\"us-gaap\"][\"AssetsCurrent\"][\"units\"][\"USD\"] :\n",
    "    try:\n",
    "        del key[\"frame\"]\n",
    "        print(key)\n",
    "    except KeyError:\n",
    "        print(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Apple[\"facts\"][\"us-gaap\"][\"Assets\"][\"units\"][\"USD\"][0][\"end\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
